1
00:00:00,400 --> 00:00:12,974
我很困惑，为什么有些人把时间线设得极短，但与此同时，却又看好在大型语言模型之上大规模
应用强化学习。如果我们真的接近人类学习者，那么这种基于可验证结果的训练方法注定会失败

2
00:00:12,974 --> 00:00:13,124
。

3
00:00:15,540 --> 00:00:30,586
目前，各大实验室正尝试在模型训练过程中，为这些模型植入多种技能。有一整条供应链的公司
正在构建强化学习环境，教导模型如何浏览网页或使用Excel来构建财务模型。

4
00:00:30,920 --> 00:00:41,905
那么，这些模型要么很快就能在工作中自主学习，这会让所有这些预先训练变得毫无意义；要么
它们不能，这意味着通用人工智能（AGI）并非迫在眉睫。人类不必经历这种特殊的训练阶段

5
00:00:41,905 --> 00:00:48,706
，他们需要排练工作中可能用到的每一款软件。巴伦·米利奇在他最近的一篇博客文章中就此提
出了一个有趣的观点。

6
00:00:48,680 --> 00:01:02,607
他写道：“当我们看到前沿模型在各种基准测试中不断改进时，我们不应该只考虑规模的扩大和
巧妙的机器学习研究思路，而应该想到，数十亿美元被支付给博士、医生和其他专家，让他们撰

7
00:01:02,607 --> 00:01:09,903
写问题，提供示例答案和推理，以精确地针对这些能力。”这种张力在机器人领域表现得最为明
显。

8
00:01:10,400 --> 00:01:23,130
从根本上说，机器人学是一个算法问题，而不是硬件或数据问题。经过很少的训练，人类就能学
会如何操作现有硬件来完成有用的工作。所以，如果我们真的有一个像人类一样的学习者，机器

9
00:01:23,130 --> 00:01:32,830
人学在很大程度上就会是一个已解决的问题。但我们没有这样的学习者，这就使得我们必须走进
千家万户，练习上百万次如何拿起盘子或叠衣服。

10
00:01:32,980 --> 00:01:46,158
现在，我听到一个反驳，来自那些认为我们将在未来五年内实现“起飞”的人，他们说我们必须
做所有这些笨拙的强化学习，以构建一个超人类的AI研究员，然后这百万个自动化的伊利亚副

11
00:01:46,158 --> 00:01:50,394
本就可以去找出如何解决从经验中进行稳健高效学习的问题。

12
00:01:50,460 --> 00:02:02,752
这让我想起了那个老笑话，“我们每卖一件都亏钱，但我们可以靠销量弥补回来。”不知怎的，
这个自动化研究员竟然能找出通用人工智能的算法，而这个问题人类已经为此绞尽脑汁半个多世

13
00:02:02,752 --> 00:02:07,874
纪了，然而它却没有孩子们所具备的基本学习能力，我觉得这简直是天方夜谭。

14
00:02:09,620 --> 00:02:21,555
此外，即便你持有这样的观点，这并不能说明实验室是如何从可验证的奖励机制中进行强化学习
的。你不需要预先植入顾问制作PPT幻灯片的技能，以便实现伊利亚的自动化。

15
00:02:21,780 --> 00:02:36,222
显然，实验室的举动暗示了一种世界观，认为这些模型在泛化能力和实际应用学习方面将持续表
现不佳，因此有必要将我们希望在经济上具有实用价值的技能，提前内置到这些模型中。

16
00:02:36,220 --> 00:02:50,339
另一个反驳论点是，即使模型可以在工作中学习这些技能，但在训练时一次性构建这些技能会更
有效率，而不是为每个用户和每家公司反复进行。而且，你看，将对浏览器和终端等常用工具的

17
00:02:50,339 --> 00:02:58,743
熟练度内置进去非常有道理，事实上，AGI的一个关键优势是它们能够以更强的能力在不同副
本之间共享知识。

18
00:02:59,840 --> 00:03:12,982
但人们真的低估了完成大多数工作所需要的，那些与公司和具体情境紧密相关的技能，而且目前
还没有一个稳健、高效的机制，能让人工智能习得这些技能。

19
00:03:15,960 --> 00:03:28,879
我最近和一位AI研究员和一位生物学家共进晚餐，结果发现这位生物学家有很长的时间线，所
以我们就问她为什么会有这么长的时间线。然后她说，你知道，她最近在实验室工作的一部分，

20
00:03:28,879 --> 00:03:35,186
就是观察玻片，然后判断玻片上的那个点到底是不是巨噬细胞，还是只是看起来像巨噬细胞。

21
00:03:35,440 --> 00:03:54,108
于是那位人工智能研究员，正如你所预料的，回答说：“听着，图像分类是教科书式的深度学习问题。这正是我们可以训练这些模型来做的事情的核心。”
我认为这是一次非常有趣的交流，因为它揭示了一个关键症结，在我与那些期待在未来几年内产生变革性经济影响的人之间。

22
00:03:54,500 --> 00:04:02,441
人类员工之所以宝贵，恰恰在于我们无需为他们工作的每一个细枝末节，都去设置那些冗长乏味
的训练流程。

23
00:04:02,740 --> 00:04:17,074
考虑到这个实验室制备玻片的特定方式，为了识别巨噬细胞的样子而建立一个定制的训练流程，
然后又为下一个实验室特定的微任务建立另一个训练循环，如此循环往复，这并不是一个划算的

24
00:04:17,074 --> 00:04:24,241
做法。你真正需要的是一个能够从语义反馈或自主经验中学习，并像人类一样进行泛化的AI。

25
00:04:24,600 --> 00:04:40,860
每天，你都要处理上百件需要判断力、临场应变能力，以及工作经验和技能的事情。这些任务不
仅因人而异，甚至对同一个人来说，每天都在变化。仅仅通过预设一套技能，是不可能自动化哪

26
00:04:40,860 --> 00:04:44,151
怕一项工作的，更不用说所有工作了。

27
00:04:46,500 --> 00:04:59,185
事实上，我认为人们真的低估了真正的AGI会有多大影响，因为他们只是在想象当前模式的延
续。他们没有考虑到服务器上数十亿个类人智能，它们可以复制并融合所有的学习成果。坦白说

28
00:04:59,185 --> 00:05:06,283
，我确实预料到这一点，也就是说，我预计在未来一二十年内会出现真正的类脑智能，这简直是
太疯狂了。

29
00:05:09,360 --> 00:05:23,199
有时人们会说，人工智能目前尚未在各公司广泛部署，也未在编程之外提供大量价值的原因是，
技术需要很长时间才能普及，我认为这是一种自我安慰。我认为人们正利用这种自我安慰来掩盖

30
00:05:23,199 --> 00:05:28,307
一个事实，那就是这些模型根本不具备产生广泛经济价值所需的能力。

31
00:05:28,340 --> 00:05:38,231
如果这些模型真的像服务器上的人类一样，它们会扩散得非常快。事实上，它们比普通人类员工
更容易整合和上手。

32
00:05:38,380 --> 00:05:51,678
他们能在几分钟内读完你所有的Slack和Drive内容，并立即提取出你其他AI员工所
具备的所有技能。此外，人类招聘市场非常像一个“柠檬市场”，你很难提前判断谁是优秀人才

33
00:05:51,678 --> 00:05:55,794
，而雇佣一个最终表现不佳的人，其成本显然是非常高的。

34
00:05:57,240 --> 00:06:11,622
这根本不是你必须面对或担心的一种动态，如果你只是启动一个经过审查的AGI模型的另一个
实例的话。因此，基于以上这些原因，我预计将人工智能劳动力融入企业，会比雇佣一个人要容

35
00:06:11,622 --> 00:06:14,190
易得多。而公司一直在雇佣人。"

36
00:06:14,252 --> 00:06:30,318
如果这些能力真的达到了通用人工智能的水平，人们就会愿意每年花费数万亿美元，来购买这些
模型所生成的代币。全世界的知识工作者每年总共赚取数十万亿美元的工资。而现在，实验室的

37
00:06:30,318 --> 00:06:36,821
收入与这个数字相去甚远的原因是，这些模型的能力远不及人类知识工作者。

38
00:06:39,892 --> 00:06:48,994
你可能会说，“看啊，标准怎么可能变成实验室每年必须赚取数万亿美元的收入呢？”对吧？就
像，直到最近，人们还在说，“这些模型能推理吗？”

39
00:06:49,052 --> 00:07:04,093
这些模型有常识吗？它们只是在做模式识别吗？很明显，人工智能的乐观派批评悲观派反复更改
标准是正确的。这通常是很公平的。我们很容易低估人工智能在过去十年中取得的巨大进展。但

40
00:07:04,093 --> 00:07:07,674
某种程度上的标准改变实际上是完全合理的。

41
00:07:07,732 --> 00:07:17,205
若2020年你给我看Gemini
3，我会确信它能自动化一半的知识工作。所以我们不断解决那些自以为是通往AGI的瓶颈。

42
00:07:17,472 --> 00:07:27,549
我们有具备通用理解能力、少样本学习能力和推理能力模型，但我们仍然没有通用人工智能。那
么，对此我们该如何理性回应呢？

43
00:07:27,872 --> 00:07:47,376
我认为完全有理由看到这一点并说：“哦，实际上，智能和劳动远比我之前意识到的要复杂得多。”
尽管我们已经非常接近，并且在许多方面已经超越了我过去对通用人工智能的定义，但模型公司并没有赚取通用人工智能所暗示的数万亿美元收入这一事实

44
00:07:48,612 --> 00:07:52,048
这清楚地表明我之前对AGI的定义太狭隘了。

45
00:07:52,612 --> 00:08:05,234
我预计这种情况未来会持续发生。我预计到2030年，实验室将在我一直关注的“持续学习”
领域取得重大进展，并且这些模型每年将带来数千亿美元的收入。但它们不会自动化所有的知识

46
00:08:05,234 --> 00:08:11,095
工作。而我就会说：“看，我们取得了很大的进步，但我们还没达到通用人工智能呢。”

47
00:08:11,112 --> 00:08:23,882
我们还需要其他一些功能。这些模型需要具备X、Y和Z这些能力。模型变得越来越令人印象深
刻，是按照人们预测的短期时间线，但变得更有用，则是按照人们预测的长期时间线。

48
00:08:28,612 --> 00:08:38,596
值得我们深思的是，我们究竟在扩展什么？在预训练阶段，我们曾观察到，损失函数在横跨多个
数量级和大量计算资源投入后，呈现出非常清晰且具有普遍性的改进趋势。

49
00:08:38,912 --> 00:08:54,480
尽管如此，这遵循幂律，其弱势正如指数增长的强势。但人们正试图利用预训练规模化所带来的
声誉——这种声誉的确定性几乎堪比宇宙物理定律——来为基于可验证奖励的强化学习的乐观预

50
00:08:54,480 --> 00:08:59,670
测辩护，而对于后者，我们没有任何成熟的、广为人知的趋势。

51
00:08:59,992 --> 00:09:06,957
当那些无畏的研究人员试图从稀缺的公开数据点中拼凑出其影响时，他们得到了相当悲观的结果
。

52
00:09:06,992 --> 00:09:21,531
举个例子，托比·博德写了一篇很棒的文章，他在文章中巧妙地将不同的O系列基准联系起来，
这让他得出了一个结论，那就是：“我们需要将总的强化学习计算量提升大约一百万倍，才能达

53
00:09:21,531 --> 00:09:24,128
到类似单个GPT级别的提升。”

54
00:09:27,472 --> 00:09:42,635
所以人们花了很多时间讨论软件奇点的可能性，即人工智能模型将编写代码，生成一个更智能的
后续系统。或者软件加硬件奇点，即人工智能也会改进其后续系统的计算硬件。然而，所有这些

55
00:09:42,635 --> 00:09:49,856
设想都忽略了，我认为在通用人工智能之上，进一步改进的主要驱动力，那就是持续学习。

56
00:09:50,912 --> 00:10:00,058
再次，思考一下人类是如何在任何事情上变得更加有能力的。这主要来自于相关领域的经验。在一次谈话中，Verran
Millich

57
00:10:00,058 --> 00:10:13,853
提出了一个非常有趣的建议，那就是未来可能就像持续学习的智能体，它们都走出去，做着不同的工作，创造着价值，然后把所有学到的东西带回蜂巢模型，这个模型会对所有这些智能体进行某种批处理蒸馏。

58
00:10:13,812 --> 00:10:29,415
智能体本身可以高度专业化，包含卡帕西所说的认知核心，以及与它们将被部署执行的任务相关
的知识和技能。解决持续学习不会是一个单一的、一劳永逸的成就。相反，它会感觉像是解决情

59
00:10:29,415 --> 00:10:30,158
境学习。

60
00:10:30,292 --> 00:10:43,215
那么，GPT-3在2020年就已经展现出上下文学习的强大能力。它的上下文学习能力是如
此出色，以至于GPT-3论文的标题就是“语言模型是少样本学习者”。但当然，GPT-3

61
00:10:43,215 --> 00:10:50,446
问世时，我们并未完全解决上下文学习问题。事实上，从理解能力到上下文长度，仍有大量进展
需要取得。

62
00:10:50,772 --> 00:11:05,539
我预计持续学习也会有类似的进展。实验室明年可能会发布一些他们称之为持续学习的东西，这
实际上也将是持续学习的进步。但人类水平的在职学习，可能还需要五到十年才能完善。

63
00:11:08,172 --> 00:11:16,159
这就是为什么我不会期待某种爆炸式增长，来自第一个攻克持续学习的模型，那个正被越来越广
泛地部署且能力越来越强。

64
00:11:16,212 --> 00:11:29,300
如果你突然间完全解决了持续学习的问题，那当然，这可能就是大局已定，胜券在握了，就像萨
蒂亚在播客上，我问他这种可能性时所说的那样。但这很可能不会发生。相反，某个实验室会找

65
00:11:29,300 --> 00:11:39,896
到如何在这个问题上取得初步进展的方法，然后，通过研究这个特性，就会清楚它是如何实现的
，接着，其他实验室很快就会复制这项突破，并稍作改进。

66
00:11:39,872 --> 00:11:53,754
此外，我有个预判，所有这些模型公司之间的竞争会一直非常激烈。而这基于我的观察，即所有
这些之前所谓的飞轮效应，无论是聊天中的用户参与度，还是合成数据，或者其他什么，都几乎

67
00:11:53,754 --> 00:11:56,729
没有削弱模型公司之间日益加剧的竞争。

68
00:11:56,932 --> 00:12:09,576
每隔一两个月，三大模型公司就会轮流登上领奖台，而其他竞争者也紧随其后，差距并不大。似
乎有某种力量在起作用，这可能是人才挖角，可能是旧金山的谣言传播，或者只是正常的逆向工

69
00:12:09,576 --> 00:12:14,393
程，迄今为止，这已经抵消了任何一家实验室可能拥有的巨大领先优势。

70
00:12:14,912 --> 00:12:25,594
这是我最初在dwarkesh.com这个博客上发布的一篇文章的旁白。我接下来会发布更
多这样的文章。我发现这实际上对我在面试前理清自己的思路非常有帮助。如果你想及时了解这

71
00:12:25,594 --> 00:12:28,518
些内容，可以在dwarkesh.com上订阅。

72
00:12:28,792 --> 00:12:31,578
不然的话，我们下期播客再见。祝你一切顺利。

