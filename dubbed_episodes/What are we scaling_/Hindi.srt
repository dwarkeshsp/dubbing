1
00:00:00,420 --> 00:00:05,343
मुझे समझ नहीं आता कि कुछ लोग इतनी कम
समय-सीमा रखते हुए भी LLMs पर RL बढ़ाने को

2
00:00:05,343 --> 00:00:10,458
लेकर आशावादी क्यों हैं। अगर हम मानव-समान
सीखने वाले के करीब हैं, तो सत्यापन योग्य

3
00:00:10,458 --> 00:00:13,144
परिणामों पर प्रशिक्षण का यह तरीका बेकार
है।

4
00:00:15,540 --> 00:00:21,821
फ़िलहाल, लैब मिड-ट्रेनिंग से इन मॉडलों
में कई कौशल डाल रही हैं। कंपनियों की पूरी

5
00:00:21,821 --> 00:00:28,102
सप्लाई चेन है जो RL वातावरण बना रही हैं,
जो मॉडल को वेब ब्राउज़र चलाना या एक्सेल

6
00:00:28,102 --> 00:00:30,725
से वित्तीय मॉडल बनाना सिखाते हैं।

7
00:00:30,920 --> 00:00:35,450
अब, या तो ये मॉडल खुद ही काम करते हुए सीख
जाएंगे, जिससे ये सारी तैयारी बेकार हो

8
00:00:35,450 --> 00:00:39,865
जाएगी, या नहीं सीखेंगे, मतलब AGI तुरंत
नहीं आएगा। इंसानों को ऐसी खास ट्रेनिंग

9
00:00:39,865 --> 00:00:44,280
नहीं लेनी पड़ती जहाँ उन्हें हर सॉफ्टवेयर
का अभ्यास करना पड़े जो उन्हें काम पर

10
00:00:44,280 --> 00:00:48,520
चाहिए। बैरन मिलिज ने इस पर एक दिलचस्प बात
अपने हालिया ब्लॉग पोस्ट में कही।

11
00:00:48,680 --> 00:00:52,878
वह लिखते हैं, "जब हम फ्रंटियर मॉडल को
बेंचमार्क पर बेहतर होते देखते हैं, तो

12
00:00:52,878 --> 00:00:57,076
हमें केवल बढ़े पैमाने और चतुर एमएल
विचारों के बारे में ही नहीं सोचना चाहिए,

13
00:00:57,076 --> 00:01:01,615
बल्कि उन अरबों डॉलर के बारे में भी सोचना
चाहिए जो पीएचडी, एमडी और अन्य विशेषज्ञों

14
00:01:01,615 --> 00:01:06,153
को इन सटीक क्षमताओं के लिए प्रश्न लिखने
और उदाहरण उत्तर व तर्क प्रदान करने के लिए

15
00:01:06,153 --> 00:01:10,181
दिए जाते हैं।" यह तनाव रोबोटिक्स में सबसे
स्पष्ट रूप से देखा जा सकता है।

16
00:01:10,400 --> 00:01:15,379
मूल रूप से, रोबोटिक्स हार्डवेयर या डेटा
की नहीं, बल्कि एल्गोरिदम की समस्या है।

17
00:01:15,379 --> 00:01:20,165
बहुत कम प्रशिक्षण से, इंसान मौजूदा
हार्डवेयर को उपयोगी काम के लिए चलाना सीख

18
00:01:20,165 --> 00:01:25,403
सकता है। तो अगर कोई इंसान जैसा सीखने वाला
होता, तो रोबोटिक्स काफी हद तक सुलझ जाती।

19
00:01:25,403 --> 00:01:30,059
लेकिन ऐसा सीखने वाला न होने से, हमें
हजारों घरों में जाकर लाखों बार बर्तन

20
00:01:30,059 --> 00:01:32,969
उठाने या कपड़े मोड़ने का अभ्यास करना
पड़ता है।

21
00:01:32,980 --> 00:01:37,833
अब, एक प्रतिवाद जो मैंने उन लोगों से सुना
है जो सोचते हैं कि हम अगले पाँच सालों में

22
00:01:37,833 --> 00:01:42,509
एक बड़ी छलांग लगाएंगे, वह यह है कि हमें
एक अतिमानवीय AI शोधकर्ता बनाने के लिए यह

23
00:01:42,509 --> 00:01:47,245
सब अटपटा RL करना पड़ा, और फिर इस स्वचालित
इल्या की लाखों प्रतियाँ अनुभव से सुदृढ़

24
00:01:47,245 --> 00:01:50,441
और प्रभावी शिक्षण को कैसे हल करें, यह पता
लगा सकती हैं।

25
00:01:50,420 --> 00:01:55,154
मुझे उस पुराने मज़ाक की याद आती है, 'हम
हर बिक्री पर घाटा उठा रहे हैं, पर मात्रा

26
00:01:55,154 --> 00:01:59,889
से भरपाई कर लेंगे।' यह स्वचालित शोधकर्ता
एजीआई के लिए एल्गोरिथम का पता लगा लेगा,

27
00:01:59,889 --> 00:02:04,505
जो एक ऐसी समस्या है जिससे इंसान आधी सदी
से भी ज़्यादा समय से जूझ रहे हैं, जबकि

28
00:02:04,505 --> 00:02:08,460
इसमें बच्चों जैसी बुनियादी सीखने की
क्षमता भी नहीं है, मुझे यह बहुत

29
00:02:08,460 --> 00:02:09,599
अविश्वसनीय लगता है।

30
00:02:09,639 --> 00:02:14,004
इसके अलावा, भले ही आप ऐसा मानते हों, यह
नहीं बताता कि लैब सत्यापन योग्य इनाम से

31
00:02:14,004 --> 00:02:16,187
रीइन्फोर्समेंट लर्निंग कैसे कर रही हैं।

32
00:02:16,520 --> 00:02:20,268
इल्या को स्वचालित करने के लिए, आपको
सलाहकार की पावरपॉइंट स्लाइड बनाने की

33
00:02:20,268 --> 00:02:24,332
क्षमता को पहले से डालने की आवश्यकता नहीं
है। तो स्पष्ट है, लैब के कार्य एक ऐसी

34
00:02:24,332 --> 00:02:28,397
विश्वदृष्टि की ओर इशारा करते हैं, जहाँ ये
मॉडल सामान्यीकरण और काम पर सीखने में

35
00:02:28,397 --> 00:02:32,462
लगातार खराब प्रदर्शन करेंगे, जिससे उन
कौशलों को इन मॉडलों में पहले से ही डालना

36
00:02:32,462 --> 00:02:36,210
आवश्यक हो जाता है जिनकी हमें उम्मीद है कि
वे आर्थिक रूप से उपयोगी होंगे।

37
00:02:36,220 --> 00:02:40,718
एक और प्रतिवाद जो आप प्रस्तुत कर सकते हैं
वह यह है कि, भले ही मॉडल ये कौशल काम करते

38
00:02:40,718 --> 00:02:44,996
हुए सीख ले, इन कौशलों को प्रशिक्षण के
दौरान एक बार ही विकसित करना हर उपयोगकर्ता

39
00:02:44,996 --> 00:02:49,439
और हर कंपनी के लिए बार-बार विकसित करने से
कहीं अधिक कुशल है। और देखिए, ब्राउज़र और

40
00:02:49,439 --> 00:02:53,608
टर्मिनल जैसे सामान्य उपकरणों के साथ
दक्षता को सीधे शामिल करना बहुत मायने रखता

41
00:02:53,608 --> 00:02:58,051
है, और वास्तव में, एजीआई का एक प्रमुख लाभ
यह होगा कि वे प्रतियों के बीच ज्ञान साझा

42
00:02:58,051 --> 00:02:59,532
करने की अधिक क्षमता रखेंगे।

43
00:02:59,840 --> 00:03:05,562
लेकिन लोग यह नहीं समझते कि ज़्यादातर
कामों में कंपनी और संदर्भ-विशेष कौशल

44
00:03:05,562 --> 00:03:11,682
कितने लगते हैं, और एआई के लिए इन्हें
सीखने का कोई मज़बूत, कुशल तरीका नहीं है।"

45
00:03:15,980 --> 00:03:19,836
मैं हाल ही में एक एआई शोधकर्ता और एक
जीवविज्ञानी के साथ रात के खाने पर था, और

46
00:03:19,836 --> 00:03:23,590
पता चला कि जीवविज्ञानी के पास लंबी
समय-सीमाएँ थीं, तो हम पूछ रहे थे कि उसके

47
00:03:23,590 --> 00:03:27,650
पास ये लंबी समय-सीमाएँ क्यों थीं। और फिर
उसने कहा, तुम्हें पता है, लैब में हाल ही

48
00:03:27,650 --> 00:03:31,658
में काम का एक हिस्सा स्लाइड्स को देखने
में लगा है और यह तय करने में कि उस स्लाइड

49
00:03:31,658 --> 00:03:35,159
में बिंदु वास्तव में एक मैक्रोफेज है या
सिर्फ मैक्रोफेज जैसा दिखता है।

50
00:03:35,460 --> 00:03:39,708
और एआई शोधकर्ता ने, जैसा कि अपेक्षित था,
जवाब दिया, "देखो, इमेज क्लासिफिकेशन एक

51
00:03:39,708 --> 00:03:43,684
पाठ्यपुस्तक डीप लर्निंग समस्या है। यह ठीक
वही है जिसके लिए हम इन मॉडलों को

52
00:03:43,684 --> 00:03:47,933
प्रशिक्षित कर सकते हैं।" और मुझे लगा कि
यह एक बहुत ही दिलचस्प बातचीत थी क्योंकि

53
00:03:47,933 --> 00:03:51,909
इसने मेरे और उन लोगों के बीच एक अहम अंतर
को दर्शाया जो अगले कुछ वर्षों में

54
00:03:51,909 --> 00:03:54,361
परिवर्तनकारी आर्थिक प्रभाव की उम्मीद करते
हैं।

55
00:03:54,500 --> 00:04:00,041
मानव कर्मचारी मूल्यवान हैं क्योंकि हमें
उनके काम के हर छोटे हिस्से के लिए ये थकाऊ

56
00:04:00,041 --> 00:04:02,673
प्रशिक्षण लूप बनाने की ज़रूरत नहीं है।

57
00:04:02,740 --> 00:04:08,631
इस लैब के स्लाइड बनाने के खास तरीके के
लिए मैक्रोफेज पहचानने की कस्टम ट्रेनिंग

58
00:04:08,631 --> 00:04:14,676
पाइपलाइन बनाना, और फिर हर छोटे काम के लिए
अलग लूप बनाना, कुल मिलाकर उत्पादक नहीं

59
00:04:14,676 --> 00:04:20,721
है। आपको वास्तव में एक ऐसी AI चाहिए जो
सिमेंटिक फीडबैक या स्व-निर्देशित अनुभव से

60
00:04:20,721 --> 00:04:24,241
सीख सके और फिर इंसान की तरह सामान्यीकरण
कर सके।

61
00:04:24,600 --> 00:04:30,115
हर दिन, आपको ऐसे कई काम करने होते हैं
जिनमें विवेक, स्थिति की समझ और काम पर

62
00:04:30,115 --> 00:04:35,928
सीखे गए कौशल की ज़रूरत होती है। ये काम न
केवल अलग-अलग लोगों के लिए, बल्कि एक ही

63
00:04:35,928 --> 00:04:41,815
व्यक्ति के लिए भी हर दिन बदलते रहते हैं।
किसी एक नौकरी को भी केवल कुछ तय कौशल से

64
00:04:41,815 --> 00:04:46,287
स्वचालित करना संभव नहीं है, सभी नौकरियों
की तो बात ही छोड़िए।

65
00:04:46,500 --> 00:04:50,194
वास्तव में, मुझे लगता है कि लोग वास्तव
में कम आंक रहे हैं कि असली एजीआई कितनी

66
00:04:50,194 --> 00:04:53,742
बड़ी बात होगी क्योंकि वे बस इस मौजूदा
व्यवस्था की ही कल्पना कर रहे हैं। वे

67
00:04:53,742 --> 00:04:57,728
अरबों मानव-जैसी बुद्धिमत्ताओं के बारे में
नहीं सोच रहे हैं जो एक सर्वर पर सभी सीखों

68
00:04:57,728 --> 00:05:01,471
को कॉपी और मर्ज कर सकती हैं। और स्पष्ट
कहूँ तो, मुझे इसकी उम्मीद है, यानी मुझे

69
00:05:01,471 --> 00:05:04,922
अगले एक या दो दशक में वास्तविक
मस्तिष्क-जैसी बुद्धिमत्ताओं की उम्मीद है,

70
00:05:04,922 --> 00:05:06,283
जो कि बहुत ही अविश्वसनीय है।

71
00:05:09,660 --> 00:05:13,424
कभी-कभी लोग कहते हैं कि एआई अभी कंपनियों
में व्यापक रूप से तैनात नहीं हैं और

72
00:05:13,424 --> 00:05:17,088
कोडिंग के अलावा बहुत अधिक मूल्य प्रदान
नहीं कर रहे हैं, इसका कारण यह है कि

73
00:05:17,088 --> 00:05:21,054
प्रौद्योगिकी को फैलने में लंबा समय लगता
है, और मुझे लगता है कि यह सिर्फ एक बहाना

74
00:05:21,054 --> 00:05:25,170
है। मुझे लगता है कि लोग इस बहाने का उपयोग
इस तथ्य को छिपाने के लिए कर रहे हैं कि इन

75
00:05:25,170 --> 00:05:28,282
मॉडलों में व्यापक आर्थिक मूल्य के लिए
आवश्यक क्षमताएं नहीं हैं।

76
00:05:28,340 --> 00:05:33,711
अगर ये मॉडल सर्वर पर इंसान होते, तो बहुत
तेज़ी से फैलते। दरअसल, उन्हें सामान्य

77
00:05:33,711 --> 00:05:39,292
कर्मचारी से कहीं ज़्यादा आसानी से जोड़ा
जा सकता। वे मिनटों में आपका पूरा स्लैक और

78
00:05:39,292 --> 00:05:44,733
ड्राइव पढ़ सकते थे, और तुरंत आपके अन्य
एआई कर्मचारियों के सभी कौशल सीख सकते थे।

79
00:05:44,940 --> 00:05:50,643
और तो और, मानव भर्ती बाज़ार एक 'लेमन
मार्केट' जैसा ही है, जहाँ अच्छे लोगों को

80
00:05:50,643 --> 00:05:56,571
पहले से पहचानना मुश्किल है, और ज़ाहिर है,
खराब व्यक्ति को काम पर रखना बहुत महंगा

81
00:05:56,571 --> 00:05:57,246
पड़ता है।

82
00:05:57,240 --> 00:06:01,679
यह ऐसी स्थिति नहीं है जिसका आपको सामना
करना पड़े या चिंता करनी पड़े, यदि आप बस

83
00:06:01,679 --> 00:06:06,176
एक जाँचे-परखे एजीआई मॉडल का नया इंस्टेंस
चला रहे हैं। तो इन्हीं कारणों से, मुझे

84
00:06:06,176 --> 00:06:10,327
लगता है कि एआई श्रम को कंपनियों में
फैलाना किसी व्यक्ति को काम पर रखने से

85
00:06:10,327 --> 00:06:14,190
कहीं ज़्यादा आसान होगा। और कंपनियाँ हर
समय लोगों को काम पर रखती हैं।

86
00:06:14,252 --> 00:06:19,650
यदि क्षमताएं वास्तव में एजीआई स्तर की
होतीं, तो लोग इन मॉडलों द्वारा उत्पादित

87
00:06:19,650 --> 00:06:24,836
टोकन खरीदने के लिए सालाना खरबों डॉलर खर्च
करने को तैयार होते। दुनिया भर के

88
00:06:24,836 --> 00:06:30,660
ज्ञानकर्मी सालाना दसियों खरब डॉलर वेतन के
रूप में कमाते हैं। और अभी प्रयोगशालाएं इस

89
00:06:30,660 --> 00:06:36,414
आंकड़े से कई गुना पीछे हैं, इसका कारण यह
है कि मॉडल मानव ज्ञानकर्मियों जितने सक्षम

90
00:06:36,414 --> 00:06:37,053
नहीं हैं।

91
00:06:39,912 --> 00:06:47,118
अब आप सोचेंगे, 'लैब को खरबों डॉलर कमाने
होंगे,' यह कैसा मानक है? हाल तक लोग पूछते

92
00:06:47,118 --> 00:06:50,360
थे, 'क्या ये मॉडल तर्क कर सकते हैं?'

93
00:06:50,432 --> 00:06:55,997
क्या वे सिर्फ पैटर्न पहचान रहे हैं? और
ज़ाहिर है, एआई समर्थक, विरोधियों की

94
00:06:55,997 --> 00:07:01,943
बार-बार लक्ष्य बदलने की आलोचना में सही
हैं। और यह अक्सर जायज़ है। पिछले दशक में

95
00:07:01,943 --> 00:07:07,661
एआई की प्रगति को कम आंकना आसान है। लेकिन
कुछ हद तक लक्ष्य बदलना जायज़ भी है।

96
00:07:07,732 --> 00:07:11,948
अगर आप मुझे 2020 में जेमिनी 3 दिखाते, तो
मुझे यकीन होता कि यह आधे ज्ञान-कार्य को

97
00:07:11,948 --> 00:07:16,005
स्वचालित कर सकता है। और इसलिए हम उन
बाधाओं को हल करते रहते हैं जिन्हें हम AGI

98
00:07:16,005 --> 00:07:20,061
के लिए पर्याप्त मानते थे। हमारे पास ऐसे
मॉडल हैं जिनमें सामान्य समझ है, उनमें

99
00:07:20,061 --> 00:07:24,171
फ़्यू-शॉट लर्निंग है, उनमें तर्कशक्ति है,
फिर भी हमारे पास अभी तक AGI नहीं है।

100
00:07:24,432 --> 00:07:27,264
तो, इस पर तार्किक प्रतिक्रिया क्या है?

101
00:07:27,872 --> 00:07:33,794
इसे देखकर यह कहना बिल्कुल उचित है कि,
"बुद्धिमत्ता और श्रम में मेरी पिछली समझ

102
00:07:33,794 --> 00:07:40,185
से कहीं ज़्यादा है।" और जबकि हम बहुत करीब
हैं, और कई मायनों में हमने पहले के AGI की

103
00:07:40,185 --> 00:07:46,186
परिभाषा को पार कर लिया है, यह तथ्य कि
मॉडल कंपनियाँ AGI से अपेक्षित खरबों डॉलर

104
00:07:46,186 --> 00:07:48,212
का राजस्व नहीं कमा रही हैं

105
00:07:48,612 --> 00:07:51,026
मेरी AGI परिभाषा बहुत संकीर्ण थी।

106
00:07:52,612 --> 00:07:56,503
और मुझे उम्मीद है कि यह आगे भी जारी
रहेगा। मुझे उम्मीद है कि 2030 तक,

107
00:07:56,503 --> 00:08:00,966
प्रयोगशालाएं मेरे पसंदीदा 'निरंतर सीखने'
में काफी प्रगति कर चुकी होंगी, और मॉडल

108
00:08:00,966 --> 00:08:04,914
सालाना अरबों डॉलर का राजस्व कमा रहे
होंगे। लेकिन वे सभी ज्ञान-कार्य को

109
00:08:04,914 --> 00:08:09,206
स्वचालित नहीं कर पाए होंगे। और मैं
कहूंगा, "देखो, हमने बहुत प्रगति की है, पर

110
00:08:09,206 --> 00:08:11,095
अभी तक एजीआई हासिल नहीं किया है।"

111
00:08:11,112 --> 00:08:17,263
हमें इन मॉडलों में X, Y और Z जैसी अन्य
क्षमताएं भी चाहिए। मॉडल कम समय-सीमा वालों

112
00:08:17,263 --> 00:08:23,337
के अनुमान से अधिक प्रभावशाली होते हैं, पर
लंबी समय-सीमा वालों के अनुमान से अधिक

113
00:08:23,337 --> 00:08:23,882
उपयोगी।

114
00:08:28,612 --> 00:08:33,540
यह पूछने लायक है, हम क्या बढ़ा रहे हैं?
प्री-ट्रेनिंग में, लॉस में सुधार का एक

115
00:08:33,540 --> 00:08:38,212
बहुत स्पष्ट और सामान्य रुझान था, जो कई
गुना कंप्यूट और परिमाण में फैला था।

116
00:08:38,212 --> 00:08:43,332
हालांकि, यह एक पावर लॉ पर आधारित था, जो
उतना ही कमजोर है जितना घातीय वृद्धि मजबूत

117
00:08:43,332 --> 00:08:43,844
होती है।

118
00:08:43,892 --> 00:08:48,370
लेकिन लोग प्री-ट्रेनिंग स्केलिंग की उस
प्रतिष्ठा को भुनाने की कोशिश कर रहे हैं,

119
00:08:48,370 --> 00:08:52,619
जो ब्रह्मांड के भौतिक नियम जितनी ही
अनुमानित है, ताकि वे सुदृढीकरण सीखने के

120
00:08:52,619 --> 00:08:56,695
बारे में आशावादी भविष्यवाणियों को सही
ठहरा सकें, जिसके लिए हमारे पास कोई

121
00:08:56,695 --> 00:09:01,288
सुस्थापित सार्वजनिक प्रवृत्ति नहीं है। और
जब साहसी शोधकर्ता दुर्लभ सार्वजनिक डेटा

122
00:09:01,288 --> 00:09:05,422
बिंदुओं से निहितार्थों को एक साथ जोड़ने
की कोशिश करते हैं, तो उन्हें काफी

123
00:09:05,422 --> 00:09:06,972
निराशाजनक परिणाम मिलते हैं।

124
00:09:06,972 --> 00:09:12,078
उदाहरण के लिए, टोबी बोर्ड के एक शानदार
पोस्ट में, उन्होंने विभिन्न ओ-सीरीज़

125
00:09:12,078 --> 00:09:17,185
बेंचमार्क के बीच चतुराई से संबंध जोड़े
हैं और इससे उन्हें यह सुझाव मिला कि,

126
00:09:17,185 --> 00:09:22,637
"हमें कुल आरएल कंप्यूट में दस लाख गुना
वृद्धि की आवश्यकता है ताकि एक जीपीटी स्तर

127
00:09:22,637 --> 00:09:24,294
के समान बढ़ावा मिल सके।"

128
00:09:27,512 --> 00:09:33,370
तो लोगों ने काफी समय से सिर्फ सॉफ्टवेयर
सिंगुलैरिटी की संभावना पर बात की है, जहाँ

129
00:09:33,370 --> 00:09:37,032
AI मॉडल खुद ही बेहतर सिस्टम बनाने वाला
कोड लिखेंगे।

130
00:09:37,272 --> 00:09:42,017
या एक सॉफ्टवेयर और हार्डवेयर सिंगुलैरिटी,
जहाँ AI अपने अगले कंप्यूटिंग हार्डवेयर को

131
00:09:42,017 --> 00:09:46,416
भी बेहतर बनाते हैं। हालांकि, ये सभी
परिदृश्य उस बात को नज़रअंदाज़ करते हैं जो

132
00:09:46,416 --> 00:09:50,873
मेरे विचार से एजीआई के ऊपर और सुधारों का
मुख्य चालक होगा, निरंतर सीखना। फिर से

133
00:09:50,873 --> 00:09:55,619
सोचिए कि इंसान किसी भी चीज़ में कैसे अधिक
सक्षम बनते हैं। यह अधिकतर संबंधित क्षेत्र

134
00:09:55,619 --> 00:09:56,776
के अनुभव से होता है।

135
00:09:56,812 --> 00:10:01,823
बातचीत में, वेरन मिलिच ने यह दिलचस्प
सुझाव दिया कि भविष्य निरंतर सीखने वाले

136
00:10:01,823 --> 00:10:06,766
एजेंटों जैसा हो सकता है जो बाहर जाकर
विभिन्न कार्य करते हैं, मूल्य उत्पन्न

137
00:10:06,766 --> 00:10:12,048
करते हैं, और अपनी सारी सीख हाइव माइंड
मॉडल में वापस लाते हैं, जो इन सभी एजेंटों

138
00:10:12,048 --> 00:10:13,809
पर बैच डिस्टिलेशन करता है।

139
00:10:13,812 --> 00:10:19,066
एजेंट खुद काफी विशिष्ट हो सकते हैं,
जिनमें कार्पेथी का 'कॉग्निटिव कोर' और

140
00:10:19,066 --> 00:10:24,977
उनके काम से संबंधित ज्ञान-कौशल शामिल हैं।
कंटीन्यूअल लर्निंग को हल करना कोई एक बार

141
00:10:24,977 --> 00:10:29,720
का काम नहीं होगा। बल्कि, यह
इन-कॉन्टेक्स्ट लर्निंग को हल करने जैसा

142
00:10:29,720 --> 00:10:30,158
लगेगा।

143
00:10:30,292 --> 00:10:34,721
अब, GPT-3 ने 2020 में ही दिखा दिया था कि
इन-कॉन्टेक्स्ट लर्निंग बहुत शक्तिशाली हो

144
00:10:34,721 --> 00:10:38,985
सकती है। इसकी इन-कॉन्टेक्स्ट लर्निंग
क्षमताएं इतनी उल्लेखनीय थीं कि GPT-3 पेपर

145
00:10:38,985 --> 00:10:43,193
का शीर्षक 'लैंग्वेज मॉडल्स आर फ्यू-शॉट
लर्नर्स' था। लेकिन ज़ाहिर है, जब GPT-3

146
00:10:43,193 --> 00:10:47,623
आया तब हमने इन-कॉन्टेक्स्ट लर्निंग को हल
नहीं किया था, और वास्तव में, समझ से लेकर

147
00:10:47,623 --> 00:10:50,446
संदर्भ की लंबाई तक, अभी भी बहुत प्रगति
करनी बाकी है।

148
00:10:50,772 --> 00:10:55,956
मुझे निरंतर सीखने में भी ऐसी ही प्रगति की
उम्मीद है। लैब अगले साल शायद कुछ जारी

149
00:10:55,956 --> 00:11:01,075
करेंगे, जिसे वे निरंतर सीखना कहेंगे, और
जो वास्तव में निरंतर सीखने की दिशा में

150
00:11:01,075 --> 00:11:06,060
प्रगति ही होगी। लेकिन मानव-स्तर की
ऑन-द-जॉब लर्निंग को सही होने में अभी पाँच

151
00:11:06,060 --> 00:11:07,722
से दस साल और लग सकते हैं।

152
00:11:08,192 --> 00:11:13,545
इसीलिए मैं उम्मीद नहीं करता असीमित लाभ
की, पहले ऐसे मॉडल से जो सतत सीखेगा और

153
00:11:13,545 --> 00:11:15,901
व्यापक रूप से तैनात व सक्षम होगा।

154
00:11:16,212 --> 00:11:21,259
यदि निरंतर सीखने की समस्या अचानक पूरी तरह
हल हो जाती, तो यह गेम, सेट, मैच होता,

155
00:11:21,259 --> 00:11:26,371
जैसा सत्य ने पॉडकास्ट पर इस संभावना पर
मेरे पूछने पर कहा था। लेकिन ऐसा शायद नहीं

156
00:11:26,371 --> 00:11:31,095
होगा। इसके बजाय, कोई लैब इस समस्या पर
शुरुआती पकड़ बनाना सीखेगी, और फिर इस

157
00:11:31,095 --> 00:11:36,272
सुविधा से प्रयोग करने पर यह स्पष्ट होगा
कि इसे कैसे लागू किया गया था, और फिर अन्य

158
00:11:36,272 --> 00:11:39,896
लैब जल्द ही इस सफलता को दोहराकर इसमें
थोड़ा सुधार करेंगी।

159
00:11:39,872 --> 00:11:44,929
इसके अलावा, मेरा पहले से ही अनुमान है कि
इन मॉडल कंपनियों में कड़ी प्रतिस्पर्धा

160
00:11:44,929 --> 00:11:49,792
बनी रहेगी। यह इस बात से स्पष्ट है कि
पिछली सभी कथित 'फ्लाईव्हील्स', जैसे चैट

161
00:11:49,792 --> 00:11:54,654
पर जुड़ाव या सिंथेटिक डेटा, ने मॉडल
कंपनियों के बीच बढ़ती प्रतिस्पर्धा को कम

162
00:11:54,654 --> 00:11:56,729
करने में बहुत कम योगदान दिया है।

163
00:11:56,912 --> 00:12:02,582
हर महीने, तीन बड़ी मॉडल कंपनियाँ शीर्ष पर
बदलती रहती हैं, और बाकी प्रतियोगी भी

164
00:12:02,582 --> 00:12:08,252
ज़्यादा पीछे नहीं। कोई ताकत काम कर रही
है, शायद टैलेंट पोचिंग, SF में अफवाहें,

165
00:12:08,252 --> 00:12:13,997
या सामान्य रिवर्स इंजीनियरिंग, जिसने अब
तक किसी एक लैब की बढ़त को बेअसर कर दिया

166
00:12:13,997 --> 00:12:19,667
है। यह एक निबंध का वाचन था जिसे मैंने
अपने ब्लॉग dwarkesh.com पर जारी किया था।

167
00:12:19,972 --> 00:12:24,660
मैं अब और निबंध प्रकाशित करूँगा। मुझे लगा
कि इंटरव्यू से पहले अपने विचारों को

168
00:12:24,660 --> 00:12:29,596
सुलझाने में यह बहुत मददगार है। उनसे जुड़े
रहने के लिए, dwarkesh.com पर सब्सक्राइब

169
00:12:29,596 --> 00:12:32,742
करें। नहीं तो, अगले पॉडकास्ट में मिलते
हैं। धन्यवाद।

