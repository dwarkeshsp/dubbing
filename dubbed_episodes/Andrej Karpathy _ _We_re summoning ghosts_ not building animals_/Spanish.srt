1
00:00:00,080 --> 00:00:01,194
El refuerzo es terrible.

2
00:00:02,800 --> 00:00:05,679
Da la casualidad de que todo lo que
teníamos antes es mucho peor.

3
00:00:06,680 --> 00:00:09,933
De hecho, soy optimista. Creo que esto
funcionará. Creo que es manejable. Solo

4
00:00:09,933 --> 00:00:12,299
sueno pesimista porque cuando veo mi
timeline de Twitter-

5
00:00:12,740 --> 00:00:15,797
Veo todo esto que no tiene sentido para
mí. Mucho de esto es, creo, honestamente

6
00:00:15,797 --> 00:00:18,390
solo, eh, recaudación de fondos. En
realidad no estamos construyendo

7
00:00:18,390 --> 00:00:21,408
animales. Estamos construyendo fantasmas.
Estas, como, una especie de entidades

8
00:00:21,408 --> 00:00:24,040
espirituales etéreas, porque son
completamente digitales, y son como,

9
00:00:24,040 --> 00:00:26,439
imitando a los humanos, y es un tipo
diferente de inteligencia.

10
00:00:26,520 --> 00:00:29,722
Todo sigue igual porque ya estamos en una
explosión de inteligencia, y lo hemos

11
00:00:29,722 --> 00:00:32,636
estado durante décadas. Todo se está
automatizando gradualmente, y lo ha

12
00:00:32,636 --> 00:00:35,469
estado durante cientos de años. No
escribas entradas de blog. No hagas

13
00:00:35,469 --> 00:00:38,753
diapositivas. No hagas nada de eso. Más
bien, construye el código, arréglalo, haz

14
00:00:38,753 --> 00:00:41,380
que funcione. Es la única manera, si no,
te faltará conocimiento.

15
00:00:41,480 --> 00:00:44,609
Si tienes un tutor de IA perfecto, quizás
puedas llegar extremadamente lejos. Los

16
00:00:44,609 --> 00:00:47,739
genios de hoy apenas rascan la superficie
de lo que una mente humana puede hacer,

17
00:00:47,739 --> 00:00:47,935
creo.

18
00:00:48,520 --> 00:00:52,487
Hoy estoy hablando con Andrej Karpathy.
Andrej, ¿por qué dices que esta será la

19
00:00:52,487 --> 00:00:54,928
Década de los Agentes y no el Año de los
Agentes?

20
00:00:54,940 --> 00:00:58,393
Mm-hmm. Bueno, antes que nada, gracias
por invitarme. Estoy emocionado de estar

21
00:00:58,393 --> 00:01:01,802
aquí. Así que, la cita que acabas de
mencionar, "La Década de los Agentes", es

22
00:01:01,802 --> 00:01:05,079
en realidad una reacción a una cita
preexistente, debería decir, donde creo

23
00:01:05,079 --> 00:01:07,293
que muchos de los... algunos de los
laboratorios...

24
00:01:07,300 --> 00:01:11,226
No estoy realmente seguro de quién dijo
esto, pero aludían a que este era el Año

25
00:01:11,226 --> 00:01:15,152
de los Agentes- ... eh, con respecto a
los LLM y, eh, cómo iban a evolucionar. Y

26
00:01:15,152 --> 00:01:18,780
creo que, um, eso me hizo reaccionar- ...
porque siento que hay demasiadas

27
00:01:18,780 --> 00:01:22,657
predicciones exageradas en la industria.
Y, eh, para mí, esto se describe mucho

28
00:01:22,657 --> 00:01:24,993
más acertadamente como la Década de los
Agentes.

29
00:01:11,200 --> 00:01:11,432
Mmm

30
00:01:24,980 --> 00:01:25,351
Ajá.

31
00:01:25,400 --> 00:01:28,550
Y tenemos algunos agentes muy tempranos
que son, de hecho, extremadamente

32
00:01:28,550 --> 00:01:32,094
impresionantes y que uso a diario. Eh, ya
sabes, Claude y Codex y demás. Pero sigo

33
00:01:32,094 --> 00:01:35,463
sintiendo que hay, eh, mucho trabajo por
hacer. Y entonces creo que mi, eh, mi

34
00:01:35,463 --> 00:01:38,263
reacción es que estaremos trabajando con
estas cosas por décadas.

35
00:01:38,260 --> 00:01:42,905
Van a mejorar, y va a ser algo
maravilloso. Pero creo que solo estaba

36
00:01:42,905 --> 00:01:46,526
reaccionando a los plazos, supongo, de la
implicación.

37
00:01:46,720 --> 00:01:50,156
Y qué crees que tomará una década lograr?
¿Cuáles son los cuellos de botella?

38
00:01:50,660 --> 00:01:53,663
Bueno, eh, haz que funcione de verdad.
Así que, en mi mente, cuando hablamos de

39
00:01:53,663 --> 00:01:56,321
un agente, supongo, o lo que los
laboratorios tienen en mente y lo que

40
00:01:56,321 --> 00:01:59,209
quizás yo también tengo en mente, es que,
eh, deberías pensarlo casi como un

41
00:01:59,209 --> 00:02:01,712
empleado o como un becario que
contratarías para trabajar contigo.

42
00:02:01,760 --> 00:02:05,340
Eh, por ejemplo, trabajas con algunos
empleados aquí. ¿Cuándo preferirías que

43
00:02:05,340 --> 00:02:08,827
un agente como Claude o Codex, eh,
hiciera ese trabajo? Ahora mismo, claro,

44
00:02:08,827 --> 00:02:11,465
no pueden. ¿Qué se necesitaría para que
pudieran hacerlo?

45
00:02:03,620 --> 00:02:03,991
Ajá.

46
00:02:11,720 --> 00:02:14,522
¿Por qué no lo haces hoy? Y la razón por
la que no lo haces hoy es porque

47
00:02:14,522 --> 00:02:17,364
simplemente no funcionan, así que... eh,
por ejemplo, no tienen suficiente

48
00:02:17,364 --> 00:02:20,479
inteligencia, no son lo suficientemente
multimodales, no pueden usar computadoras

49
00:02:20,479 --> 00:02:23,126
y todo ese tipo de cosas, y no hacen
muchas de las cosas a las que te

50
00:02:23,126 --> 00:02:25,618
referiste antes. Sabes, no tienen
aprendizaje continuo. No puedes

51
00:02:25,618 --> 00:02:28,265
simplemente decirles algo y que lo
recuerden. Y simplemente les falta

52
00:02:28,265 --> 00:02:30,990
capacidad cognitiva y no está
funcionando, y creo que llevará alrededor

53
00:02:30,990 --> 00:02:32,664
de una década resolver todos esos
problemas.

54
00:02:12,720 --> 00:02:12,952
Sí.

55
00:02:15,240 --> 00:02:15,379
Bien

56
00:02:26,560 --> 00:02:26,931
Claro.

57
00:02:32,700 --> 00:02:36,043
Qué interesante. Así que, eh, co- como un
podcaster profesional y co-

58
00:02:37,900 --> 00:02:42,732
Para un espectador de la IA desde lejos,
es fácil para mí identificar, como, "Oh,

59
00:02:42,732 --> 00:02:46,599
esto es lo que falta, falta el
aprendizaje continuo", o "Falta la

60
00:02:46,599 --> 00:02:51,371
multimodalidad". Pero realmente no tengo
una buena forma de intentar ponerle una

61
00:02:51,371 --> 00:02:52,157
fecha límite.

62
00:02:52,400 --> 00:02:52,585
Mmm.

63
00:02:52,600 --> 00:02:56,112
Como, si alguien dice, "¿Cuánto tiempo
tomará el aprendizaje continuo?" Yo... No

64
00:02:56,112 --> 00:02:59,624
tengo, como, una idea previa de que "Este
es un proyecto que debería durar cinco

65
00:02:59,624 --> 00:03:03,092
años, diez años, cincuenta años." ¿Por
qué una década? ¿Por qué no un año? ¿Por

66
00:03:03,092 --> 00:03:04,070
qué no cincuenta años?

67
00:03:00,600 --> 00:03:01,017
Ajá.

68
00:03:04,400 --> 00:03:07,628
Pues, sí, supongo que aquí es donde
entras, como, un poco en mi propia

69
00:03:07,628 --> 00:03:11,138
intuición, y también... haciendo una
pequeña extrapolación con respecto a mi

70
00:03:11,138 --> 00:03:14,694
propia experiencia en el campo, ¿verdad?
Así que, supongo que he estado en IA

71
00:03:14,694 --> 00:03:18,157
durante casi dos décadas. Quiero decir,
van a ser, quizás 15 años o así. No

72
00:03:18,157 --> 00:03:21,293
tanto. Eh, tuviste a Richard Sutton
aquí... que estuvo por aquí, por

73
00:03:21,293 --> 00:03:22,743
supuesto, por mucho más tiempo.

74
00:03:22,960 --> 00:03:26,093
Pero sí tengo unos 15 años de experiencia
de gente haciendo predicciones, de ver

75
00:03:26,093 --> 00:03:28,908
cómo realmente, uh, resultaron. Y
también, estuve en la industria por un

76
00:03:28,908 --> 00:03:31,922
tiempo y estuve... en investigación, y
trabajé en la industria por un tiempo.

77
00:03:31,960 --> 00:03:36,453
Así que, supongo que me queda, uh, una
intuición general de eso. Uh, y, uh,

78
00:03:36,453 --> 00:03:40,704
siento que los problemas son, uh,
manejables. Son superables. Pero, uh,

79
00:03:40,704 --> 00:03:44,895
siguen siendo difíciles. Y si lo
promediara, uh, simplemente se siente

80
00:03:44,895 --> 00:03:46,960
como una década, supongo, para mí.

81
00:03:41,360 --> 00:03:41,777
Así es.

82
00:03:46,960 --> 00:03:50,525
E- e- esto es, la verdad, bastante
interesante. Yo, yo quiero, eh, escuchar

83
00:03:50,525 --> 00:03:54,380
no solo la historia, sino también lo que
la gente en la sala... sentía que estaba

84
00:03:54,380 --> 00:03:57,849
a punto de ocurrir en los diferentes
momentos de grandes avances, como...

85
00:03:57,849 --> 00:04:01,511
¿cuáles fueron las formas en que sus
sentimientos eran demasiado pesimistas o

86
00:04:01,511 --> 00:04:04,932
demasiado optimistas? Sí, tal vez...
¿deberíamos repasarlos uno por uno?

87
00:04:03,060 --> 00:04:03,338
Claro.

88
00:04:04,960 --> 00:04:08,783
Sí, bueno. Es una pregunta enorme, porque
hablamos de quince años de cosas que

89
00:04:08,783 --> 00:04:12,159
pasaron. La IA es, de hecho, tan
maravillosa porque ha habido varios,

90
00:04:12,159 --> 00:04:16,032
diría yo, cambios sísmicos que hicieron
que todo el campo, de repente, se viera

91
00:04:16,032 --> 00:04:19,309
de otra forma. ¿Verdad? Y supongo que he
vivido dos o tres de esos.

92
00:04:12,600 --> 00:04:12,692
Sí

93
00:04:19,399 --> 00:04:19,724
Claro.

94
00:04:19,720 --> 00:04:23,009
Y sigo pensando que seguirá habiendo
algunos, porque vienen con una especie de

95
00:04:23,009 --> 00:04:26,255
regularidad casi sorprendente. Bueno,
cuando mi carrera empezó, claro, cuando

96
00:04:26,255 --> 00:04:29,074
empecé a trabajar en aprendizaje
profundo, cuando me interesé en el

97
00:04:29,074 --> 00:04:32,363
aprendizaje profundo, esto fue como por
casualidad, por estar justo al lado de

98
00:04:32,363 --> 00:04:35,439
Geoff Hinton en la Universidad de
Toronto. Y Geoff Hinton, claro, es como

99
00:04:35,439 --> 00:04:36,763
la figura del padrino de la IA.

100
00:04:36,940 --> 00:04:40,291
Y él estaba entrenando todas estas redes
neuronales, y me pareció increíble e

101
00:04:40,291 --> 00:04:43,731
interesante. Pero esto no era, digamos,
lo principal que todos en la IA estaban

102
00:04:43,731 --> 00:04:47,260
haciendo ni de lejos. Esto era un pequeño
tema de nicho, al margen. Eso fue como,

103
00:04:47,260 --> 00:04:50,259
quizás, el primer, digamos, cambio
sísmico dramático... que llegó con

104
00:04:50,259 --> 00:04:50,964
AlexNet y demás.

105
00:04:43,340 --> 00:04:43,618
Claro.

106
00:04:51,580 --> 00:04:55,271
Diría que AlexNet, en cierto modo,
reorientó a todos, y todos empezaron a

107
00:04:55,271 --> 00:04:59,322
entrenar redes neuronales. Pero seguía
siendo, como, muy... por tarea, por tarea

108
00:04:59,322 --> 00:05:03,168
específica. Así que, quizás tengo un
clasificador de imágenes o un traductor

109
00:05:03,168 --> 00:05:06,911
automático neuronal o algo así. Y la
gente, muy lentamente, se interesó en

110
00:05:06,911 --> 00:05:07,833
agentes, diría yo.

111
00:05:08,780 --> 00:05:11,666
Y la gente empezó a preguntarse, "Bueno,
quizás ya tenemos una marca de

112
00:05:11,666 --> 00:05:14,965
verificación junto a la corteza visual o
algo por el estilo, pero ¿qué hay de las

113
00:05:14,965 --> 00:05:18,223
otras partes del cerebro y cómo podemos
obtener un agente o una entidad completa

114
00:05:18,223 --> 00:05:21,316
que realmente pueda interactuar en el
mundo?" Y yo diría que el cambio en el

115
00:05:21,316 --> 00:05:24,244
aprendizaje por refuerzo profundo de
Atari, eh, alrededor de 2013 o así—

116
00:05:24,640 --> 00:05:28,000
uh, fue parte de ese esfuerzo inicial de
agentes, para mí, porque fue un intento

117
00:05:28,000 --> 00:05:30,978
de conseguir agentes que no solo
percibieran el mundo, sino que también

118
00:05:30,978 --> 00:05:33,998
actuaran e interactuaran- ... y
obtuvieran recompensas del entorno. Y en

119
00:05:33,998 --> 00:05:35,785
ese momento, eran juegos de Atari,
¿verdad?

120
00:05:36,600 --> 00:05:40,004
Y, en cierto modo, siento que eso fue un
paso en falso, en realidad. Y fue un paso

121
00:05:40,004 --> 00:05:43,198
en falso que, de hecho, incluso el primer
OpenAI, del que yo formé parte, por

122
00:05:43,198 --> 00:05:46,434
supuesto, uh, como que adoptó porque en
aquel momento, el espíritu de la época

123
00:05:46,434 --> 00:05:49,838
era el de los entornos de aprendizaje por
refuerzo, juegos, jugar a juegos, vencer

124
00:05:49,838 --> 00:05:53,243
juegos, había muchos tipos diferentes de
juegos, y OpenAI estaba haciendo mucho de

125
00:05:53,243 --> 00:05:53,411
eso.

126
00:05:53,960 --> 00:05:57,809
Así que, esa fue quizás, como, otra parte
prominente de, diría yo, la IA donde

127
00:05:57,809 --> 00:06:01,508
quizás por dos o tres o cuatro años,
todos estaban haciendo aprendizaje por

128
00:06:01,508 --> 00:06:02,458
refuerzo en juegos.

129
00:06:02,480 --> 00:06:03,362
Mmm.

130
00:06:03,400 --> 00:06:06,978
Y, eh, básicamente fue un pequeño paso en
falso. Y lo que intentaba hacer en

131
00:06:06,978 --> 00:06:10,557
OpenAI, en realidad, es que siempre
sospeché un poco de los juegos como algo

132
00:06:10,557 --> 00:06:12,084
que realmente llevaría a la IAG.

133
00:06:12,128 --> 00:06:16,238
Porque en mi mente, se busca algo como un
contable o, eh, algo que de verdad

134
00:06:16,238 --> 00:06:20,677
interactúe con la realidad, y no entendía
cómo los videojuegos, digamos, encajaban

135
00:06:20,677 --> 00:06:24,678
ahí. Así que, mi proyecto en OpenAI, por
ejemplo, era, eh, en el marco del

136
00:06:24,678 --> 00:06:28,898
Proyecto Universe, sobre un agente que
utilizaba teclado y ratón para manejar,

137
00:06:28,898 --> 00:06:29,775
eh, páginas web.

138
00:06:29,788 --> 00:06:30,020
Mm.

139
00:06:30,008 --> 00:06:33,175
Y realmente quería tener algo que, por
ejemplo, interactúe con, sabes, el mundo

140
00:06:33,175 --> 00:06:36,465
digital real- ... que pueda hacer trabajo
de conocimiento. Y resulta que, eh, esto

141
00:06:36,465 --> 00:06:39,349
era extremadamente temprano, demasiado
temprano. Tan temprano que la- no

142
00:06:39,349 --> 00:06:41,339
deberíamos haber estado trabajando en
eso, ¿sabes?

143
00:06:38,647 --> 00:06:38,972
Ajá.

144
00:06:41,888 --> 00:06:45,727
Eh, porque, uh, si solo estás tanteando a
ciegas, aporreando el teclado y haciendo

145
00:06:45,727 --> 00:06:49,235
clic con el ratón, intentando obtener
recompensas en estos entornos, uh, tu

146
00:06:49,235 --> 00:06:52,790
recompensa será demasiado escasa y
simplemente no aprenderás, y vas a quemar

147
00:06:52,790 --> 00:06:56,582
un bosque, uh, computando. Y nunca vas a
lograr que algo despegue realmente. Y lo

148
00:06:56,582 --> 00:07:00,185
que te está faltando es este, uh, poder
de representación en la red neuronal.

149
00:06:53,967 --> 00:06:54,106
Sí.

150
00:07:00,227 --> 00:07:00,644
Así es.

151
00:07:00,907 --> 00:07:03,622
Y así, por ejemplo, hoy, la gente entrena
a esos agentes informáticos, pero lo

152
00:07:03,622 --> 00:07:06,372
hacen sobre un gran modelo de lenguaje, y
así tienes que conseguir el modelo de

153
00:07:06,372 --> 00:07:09,052
lenguaje primero. Tienes que conseguir
las representaciones primero, y tienes

154
00:07:09,052 --> 00:07:11,309
que hacer eso con todo el
pre-entrenamiento y todo lo de los LLM.

155
00:07:11,768 --> 00:07:14,768
Así que, en cierto modo, tengo la
sensación de que, en términos generales,

156
00:07:14,768 --> 00:07:17,891
era como si la gente siguiera intentando
conseguirlo todo demasiado pronto en

157
00:07:17,891 --> 00:07:20,974
varias ocasiones, donde la gente, por
ejemplo, realmente intenta ir tras los

158
00:07:20,974 --> 00:07:24,220
agentes demasiado pronto, diría yo. Y eso
fue Atari y Universe, eh, e incluso mi

159
00:07:24,220 --> 00:07:27,549
propia experiencia. Y en realidad hay que
hacer algunas cosas primero antes de que

160
00:07:27,549 --> 00:07:29,275
lleguemos, por así decirlo, a esos
agentes.

161
00:07:29,948 --> 00:07:33,630
Eh, y quizás ahora los agentes son mucho
más competentes, pero quizás todavía nos

162
00:07:33,630 --> 00:07:37,221
faltan, eh, algunas partes, eh, de esa
pila. Pero yo diría que quizás esos son,

163
00:07:37,221 --> 00:07:40,950
eh, los tres grandes grupos de lo que la
gente estaba haciendo. Eh, entrenar redes

164
00:07:40,950 --> 00:07:44,632
neuronales, eh, para cada tarea, probando
la primera generación de agentes, eh, y

165
00:07:44,632 --> 00:07:48,315
luego quizás los LLM y buscando realmente
el poder de representación de las redes

166
00:07:48,315 --> 00:07:51,031
neuronales antes de que, eh, le añadas
todo lo demás encima.

167
00:07:51,087 --> 00:07:54,551
Interesante. Sí, supongo que si tuviera
que darle la mejor interpretación a la

168
00:07:54,551 --> 00:07:57,926
perspectiva de Sutton, sería que los
seres humanos pueden con todo a la vez,

169
00:07:57,926 --> 00:08:01,526
¿verdad? Incluso los animales pueden con
todo a la vez, ¿verdad? Los animales son

170
00:08:01,526 --> 00:08:04,451
quizás un mejor ejemplo porque ni
siquiera tienen el andamiaje del

171
00:08:04,451 --> 00:08:07,871
lenguaje. Simplemente son lanzados al
mundo y tienen que darle sentido a todo

172
00:08:07,871 --> 00:08:08,501
sin etiquetas.

173
00:08:03,467 --> 00:08:03,792
Ajá.

174
00:08:08,748 --> 00:08:11,879
Eh- ... y la visión para la IAG debería
ser algo que, por ejemplo, solo mire

175
00:08:11,879 --> 00:08:15,052
datos sensoriales- ... mire la pantalla
del ordenador, y que simplemente, por

176
00:08:15,052 --> 00:08:18,350
ejemplo, entienda lo que pasa desde cero.
Es decir, si un humano fuera puesto en

177
00:08:18,350 --> 00:08:21,397
una situación similar, tuviera que ser
entrenado desde cero, bueno, quiero

178
00:08:21,397 --> 00:08:24,737
decir, esto es como un humano creciendo o
un animal creciendo. Entonces, ¿por qué

179
00:08:24,737 --> 00:08:27,994
no debería ser esa la visión para la IA
en lugar de, por ejemplo, esto de hacer

180
00:08:27,994 --> 00:08:29,413
millones de años de entrenamiento?

181
00:08:14,207 --> 00:08:14,346
Sí

182
00:08:29,467 --> 00:08:33,013
Sí. Creo que esa es una muy buena
pregunta, y creo que, eh... Quiero decir,

183
00:08:33,013 --> 00:08:36,751
Sutton estuvo en tu podcast... y yo vi el
podcast, y tenía un escrito sobre ese

184
00:08:36,751 --> 00:08:39,962
podcast... que casi que se adentra un
poco en cómo veo yo las cosas.

185
00:08:34,508 --> 00:08:34,740
Sí.

186
00:08:40,127 --> 00:08:43,651
Y la verdad es que siento que soy muy
cuidadoso al hacer analogías con

187
00:08:43,651 --> 00:08:47,789
animales... porque se originaron mediante
un proceso de optimización muy distinto.

188
00:08:44,588 --> 00:08:44,820
Mm

189
00:08:48,068 --> 00:08:51,842
Los animales han evolucionado, y de hecho
vienen con una gran cantidad de hardware

190
00:08:51,842 --> 00:08:55,571
de serie. Eh, y cuando... por ejemplo, mi
ejemplo en la publicación fue la cebra.

191
00:08:55,571 --> 00:08:59,159
Una cebra nace, y unos minutos después,
está corriendo y siguiendo a su madre.

192
00:08:59,159 --> 00:09:01,303
Eso es algo extremadamente complicado de
hacer.

193
00:09:01,387 --> 00:09:01,897
Sí.

194
00:09:02,127 --> 00:09:04,727
Mmm, eso no es aprendizaje por refuerzo.
Eso es intrínseco.

195
00:09:04,768 --> 00:09:08,551
Y la evolución obviamente tiene alguna
forma de codificar los pesos de nuestras

196
00:09:08,551 --> 00:09:12,480
redes neuronales en ATCG, y no tengo idea
de cómo funciona eso, pero aparentemente

197
00:09:12,480 --> 00:09:16,021
funciona. Así que, siento que, uh, los
cerebros simplemente vinieron de un

198
00:09:16,021 --> 00:09:19,562
proceso muy diferente, y yo- yo dudo
mucho en inspirarme en ello porque en

199
00:09:19,562 --> 00:09:22,957
realidad no estamos ejecutando ese
proceso. Así que, en mi publicación,

200
00:09:22,957 --> 00:09:25,480
dije: "En realidad no estamos
construyendo animales".

201
00:09:25,627 --> 00:09:29,071
Eh, estamos construyendo fantasmas... o,
digamos, espíritus, o lo que la gente

202
00:09:29,071 --> 00:09:32,516
quiera llamar a eso, eh, porque, mmm, no
estamos, eh, no estamos realizando un

203
00:09:32,516 --> 00:09:34,171
entrenamiento basado en la evolución.

204
00:09:26,887 --> 00:09:27,072
Sí.

205
00:09:34,367 --> 00:09:37,238
Eh, estamos haciendo el entrenamiento
básicamente por imitación de los

206
00:09:37,238 --> 00:09:40,568
humanos... y los datos que han subido a
internet. Y así terminas con estas, como,

207
00:09:40,568 --> 00:09:43,440
una especie de entidades espirituales
etéreas porque son completamente

208
00:09:43,440 --> 00:09:46,644
digitales, y están, como, imitando a los
humanos. Y es un tipo de inteligencia

209
00:09:46,644 --> 00:09:49,974
diferente. Como, si imaginas un espacio
de inteligencias, estamos partiendo de un

210
00:09:49,974 --> 00:09:50,806
punto casi distinto.

211
00:09:37,288 --> 00:09:37,520
Sí

212
00:09:50,848 --> 00:09:53,862
No estamos, no estamos realmente
construyendo animales, pero creo que

213
00:09:53,862 --> 00:09:56,876
también es posible hacerlos más
animalescos con el tiempo. Y creo que

214
00:09:56,876 --> 00:10:00,467
deberíamos estar haciendo eso. Y entonces
siento que... Perdón, solo un punto más,

215
00:10:00,467 --> 00:10:04,102
solo- Siento que Sutton básicamente tiene
un muy... Su marco es, queremos construir

216
00:10:04,102 --> 00:10:04,501
animales.

217
00:09:55,408 --> 00:09:55,593
Mm.

218
00:10:04,508 --> 00:10:04,740
Sí.

219
00:10:04,748 --> 00:10:07,114
Y de hecho creo que eso sería
maravilloso. Si logramos que eso

220
00:10:07,114 --> 00:10:10,139
funcione, sería realmente increíble. Si
existiera un único, como, algoritmo que

221
00:10:10,139 --> 00:10:13,203
pudieras simplemente, sabes, ejecutarlo
en la red y que aprendiera absolutamente

222
00:10:13,203 --> 00:10:14,871
todo, eso... sería verdaderamente
increíble.

223
00:10:15,028 --> 00:10:18,765
Casi sospecho que no estoy realmente
seguro de que exista, y eso ciertamente

224
00:10:18,765 --> 00:10:22,703
no es lo que hacen los animales porque
los animales tienen este bucle externo de

225
00:10:22,703 --> 00:10:23,201
evolución.

226
00:10:23,548 --> 00:10:23,733
Bien.

227
00:10:23,708 --> 00:10:26,847
Y mucho de lo que parece aprendizaje es
en realidad mucha más maduración del

228
00:10:26,847 --> 00:10:30,071
cerebro, y creo que en realidad hay muy
poco aprendizaje por refuerzo para los

229
00:10:30,071 --> 00:10:33,169
animales. Y creo que gran parte del
aprendizaje por refuerzo es en realidad

230
00:10:33,169 --> 00:10:36,559
más como tareas motoras. No son tareas de
inteligencia. Así que, en realidad, creo

231
00:10:36,559 --> 00:10:39,615
que los humanos no usan realmente el
aprendizaje por refuerzo, hablando en

232
00:10:39,615 --> 00:10:41,122
términos generales, es lo que diría.

233
00:10:41,147 --> 00:10:43,837
¿Puedes repetir la última frase? Mucha de
esa inteligencia no es motora, ¿qué es?

234
00:10:43,837 --> 00:10:44,072
Perdón.

235
00:10:44,228 --> 00:10:47,822
Mu- mucha parte del aprendizaje por
refuerzo, desde mi perspectiva, serían

236
00:10:47,822 --> 00:10:51,811
cosas mucho más, como, motoras, como, eh,
tareas simples, como lanzar un aro, algo

237
00:10:51,811 --> 00:10:55,603
así. Pero no creo que los humanos usen el
aprendizaje por refuerzo para muchas

238
00:10:55,603 --> 00:10:58,903
tareas de inteligencia, como la
resolución de problemas y cosas así.

239
00:10:50,407 --> 00:10:50,546
Sí.

240
00:10:51,768 --> 00:10:51,907
Sí.

241
00:10:58,928 --> 00:10:59,578
Interesante.

242
00:10:59,588 --> 00:11:03,922
Eso no significa que no debamos hacerlo
para investigación, pero siento que es lo

243
00:11:03,922 --> 00:11:05,439
que los animales hacen o no.

244
00:11:06,028 --> 00:11:09,750
Sí. Voy a tomarme un segundo para digerir
eso, porque hay muchas... ideas

245
00:11:09,750 --> 00:11:13,266
diferentes. Quizás una pregunta
aclaratoria que puedo hacer para, eh,

246
00:11:13,266 --> 00:11:17,454
entender mejor la perspectiva. Entonces,
creo que sugieres que, mira, la evolución

247
00:11:17,454 --> 00:11:21,332
está haciendo el tipo de cosa que hace el
pre-entrenamiento en el sentido de

248
00:11:21,332 --> 00:11:23,814
construir algo que luego pueda entender
el mundo.

249
00:11:24,068 --> 00:11:28,340
La diferencia, supongo, es que la
evolución, en el caso de los seres

250
00:11:28,340 --> 00:11:32,676
humanos, debe ser ajustada con una
precisión extrema a través de tres

251
00:11:32,676 --> 00:11:36,885
gigabytes de ADN, lo cual es muy distinto
a los pesos de un modelo.

252
00:11:37,127 --> 00:11:40,730
Quiero decir, literalmente, los pesos del
modelo son un cerebro, lo cual obviamente

253
00:11:40,730 --> 00:11:44,334
no está codificado en el- el esperma y el
óvu- o sea, eso no existe en el esperma y

254
00:11:44,334 --> 00:11:46,136
el óvulo, así que tiene que ser
cultivado.

255
00:11:46,188 --> 00:11:49,730
Y además, la información de cada sinapsis
en el cerebro simplemente no puede

256
00:11:49,730 --> 00:11:53,556
existir en los tres gigabytes que existen
en el ADN. La evolución parece estar más

257
00:11:53,556 --> 00:11:57,193
cerca de encontrar el algoritmo... que
luego realiza el aprendizaje a lo largo

258
00:11:57,193 --> 00:12:00,924
de la vida. Ahora, quizás el aprendizaje
a lo largo de la vida no sea análogo al

259
00:12:00,924 --> 00:12:04,514
RL, según tu punto de vista. ¿Es eso
compatible con lo que decías, o estarías

260
00:12:04,514 --> 00:12:05,553
en desacuerdo con eso?

261
00:11:58,308 --> 00:11:58,586
Ajá.

262
00:12:05,588 --> 00:12:08,158
Creo que sí. Estoy de acuerdo contigo en
que hay una compresión milagrosa

263
00:12:08,158 --> 00:12:11,085
ocurriendo... porque obviamente los pesos
de la red neuronal no se almacenan en los

264
00:12:11,085 --> 00:12:11,300
ATCGs.

265
00:12:11,308 --> 00:12:11,447
Sí.

266
00:12:11,748 --> 00:12:14,814
Existe una especie de compresión
dramática, y hay, digamos, algoritmos de

267
00:12:14,814 --> 00:12:18,178
aprendizaje codificados que se encargan
de hacer parte del aprendizaje en línea.

268
00:12:18,178 --> 00:12:20,478
Por eso, estoy completamente de acuerdo
contigo en eso.

269
00:12:20,608 --> 00:12:23,818
Uh, básicamente, diría que soy mucho más,
digamos, de mentalidad práctica. No lo

270
00:12:23,818 --> 00:12:26,662
abordo desde la perspectiva de
"construyamos animales". Vengo de la, de

271
00:12:26,662 --> 00:12:29,791
la perspectiva de "construyamos cosas que
sean útiles". Así que llevo un casco

272
00:12:29,791 --> 00:12:32,717
puesto, y solo estoy observando que,
miren, no vamos a hacer evolución...

273
00:12:32,717 --> 00:12:33,936
porque no sé cómo se hace eso.

274
00:12:34,664 --> 00:12:38,170
uh, pero resulta que podemos construir
estas entidades fantasma, uh, tipo

275
00:12:38,170 --> 00:12:41,823
espíritu... imitando documentos de
internet. Esto funciona. Y en realidad es

276
00:12:41,823 --> 00:12:45,719
como, es una forma de llevarte a algo que
tiene mucho conocimiento e inteligencia

277
00:12:45,719 --> 00:12:49,079
incorporados de alguna manera, uh,
similar a lo que quizás ha hecho la

278
00:12:49,079 --> 00:12:52,878
evolución. Por eso, de alguna manera,
llamo al pre-entrenamiento una especie de

279
00:12:52,878 --> 00:12:53,657
evolución cutre.

280
00:12:53,843 --> 00:12:57,326
Es como la versión prácticamente posible
con nuestra tecnología y lo que tenemos

281
00:12:57,326 --> 00:13:00,545
disponible para llegar a un punto de
partida donde realmente podamos hacer

282
00:13:00,545 --> 00:13:02,573
cosas como el aprendizaje por refuerzo y
demás.

283
00:13:02,664 --> 00:13:05,945
Ajá. Solo para reforzar, la otra
perspectiva, porque después de hacer esta

284
00:13:05,945 --> 00:13:09,496
entrevista y pensarlo un poco, aquí hay
un punto importante. La evolución no nos

285
00:13:09,496 --> 00:13:12,688
da el conocimiento en sí, ¿verdad? Nos da
el algoritmo para encontrar el

286
00:13:12,688 --> 00:13:15,295
conocimiento. Y eso parece diferente del
pre-entrenamiento.

287
00:13:15,324 --> 00:13:18,367
Entonces, si- quizás la perspectiva sea
que el pre-entrenamiento contribuye a

288
00:13:18,367 --> 00:13:21,450
formar el tipo de entidad que es capaz de
aprender de una forma más eficiente.

289
00:13:21,450 --> 00:13:24,172
Enseña meta-aprendizaje, y por
consiguiente, es algo muy similar a lo

290
00:13:24,172 --> 00:13:25,494
que sería encontrar un algoritmo.

291
00:13:21,524 --> 00:13:21,756
Mmm.

292
00:13:25,504 --> 00:13:25,875
Ajá.

293
00:13:26,144 --> 00:13:29,142
Um, pero si la evolución nos da
conocimiento, y el pre-entrenamiento

294
00:13:29,142 --> 00:13:31,066
también, no son... Esa analogía no
funciona.

295
00:13:31,084 --> 00:13:34,355
Sí, sí. Es sutil, y creo que tienes razón
en oponerte, pero básicamente, lo que el

296
00:13:34,355 --> 00:13:37,264
pre-entrenamiento hace... Entonces,
básicamente obtienes el predictor del

297
00:13:37,264 --> 00:13:37,910
siguiente token-

298
00:13:37,983 --> 00:13:41,016
A través de internet y estás entrenando
eso en una red neuronal. En realidad,

299
00:13:41,016 --> 00:13:43,650
está haciendo dos cosas que no están
relacionadas. Número uno, está

300
00:13:43,650 --> 00:13:46,843
adquiriendo todo este conocimiento, como
yo lo llamo. Número dos, en realidad, se

301
00:13:46,843 --> 00:13:47,921
está volviendo inteligente.

302
00:13:47,983 --> 00:13:48,400
Ajá.

303
00:13:48,464 --> 00:13:51,240
Mmm, al observar los patrones
algorítmicos de internet, en realidad,

304
00:13:51,240 --> 00:13:54,265
como que activa todos estos pequeños
circuitos y algoritmos dentro- la red

305
00:13:54,265 --> 00:13:57,332
neuronal para hacer cosas como el
aprendizaje en contexto- y todo este tipo

306
00:13:57,332 --> 00:13:57,705
de cosas.

307
00:13:56,584 --> 00:13:56,723
Sí

308
00:13:57,944 --> 00:14:01,276
Y en realidad, no necesitas ni quieres el
conocimiento. De hecho, creo que eso

309
00:14:01,276 --> 00:14:04,652
probablemente está frenando las redes
neuronales en general, porque en realidad

310
00:14:04,652 --> 00:14:07,639
es como hacer que dependan del
conocimiento un poco demasiado a veces.

311
00:14:07,639 --> 00:14:11,058
Por ejemplo, siento que los agentes, algo
en lo que no son muy buenos es salirse

312
00:14:11,058 --> 00:14:13,222
del colector de datos de lo que existe en
internet.

313
00:14:11,624 --> 00:14:11,902
Ajá

314
00:14:13,584 --> 00:14:16,834
Si tuvieran menos conocimiento o
memoria... quizás serían mejores. Y lo

315
00:14:16,834 --> 00:14:20,224
que creo que debemos hacer de ahora en
adelante, y esto sería parte de los

316
00:14:20,224 --> 00:14:22,871
paradigmas de investigación, es que
necesitamos empezar...

317
00:14:17,424 --> 00:14:17,656
Sí.

318
00:14:22,964 --> 00:14:26,518
Eh, necesitamos encontrar formas de
eliminar parte del conocimiento y de

319
00:14:26,518 --> 00:14:30,072
mantener lo que yo llamo este nú- eh,
este núcleo cognitivo. Es como una

320
00:14:30,072 --> 00:14:33,826
entidad inteligente que está despojada de
conocimiento pero que contiene los

321
00:14:33,826 --> 00:14:37,680
algoritmos y contiene la magia de la
inteligencia, la resolución de problemas,

322
00:14:37,680 --> 00:14:39,032
sus estrategias y todo eso.

323
00:14:28,444 --> 00:14:28,769
Ajá.

324
00:14:39,384 --> 00:14:43,729
Hay tantas cosas interesantes ahí. Bien,
empecemos con el aprendizaje en contexto.

325
00:14:43,729 --> 00:14:47,699
Este es un punto obvio, pero creo que
vale la pena decirlo explícitamente y

326
00:14:47,699 --> 00:14:48,718
meditar sobre ello.

327
00:14:48,844 --> 00:14:52,235
La situación en la que estos modelos
parecen más inteligentes, en la que es

328
00:14:52,235 --> 00:14:55,764
como, les hablo y digo, "Guau, realmente
hay algo al otro lado que me responde

329
00:14:55,764 --> 00:14:59,155
pensando en cosas." Si, por ejemplo,
comete un error, es como, "Oh, espera,

330
00:14:59,155 --> 00:15:02,455
esa no es la forma correcta de pensarlo,
me retracto." Todo eso ocurre en

331
00:15:02,455 --> 00:15:05,709
contexto. Ahí es donde siento que la
verdadera inteligencia se puede ver

332
00:15:05,709 --> 00:15:06,305
visiblemente.

333
00:15:06,384 --> 00:15:06,709
Claro.

334
00:15:06,744 --> 00:15:10,770
Y ese proceso de aprendizaje en contexto
se desarrolla mediante el descenso de

335
00:15:10,770 --> 00:15:12,966
gradiente en el pre-entrenamiento,
¿verdad?

336
00:15:13,004 --> 00:15:16,995
Como que meta... Aprende meta-aprendizaje
en contexto de forma espontánea, pero el

337
00:15:16,995 --> 00:15:21,035
aprendizaje en contexto en sí mismo no es
descenso de gradiente. De la misma manera

338
00:15:21,035 --> 00:15:24,779
que nuestra inteligencia de por vida como
humanos para poder hacer cosas está

339
00:15:24,779 --> 00:15:28,475
condicionada por la evolución, pero
nuestro aprendizaje real durante nuestra

340
00:15:28,475 --> 00:15:29,165
vida es, como-

341
00:15:29,384 --> 00:15:30,684
ocurriendo por otro proceso.

342
00:15:30,964 --> 00:15:32,960
No concuerdo del todo, pero sigue con tu
idea.

343
00:15:32,964 --> 00:15:36,261
Ah, vale. De hecho, tengo mucha
curiosidad por ver cómo falla esa

344
00:15:36,261 --> 00:15:36,725
analogía.

345
00:15:36,784 --> 00:15:40,030
Creo que dudo en decir que el aprendizaje
en contexto no hace descenso de

346
00:15:40,030 --> 00:15:43,457
gradiente, eh, porque... no hace un
descenso de gradiente explícito. Pero aún

347
00:15:43,457 --> 00:15:46,794
así creo que... el aprendizaje en
contexto es, básicamente, completación de

348
00:15:46,794 --> 00:15:47,650
patrones dentro de-

349
00:15:47,664 --> 00:15:47,896
Mmm

350
00:15:48,044 --> 00:15:51,383
eh, una ventana de tokens, ¿verdad? Y
resulta que hay una enorme cantidad de

351
00:15:51,383 --> 00:15:54,855
patrones en internet. Y tienes razón, el
modelo como que aprende a completar el

352
00:15:54,855 --> 00:15:57,927
patrón. Y eso está en los pesos. Los
pesos de la red neuronal intentan

353
00:15:57,927 --> 00:16:01,311
descubrir patrones y completar el patrón.
Y hay una especie de adaptación que

354
00:16:01,311 --> 00:16:03,136
ocurre dentro de la red neuronal,
¿verdad?

355
00:15:54,364 --> 00:15:54,642
Claro.

356
00:15:59,764 --> 00:16:00,089
Ajá.

357
00:16:03,564 --> 00:16:07,019
Eh, que es algo mágico y simplemente
surge de... internet, simplemente porque

358
00:16:07,019 --> 00:16:07,929
hay muchos patrones.

359
00:16:08,084 --> 00:16:11,698
Diré que ha habido algunos artículos que
me parecieron interesantes que realmente

360
00:16:11,698 --> 00:16:15,086
analizan los mecanismos detrás del
aprendizaje en contexto. Y sí creo que es

361
00:16:15,086 --> 00:16:18,565
posible que el aprendizaje en contexto
ejecute un pequeño bucle de descenso de

362
00:16:18,565 --> 00:16:21,862
gradiente, uh, internamente en las capas
de la red neuronal. Y recuerdo un

363
00:16:21,862 --> 00:16:25,341
artículo en particular donde estaban
haciendo, uh, regresión lineal, de hecho,

364
00:16:25,341 --> 00:16:28,594
usando aprendizaje en contexto. Así que
básicamente tus entradas a la red

365
00:16:28,594 --> 00:16:29,678
neuronal son pares X, Y.

366
00:16:13,664 --> 00:16:13,896
Mmm.

367
00:16:29,704 --> 00:16:30,214
Ajá.

368
00:16:30,684 --> 00:16:34,481
X, Y, X, Y, X, Y que resultan estar en
una línea. Y luego haces X y esperas la

369
00:16:34,481 --> 00:16:38,426
Y. Y la red neuronal, cuando la entrenas
de esta manera, de hecho, hace, eh, hace

370
00:16:38,426 --> 00:16:42,174
regresión lineal. Y, eh, normalmente
cuando ejecutas regresión lineal, tienes

371
00:16:42,174 --> 00:16:45,921
un pequeño optimizador de descenso de
gradiente que, básicamente, eh, mira X,

372
00:16:45,921 --> 00:16:49,916
Y, mira un error... calcula el gradiente
de los pesos y hace la actualización unas

373
00:16:49,916 --> 00:16:50,606
cuantas veces.

374
00:16:33,284 --> 00:16:33,469
Sí.

375
00:16:35,364 --> 00:16:35,503
Sí.

376
00:16:47,544 --> 00:16:47,683
Mmm

377
00:16:50,683 --> 00:16:54,386
Pues resulta que, cuando observaron los
pesos de ese algoritmo de aprendizaje en

378
00:16:54,386 --> 00:16:57,808
contexto, uh, de hecho, encontraron
ciertas analogías con, uh, la mecánica

379
00:16:57,808 --> 00:17:01,511
del descenso de gradiente. De hecho, creo
que el propio artículo fue incluso más

380
00:17:01,511 --> 00:17:04,840
contundente porque codificaron
directamente los pesos de la red neuronal

381
00:17:04,840 --> 00:17:08,262
para llevar a cabo el descenso de
gradiente a través de, uh, la atención y

382
00:17:08,262 --> 00:17:10,559
todos los componentes internos de la red
neuronal.

383
00:17:10,723 --> 00:17:13,499
Así que, supongo que mi única objeción es
que quién sabe cómo funciona el

384
00:17:13,499 --> 00:17:16,584
aprendizaje en contexto, pero en realidad
creo que probablemente esté haciendo un

385
00:17:16,584 --> 00:17:19,284
poco de algún tipo de descenso de
gradiente peculiar internamente y que

386
00:17:19,284 --> 00:17:22,407
creo que eso es, eso es posible. Así que,
supongo que solo estaba objetando que tú

387
00:17:22,407 --> 00:17:25,492
dices que no está haciendo aprendizaje en
contexto. Quién sabe qué está haciendo,

388
00:17:25,492 --> 00:17:28,230
pero probablemente esté haciendo algo
similar a eso, pero no lo sabemos.

389
00:17:28,324 --> 00:17:31,822
Entonces, vale la pena pensar, okay, si
ambos están implementando descenso de

390
00:17:31,822 --> 00:17:35,274
gradiente- perdón, si el aprendizaje en
contexto y el preentrenamiento están

391
00:17:35,274 --> 00:17:38,911
implementando algo como el descenso de
gradiente- ... ¿por qué parece que con el

392
00:17:38,911 --> 00:17:42,639
aprendizaje en contexto estamos llegando
a algo como el aprendizaje continuo, algo

393
00:17:42,639 --> 00:17:46,368
parecido a la inteligencia real, mientras
que no tienes esa sensación análoga solo

394
00:17:46,368 --> 00:17:49,268
con el preentrenamiento? Al menos- ... se
podría argumentar eso.

395
00:17:37,184 --> 00:17:37,276
Bien

396
00:17:48,024 --> 00:17:48,163
Mmm

397
00:17:49,304 --> 00:17:53,506
Y entonces, si es el mismo algoritmo,
¿qué podría ser distinto? Bueno, una

398
00:17:53,506 --> 00:17:57,939
forma de verlo es cuánta información
llega a almacenar el modelo por cada dato

399
00:17:57,939 --> 00:18:01,796
que recibe del entrenamiento. Y si miras
el pre-entrenamiento, si...

400
00:18:01,864 --> 00:18:05,975
Eh, creo que si miras LLaMA 3, por
ejemplo- ... creo que está entrenado con

401
00:18:05,975 --> 00:18:10,532
15 billones de tokens. Y, eh, si miras un
modelo de 70B, eso sería el equivalente a

402
00:18:10,532 --> 00:18:14,866
0.07 bits por token- ... en lo que ve en
el pre-entrenamiento en términos de la

403
00:18:14,866 --> 00:18:18,922
información en los pesos del modelo- ...
comparado con los tokens que lee.

404
00:18:18,922 --> 00:18:20,811
Mientras que si miras la caché KV-

405
00:18:20,844 --> 00:18:21,029
Mmm

406
00:18:21,184 --> 00:18:25,571
y cómo crece por cada token adicional en
el aprendizaje en contexto, es como 320

407
00:18:25,571 --> 00:18:29,681
kilobytes. Así que, esa es una diferencia
de 35 millones de veces en cuánta

408
00:18:29,681 --> 00:18:34,124
información por token es asimilada por el
modelo. Me pregunto si eso es relevante

409
00:18:34,124 --> 00:18:34,790
en absoluto.

410
00:18:26,244 --> 00:18:26,476
Sí.

411
00:18:33,004 --> 00:18:33,468
Sí, ajá.

412
00:18:34,844 --> 00:18:37,665
Creo que estoy bastante de acuerdo.
Quiero decir, la forma en que suelo

413
00:18:37,665 --> 00:18:40,770
expresarlo es que cualquier cosa que
ocurre durante el entrenamiento de la red

414
00:18:40,770 --> 00:18:43,995
neuronal, el conocimiento es solo como un
recuerdo difuso de lo que sucedió en el

415
00:18:43,995 --> 00:18:47,220
tiempo de entrenamiento, y eso es porque
la compresión es drástica. Estás tomando

416
00:18:47,220 --> 00:18:50,203
15 billones de tokens y los estás
comprimiendo a tu red final de unos pocos

417
00:18:50,203 --> 00:18:53,348
miles de millones de parámetros, así que
obviamente, hay una cantidad masiva de

418
00:18:53,348 --> 00:18:53,791
compresión.

419
00:18:54,264 --> 00:18:57,259
Bueno, yo, de alguna manera, me refiero a
ello como una vaga reminiscencia de los

420
00:18:57,259 --> 00:19:00,142
documentos de internet... mientras que
cualquier cosa que ocurre en la ventana

421
00:19:00,142 --> 00:19:02,913
de contexto de la red neuronal, eh,
cuando introduces todos los tokens y se

422
00:19:02,913 --> 00:19:05,609
va construyendo toda esta representación
de caché KV, es muy directamente

423
00:19:05,609 --> 00:19:08,567
accesible para la red neuronal. Así que
comparo la caché KV y aquello que ocurre

424
00:19:08,567 --> 00:19:10,889
en tiempo de prueba con, como, más bien
una memoria de trabajo.

425
00:19:11,224 --> 00:19:14,644
Todo lo que está en la ventana de
contexto es muy directamente accesible a

426
00:19:14,644 --> 00:19:18,204
la red neuronal, así que siempre hay
estas analogías casi sorprendentes entre

427
00:19:18,204 --> 00:19:21,343
LLMs y humanos, y las encuentro algo
sorprendentes porque no estamos

428
00:19:21,343 --> 00:19:24,716
intentando construir un cerebro humano,
claro. Directamente, solo estamos

429
00:19:24,716 --> 00:19:27,152
descubriendo que esto funciona y lo
estamos haciendo.

430
00:19:24,104 --> 00:19:24,196
Sí.

431
00:19:27,384 --> 00:19:31,031
Pero sí creo que cualquier cosa que esté
en los pesos, es como un recuerdo borroso

432
00:19:31,031 --> 00:19:34,589
de lo que leíste hace un año. Cualquier
cosa que le des como contexto, eh, en el

433
00:19:34,589 --> 00:19:38,281
momento de la prueba está directamente en
la memoria de trabajo, eh, y creo que esa

434
00:19:38,281 --> 00:19:41,614
es una analogía muy poderosa para
reflexionar. Así que cuando, por ejemplo,

435
00:19:41,614 --> 00:19:45,036
vas a un LLM y le preguntas sobre, eh,
algún libro y lo que pasó en él, como,

436
00:19:45,036 --> 00:19:46,703
eh, el libro de Nick Lane o algo así-

437
00:19:47,223 --> 00:19:50,261
El LLM a menudo te dará cosas que son más
o menos correctas, pero si le das el

438
00:19:50,261 --> 00:19:53,220
capítulo completo y le haces preguntas,
conseguirás resultados mucho mejores

439
00:19:53,220 --> 00:19:55,943
porque ahora está cargado en la memoria
de trabajo del modelo. Así que

440
00:19:55,943 --> 00:19:58,981
básicamente estoy de acuerdo con tu forma
tan extensa de decir que estoy más o

441
00:19:58,981 --> 00:20:00,086
menos de acuerdo, y por eso.

442
00:20:00,084 --> 00:20:04,838
Mm-hmm. Dando un paso atrás, ¿qué parte
de la inteligencia humana hemos fallado

443
00:20:04,838 --> 00:20:06,910
más en replicar con estos modelos?

444
00:20:08,504 --> 00:20:13,055
Mmm, casi siento que, mmm, es que, uh,
es... es mucho de ello,

445
00:20:14,384 --> 00:20:18,009
Aún. Entonces, quizás una forma de
pensarlo, no sé si esta es la, la mejor

446
00:20:18,009 --> 00:20:21,784
manera, pero casi siento que, de nuevo,
haciendo estas analogías, imperfectas

447
00:20:21,784 --> 00:20:25,459
como son, eh, nos hemos topado con la red
neuronal transformadora, que, eh,

448
00:20:25,459 --> 00:20:29,283
extremadamente potente, muy general.
Puedes entrenar transformadores con audio

449
00:20:29,283 --> 00:20:32,959
o video o texto o lo que quieras, y
simplemente aprende patrones, y son muy

450
00:20:32,959 --> 00:20:34,399
potentes y funciona muy bien.

451
00:20:35,384 --> 00:20:39,241
Eso, para mí, casi indica que esto es
como un trozo de tejido cortical. Eh, es

452
00:20:39,241 --> 00:20:42,799
algo así, porque la corteza es
famosamente muy, eh, plástica también. Se

453
00:20:42,799 --> 00:20:46,857
pueden reconectar, eh, sabes, partes del
cerebro, y, eh, hubo experimentos un poco

454
00:20:46,857 --> 00:20:50,865
espantosos de reconectar, por ejemplo, la
corteza visual a la corteza auditiva y,

455
00:20:50,865 --> 00:20:53,170
eh, este animal, pues, aprendió bien,
etcétera.

456
00:20:53,624 --> 00:20:56,905
Pues, creo que esto es como tejido
cortical. Creo que cuando estamos

457
00:20:56,905 --> 00:21:00,578
haciendo razonamiento y planificación
dentro de las redes neuronales, o sea,

458
00:21:00,578 --> 00:21:04,006
básicamente realizando rastros de
razonamiento, eh, para los modelos de

459
00:21:04,006 --> 00:21:07,581
pensamiento, eso es como la corteza
prefrontal. Eh, y luego, eh, creo que,

460
00:21:07,581 --> 00:21:11,450
uh, quizás esas sean como pequeñas marcas
de verificación, pero aún así creo que

461
00:21:11,450 --> 00:21:15,172
hay muchas, eh, partes del cerebro y
núcleos que todavía no se han explorado.

462
00:21:15,184 --> 00:21:18,328
Quizás, por ejemplo, los ganglios basales
hacen algo de aprendizaje por refuerzo al

463
00:21:18,328 --> 00:21:21,205
afinar los modelos de aprendizaje por
refuerzo, pero, sabes, sabes, mientras

464
00:21:21,205 --> 00:21:24,235
que, como, el hipocampo, no es obvio qué
sería eso. Algunas partes probablemente

465
00:21:24,235 --> 00:21:27,379
no son importantes. Quizás el cerebelo no
es importante para la cognición, se cree,

466
00:21:27,379 --> 00:21:28,837
así que quizás podamos saltarnos algo.

467
00:21:29,104 --> 00:21:32,303
Pero sigo pensando que existen, por
ejemplo, la amígdala, todas las emociones

468
00:21:32,303 --> 00:21:35,630
y los instintos, y probablemente, como,
un montón de otros núcleos en el cerebro

469
00:21:35,630 --> 00:21:38,872
que son muy antiguos que no creo que
hayamos, como, realmente replicado. No sé

470
00:21:38,872 --> 00:21:41,819
en realidad si deberíamos estar
persiguiendo, sabes, la construcción de

471
00:21:41,819 --> 00:21:43,082
un análogo del cerebro humano.

472
00:21:43,144 --> 00:21:46,917
Eh, de nuevo, soy ingeniero de corazón,
pero, eh, aún creo que quizás otra forma

473
00:21:46,917 --> 00:21:50,499
de responder la pregunta es que no vas a
contratar a esto como becario, y le

474
00:21:50,499 --> 00:21:54,225
faltan muchas cosas... porque viene con
muchos de estos déficits cognitivos que

475
00:21:54,225 --> 00:21:57,855
todos sentimos intuitivamente cuando
hablamos con los modelos, eh, y por eso,

476
00:21:57,855 --> 00:22:01,724
simplemente, aún no está del todo ahí, y
puedes verlo como que no todas las partes

477
00:22:01,724 --> 00:22:03,252
del cerebro están completas aún.

478
00:21:53,084 --> 00:21:53,223
Mm-hm

479
00:22:03,404 --> 00:22:07,476
Mm-hmm. Esto quizás sea irrelevante para
la... la cuestión de reflexionar sobre la

480
00:22:07,476 --> 00:22:11,347
velocidad con la que estos problemas
serán resueltos. Así que a veces la gente

481
00:22:11,347 --> 00:22:15,169
dice, uh, en relación con el aprendizaje
continuo: "Mira, la verdad es que ya

482
00:22:15,169 --> 00:22:15,571
podrías"

483
00:22:15,944 --> 00:22:19,388
y- se podría replicar esta capacidad con
gran facilidad. De la misma manera que el

484
00:22:19,388 --> 00:22:22,322
aprendizaje en contexto emergió de forma
espontánea como resultado del

485
00:22:22,322 --> 00:22:25,256
pre-entrenamiento, m- el aprendizaje
continuo a lo largo de horizontes

486
00:22:25,256 --> 00:22:28,615
temporales más extensos surgirá de manera
espontánea si se incentiva al modelo a

487
00:22:28,615 --> 00:22:31,890
recopilar y recordar información a lo
largo de horizontes más amplios, o bien,

488
00:22:31,890 --> 00:22:33,591
horizontes que superen una única sesión.

489
00:22:33,784 --> 00:22:37,716
Entonces, si hay, uhm, algún tipo de RL
de bucle externo que tiene muchas

490
00:22:37,716 --> 00:22:41,976
sesiones dentro de ese bucle externo,
entonces, como, este aprendizaje continuo

491
00:22:41,976 --> 00:22:45,964
donde usa, como, se autoajusta donde
escribe en una memoria externa o algo

492
00:22:45,964 --> 00:22:49,732
así, simplemente, como, surgirá
espontáneamente. ¿Crees, crees que las

493
00:22:49,732 --> 00:22:52,081
cosas son... sí... realmente tan
plausibles?

494
00:22:52,124 --> 00:22:55,174
No tengo, uh, un previo claro de cuán
plausible es. Sí. ¿Cuán probable es que

495
00:22:55,174 --> 00:22:55,374
pase?

496
00:22:55,384 --> 00:22:58,981
No sé si resueno completamente con eso-
... porque siento que estos modelos,

497
00:22:58,981 --> 00:23:02,435
cuando los inicias y tienen cero tokens
en la ventana, siempre están como

498
00:23:02,435 --> 00:23:06,224
reiniciando desde cero donde estaban. Así
que no sé realmente en esa cosmovisión

499
00:23:06,224 --> 00:23:10,158
cómo se ve, uh, porque, um, de nuevo, ha-
quizás haciendo algunas analogías con los

500
00:23:10,158 --> 00:23:13,755
humanos solo porque creo que es más o
menos concreto y algo interesante para

501
00:23:13,755 --> 00:23:14,331
reflexionar.

502
00:23:14,424 --> 00:23:17,619
Siento que cuando estoy despierto, estoy
construyendo una ventana de contexto de

503
00:23:17,619 --> 00:23:20,653
lo que ocurre en el día... pero siento
que cuando duermo, algo mágico sucede

504
00:23:20,653 --> 00:23:23,526
donde, eh, en realidad no creo que esa
ventana de contexto se quede. Eh-

505
00:23:23,644 --> 00:23:27,187
Creo que existe un proceso de destilación
en las conexiones neuronales de mi

506
00:23:27,187 --> 00:23:30,730
cerebro. Eh, y esto sucede mientras
dormimos y otras cosas por el estilo. No

507
00:23:30,730 --> 00:23:34,415
tenemos un equivalente para todo eso en,
eh, los grandes modelos de lenguaje, y

508
00:23:34,415 --> 00:23:38,053
eso, para mí, está más relacionado con
cuando hablas de aprendizaje continuo y

509
00:23:38,053 --> 00:23:41,455
demás, eh, como algo ausente. Estos
modelos no tienen realmente una, esta

510
00:23:41,455 --> 00:23:44,809
fase de destilación, eh, de tomar lo que
sucedió, analizarlo, y pensarlo

511
00:23:44,809 --> 00:23:45,470
obsesivamente,

512
00:23:27,084 --> 00:23:27,455
Claro.

513
00:23:38,164 --> 00:23:38,256
Sí.

514
00:23:46,224 --> 00:23:49,724
Eh, o sea, básicamente haciendo algún
tipo de proceso de generación de datos

515
00:23:49,724 --> 00:23:53,364
sintéticos y destilándolo de vuelta... de
vuelta en los pesos y quizás teniendo

516
00:23:53,364 --> 00:23:56,958
una, sabes, red neuronal específica por
persona, eh, quizás es una LoRA, no es

517
00:23:56,958 --> 00:24:00,599
una completa... eh, sí, no es una red
neuronal de pesos completos que sea... Es

518
00:24:00,599 --> 00:24:04,099
solo pequeña, así que... algunos del
pequeño, subconjunto disperso... de los

519
00:24:04,099 --> 00:24:04,892
pesos se cambian.

520
00:24:05,104 --> 00:24:08,231
Pero básicamente, sí queremos crear
formas de generar individuos con

521
00:24:08,231 --> 00:24:11,405
contextos muy largos. No es solo
permanecer en la ventana de contexto

522
00:24:11,405 --> 00:24:14,859
porque las ventanas de contexto crecen
muchísimo. Como, quizás tengamos una

523
00:24:14,859 --> 00:24:18,453
atención dispersa muy elaborada sobre
ello. Pero sigo pensando que los humanos

524
00:24:18,453 --> 00:24:22,047
obviamente tienen algún proceso para
destilar parte de ese conocimiento en los

525
00:24:22,047 --> 00:24:23,540
pesos. Nos lo estamos perdiendo.

526
00:24:17,284 --> 00:24:17,748
Ajá.

527
00:24:23,884 --> 00:24:27,414
Y también creo que los humanos tienen
algún tipo de esquema de atención

528
00:24:27,414 --> 00:24:28,574
dispersa muy elaborado-

529
00:24:28,784 --> 00:24:28,969
Mmm

530
00:24:30,264 --> 00:24:33,583
Y creo que, uh, estamos empezando a ver
algunos indicios tempranos de eso. Así

531
00:24:33,583 --> 00:24:36,946
que, DeepSeek, uh, v3.2, uh, acaba de ser
lanzado y vi que tienen algo como una

532
00:24:36,946 --> 00:24:40,439
atención dispersa, por ejemplo, y esta es
una manera de tener ventanas de contexto

533
00:24:40,439 --> 00:24:43,629
muy, muy extensas. Así que casi siento
que estamos rehaciendo muchos de los

534
00:24:43,629 --> 00:24:47,122
trucos cognitivos que la evolución ideó a
través de un proceso muy diferente. Pero

535
00:24:47,122 --> 00:24:50,140
creo que vamos a converger en una
arquitectura similar, cognitivamente.

536
00:24:50,164 --> 00:24:53,739
Es interesante. De aquí a diez años,
¿crees que seguirá siendo algo parecido a

537
00:24:53,739 --> 00:24:56,990
un transformador, pero con una atención
mucho más modificada y MLPs más

538
00:24:56,990 --> 00:24:58,523
dispersas, y cosas por el estilo?

539
00:24:58,544 --> 00:25:01,556
Bueno, la forma en que me gusta verlo es
así: ok, pensemos en, eh, traducción y

540
00:25:01,556 --> 00:25:04,182
variación en el tiempo, ¿verdad?
Entonces, hace diez años, ¿dónde nos

541
00:25:04,182 --> 00:25:07,116
encontrábamos? En 2015, eh, teníamos, eh,
redes neuronales convolucionales...

542
00:25:07,116 --> 00:25:09,318
principalmente. Las redes residuales
acababan de aparecer.

543
00:25:09,404 --> 00:25:13,253
Eh, tan notablemente similar, supongo,
pero bastante diferente aún. O sea,

544
00:25:13,253 --> 00:25:14,465
Transformer no existía.

545
00:25:14,664 --> 00:25:18,276
Bueno, sabes, todas estas, eh, digamos,
modificaciones más modernas en un

546
00:25:18,276 --> 00:25:22,340
transformador no existían. Así que quizás
algunas de las cosas por las que podemos

547
00:25:22,340 --> 00:25:26,102
apostar, creo, en 10 años, eh, por la
equivariancia traslacional es, eh, que

548
00:25:26,102 --> 00:25:29,865
seguimos entrenando redes neuronales
gigantes con, eh, paso hacia adelante y

549
00:25:29,865 --> 00:25:32,775
hacia atrás y actualización mediante
descenso de gradiente.

550
00:25:33,164 --> 00:25:36,214
Eh, pero quizás se ve un poco
diferente... [pause] y es que,

551
00:25:36,214 --> 00:25:38,179
simplemente, todo es mucho más grande.

552
00:25:36,304 --> 00:25:36,443
Mmm

553
00:25:38,424 --> 00:25:41,708
De hecho, recientemente, yo también me
remonté hasta el año mil novecientos

554
00:25:41,708 --> 00:25:45,127
ochenta y nueve, lo cual fue un ejercicio
bastante divertido para mí hace unos

555
00:25:45,127 --> 00:25:48,411
años, porque estaba reproduciendo la red
convolucional de Yann LeCun de mil

556
00:25:48,411 --> 00:25:51,652
novecientos ochenta y nueve, que fue la
primera red neuronal, que yo sepa,

557
00:25:51,652 --> 00:25:55,248
entrenada mediante descenso de gradiente,
como las redes neuronales modernas, para

558
00:25:55,248 --> 00:25:56,535
el reconocimiento de dígitos.

559
00:25:56,584 --> 00:25:57,001
Pues, a ver.

560
00:25:57,184 --> 00:26:00,538
Y a mí me interesaba, bueno, ¿cómo podría
modernizar esto? ¿Qué parte de esto son

561
00:26:00,538 --> 00:26:03,724
algoritmos? ¿Qué parte de esto son datos?
¿Qué parte de este progreso es, eh,

562
00:26:03,724 --> 00:26:04,521
cómputo y sistemas?

563
00:26:04,564 --> 00:26:07,838
Y pude muy rápidamente, como, reducir a
la mitad la tasa de aprendizaje solo

564
00:26:07,838 --> 00:26:11,288
conociendo mi trampa, viajar en el tiempo
33 años. Así que si viajo en el tiempo

565
00:26:11,288 --> 00:26:14,607
mediante algoritmos 33 años, podría
ajustar lo que Yann LeCun hizo en 1989, y

566
00:26:14,607 --> 00:26:18,143
podría básicamente reducir a la mitad el
aprendizaje, reducir a la mitad el error.

567
00:26:18,143 --> 00:26:21,375
Pero, eh, para obtener mayores ganancias,
tuve que añadir muchos más datos.

568
00:26:21,424 --> 00:26:24,787
Tuve que multiplicar por diez el conjunto
de entrenamiento, y luego tuve que añadir

569
00:26:24,787 --> 00:26:27,986
más optimizaciones computacionales, tuve
que entrenar básicamente por mucho más

570
00:26:27,986 --> 00:26:30,201
tiempo, con dropout y otras técnicas de
regularización.

571
00:26:30,464 --> 00:26:33,597
Y es casi como si todas estas cosas
tuvieran que mejorar simultáneamente. Así

572
00:26:33,597 --> 00:26:36,894
que, uh, sabes, probablemente tendremos
muchos más datos. Probablemente tendremos

573
00:26:36,894 --> 00:26:39,698
mucho mejor hardware, probablemente
tendremos mucho mejores kernels y

574
00:26:39,698 --> 00:26:41,841
software. Probablemente tendremos mejores
algoritmos.

575
00:26:41,844 --> 00:26:45,463
Y todos esos, es casi como si ninguno de
ellos estuviera ganando demasiado. Todos

576
00:26:45,463 --> 00:26:47,091
ellos son sorprendentemente iguales.

577
00:26:48,144 --> 00:26:48,654
Mmm.

578
00:26:48,744 --> 00:26:51,614
Y esto ha sido más o menos la tendencia
desde hace un tiempo. Así que, supongo,

579
00:26:51,614 --> 00:26:53,932
para responder quizás a tu pregunta,
espero ver diferencias, uh,

580
00:26:53,932 --> 00:26:56,728
algorítmicamente en comparación con lo
que está pasando hoy. Uh, pero también

581
00:26:56,728 --> 00:26:59,414
espero que algunas de las cosas que han
per- permanecido por un tiempo muy

582
00:26:59,414 --> 00:27:02,211
prolongado probablemente sigan estando
ahí. Probablemente siga siendo una red

583
00:27:02,211 --> 00:27:05,044
neuronal gigante entrenada con descenso
de gradiente. Esa sería mi suposición.

584
00:27:05,044 --> 00:27:08,696
Es sorprendente que todas esas cosas
juntas solo redujeran a la mitad, um...

585
00:27:08,696 --> 00:27:12,446
uh, la mitad del error. Lo cual es, o
sea, es como si treinta años de progreso

586
00:27:12,446 --> 00:27:16,245
fueran, uh... Quizás, quizás la mitad es
mucho, porque si reduces el error a la

587
00:27:16,245 --> 00:27:18,047
mitad, eso en realidad significa que-

588
00:27:12,204 --> 00:27:12,529
Claro.

589
00:27:18,044 --> 00:27:18,740
Media es mucho.

590
00:27:18,704 --> 00:27:19,957
Sí, sí. De acuerdo. Eh... bueno...

591
00:27:19,984 --> 00:27:23,285
Pero, supongo que lo que me sorprendió es
que todo necesita mejorar en todos los

592
00:27:23,285 --> 00:27:26,211
aspectos. Eh, la arquitectura, el
optimizador, la función de pérdida, y

593
00:27:26,211 --> 00:27:29,555
también ha mejorado en todos los aspectos
desde siempre. Así que espero que todos

594
00:27:29,555 --> 00:27:31,268
esos cambios estén bien, vivos y
coleando.

595
00:27:23,564 --> 00:27:23,842
Claro.

596
00:27:31,304 --> 00:27:35,119
Bueno, sí. De hecho, yo estaba a punto de
hacerte una pregunta muy similar sobre

597
00:27:35,119 --> 00:27:38,452
nanochat. Porque como acabas de
codificarlo hace muy poco, cada uno de

598
00:27:38,452 --> 00:27:42,170
los pasos en el proceso de construir un
chatbot está fresco en tu memoria RAM.

599
00:27:34,164 --> 00:27:34,303
Ajá.

600
00:27:42,204 --> 00:27:42,529
Claro.

601
00:27:42,844 --> 00:27:47,430
Y me pregunto si tú también pensaste algo
parecido, como, oh, que no hubo un único

602
00:27:47,430 --> 00:27:51,903
factor realmente relevante para el paso
de GPT-2 a nanochat. Eh, ¿cuáles fueron,

603
00:27:51,903 --> 00:27:55,754
digamos, los aprendizajes más
sorprendentes de toda esta experiencia?

604
00:27:55,844 --> 00:27:59,543
¿Hablando de nanochat? Así que nanochat
es una especie de repositorio que lancé,

605
00:27:59,543 --> 00:28:01,416
¿fue ayer... o anteayer? No lo recuerdo.

606
00:27:59,824 --> 00:28:00,102
Ajá

607
00:28:03,264 --> 00:28:05,214
Se ve la privación de sueño que hubo en
el...

608
00:28:06,024 --> 00:28:09,950
Sí, sí, sí. Eh, bueno, solo busca ser
un... Busca ser el repositorio más

609
00:28:09,950 --> 00:28:13,986
completo y sencillo que abarque todo el
proceso de principio a fin... para

610
00:28:13,986 --> 00:28:15,590
construir un clon de ChatGPT.

611
00:28:15,984 --> 00:28:19,799
Y así, sabes, tienes todos los pasos, no
solo cualquier paso individual, lo cual

612
00:28:19,799 --> 00:28:23,519
es m- mucho... Trabajé en todos los pasos
individuales en el pasado y publiqué

613
00:28:23,519 --> 00:28:27,093
pequeños fragmentos de código que
podrían, eh, mostrarte cómo se hace en un

614
00:28:27,093 --> 00:28:30,813
sentido algorítmico, eh, eh, en código
simple. Pero esto se encarga de todo el

615
00:28:30,813 --> 00:28:31,634
proceso completo.

616
00:28:32,084 --> 00:28:36,150
Yo, yo creo que en cuanto a aprender, no
es, uh, no es tanto que, um, haya

617
00:28:36,150 --> 00:28:40,439
encontrado algo que aprendiera de ello
necesariamente. Ya tenía más o menos en

618
00:28:40,439 --> 00:28:44,839
mente cómo se construye, y esto es solo
un proceso de construirlo mecánicamente,

619
00:28:44,839 --> 00:28:49,295
um, y de hacerlo lo suficientemente claro
y, uh, para que la gente pueda aprender

620
00:28:49,295 --> 00:28:51,635
de ello y, um, que, uh, lo encuentren
útil.

621
00:28:51,704 --> 00:28:54,708
Sí. ¿Cuál es la mejor manera para que
alguien aprenda de ello? Eh, ¿es como

622
00:28:54,708 --> 00:28:57,550
borrar todo el código e intentar
reimplementarlo desde cero, o añadirle

623
00:28:57,550 --> 00:28:58,159
modificaciones?

624
00:28:58,424 --> 00:29:02,370
Uhm, sí, creo que es una gran pregunta.
Yo diría que, básicamente, son unas ocho

625
00:29:02,370 --> 00:29:05,018
mil líneas de código que te guían por
todo el proceso.

626
00:29:05,064 --> 00:29:08,299
Quizás lo pondría en el monitor derecho,
si tienes dos monitores, lo pones en

627
00:29:08,299 --> 00:29:09,150
el... en el derecho.

628
00:29:09,464 --> 00:29:12,955
Eh, y si quieres construirlo desde cero,
lo construyes desde el principio. No se

629
00:29:12,955 --> 00:29:16,313
te permite copiar y pegar. Se te permite
referenciar, pero no copiar y pegar.

630
00:29:16,313 --> 00:29:17,683
Quizás así es como yo lo haría.

631
00:29:17,784 --> 00:29:21,805
Eh, pero también creo que el repositorio
por sí solo, es como una bestia bastante

632
00:29:21,805 --> 00:29:25,675
grande. Es decir, es... Cuando escribes
este código, no vas de arriba a abajo.

633
00:29:25,675 --> 00:29:29,697
Vas por trozos y haces crecer los trozos,
y esa información está ausente, como si

634
00:29:29,697 --> 00:29:31,205
no supieras por dónde empezar.

635
00:29:31,264 --> 00:29:34,499
Y entonces, creo que... no es solamente
un repositorio final lo que se necesita,

636
00:29:34,499 --> 00:29:37,242
es más bien la construcción de ese
repositorio, que es un proceso de

637
00:29:37,242 --> 00:29:40,273
crecimiento complejo y por etapas. Eh,
así que esa parte todavía no existe.

638
00:29:38,204 --> 00:29:38,621
Correcto.

639
00:29:40,484 --> 00:29:43,852
Me, me encantaría de hecho añadir eso
probablemente más tarde esta semana o

640
00:29:43,852 --> 00:29:47,402
algo así de alguna manera. Quizás sea,
uh, probablemente un video o algo por el

641
00:29:47,402 --> 00:29:50,998
estilo. Pero, um, pero quizás en términos
generales, bueno, lo que intentaría es

642
00:29:50,998 --> 00:29:54,230
que lo construyas tú mismo, uh, pero, uh,
no te permitas copiar y pegar.

643
00:29:54,264 --> 00:29:54,867
Claro que sí.

644
00:29:54,944 --> 00:29:58,192
Yo creo que hay casi dos tipos de
conocimiento, como el conocimiento

645
00:29:58,192 --> 00:30:01,682
superficial de alto nivel. Pero la
cuestión es que cuando construyes algo

646
00:30:01,682 --> 00:30:05,609
desde cero, te ves obligado a aceptar lo
que realmente no entiendes y no sabes que

647
00:30:05,609 --> 00:30:09,293
no lo entiendes. Y siempre lleva a una
comprensión más profunda. Y, eh, no es

648
00:30:09,293 --> 00:30:12,977
solo la única forma de construir, es
como, eh, si no puedo construirlo, no lo

649
00:30:12,977 --> 00:30:15,934
entiendo. ¿Es una cita de Feynman, creo,
o algo por el estilo?

650
00:30:05,704 --> 00:30:06,214
Curioso.

651
00:30:15,944 --> 00:30:17,058
Sí, por supuesto que sí.

652
00:30:17,284 --> 00:30:20,267
Cien por cien, siempre he creído esto muy
firmemente, uh, porque hay todas estas

653
00:30:20,267 --> 00:30:23,175
micro cosas que simplemente no están bien
organizadas y realmente no tienes el

654
00:30:23,175 --> 00:30:25,857
conocimiento. Solo crees que tienes el
conocimiento. Así que no escribas

655
00:30:25,857 --> 00:30:28,878
entradas de blog, no hagas diapositivas,
no hagas nada de eso- ... como construir

656
00:30:28,878 --> 00:30:31,409
el código, organizarlo, hacerlo
funcionar. Es la única manera. De lo

657
00:30:31,409 --> 00:30:32,655
contrario, te falta conocimiento.

658
00:30:32,924 --> 00:30:36,732
Uhm, tuiteaste que los modelos de
codificación en realidad te fueron de muy

659
00:30:36,732 --> 00:30:40,798
poca ayuda para armar este repositorio. Y
tengo curiosidad por saber por qué fue

660
00:30:40,798 --> 00:30:41,004
así.

661
00:30:41,044 --> 00:30:44,694
Sí. Eh, el repositorio, supongo que lo
construí durante un período de poco más

662
00:30:44,694 --> 00:30:48,061
de un mes. Y diría que hay como tres
clases principales de cómo la gente

663
00:30:48,061 --> 00:30:51,664
interactúa con el código ahora mismo.
Algunas personas rechazan completamente

664
00:30:51,664 --> 00:30:55,220
todos los LLM y simplemente están, eh,
escribiendo desde cero. Creo que esto

665
00:30:55,220 --> 00:30:56,880
probablemente ya no es lo correcto.

666
00:30:56,988 --> 00:31:00,258
Eh, la parte intermedia, que es donde
estoy, es que todavía escribes muchas

667
00:31:00,258 --> 00:31:03,616
cosas desde cero, pero usas, eh, el
autocompletado, eh, que está básicamente,

668
00:31:03,616 --> 00:31:06,798
eh, disponible ahora a través de estos
modelos. Así que cuando empiezas a

669
00:31:06,798 --> 00:31:09,449
escribir un pequeño trozo, se
autocompletará por ti, y puedes

670
00:31:09,449 --> 00:31:12,498
simplemente tocar para seguir- y la
mayoría de las veces, es correcto.

671
00:31:12,548 --> 00:31:15,756
A veces no lo es y tú lo editas. Pero
sigues siendo en gran medida el, eh, el

672
00:31:15,756 --> 00:31:18,965
arquitecto, por así decirlo, de lo que
estás escribiendo. Y luego está el, ya

673
00:31:18,965 --> 00:31:19,978
sabes, el 'vibe coding'.

674
00:31:20,088 --> 00:31:23,774
Eh, sabes, hola, por favor implementa
esto o aquello, eh, sabes, introduce, y

675
00:31:23,774 --> 00:31:27,605
luego deja que el modelo lo haga. Y esos
son los agentes. Mmm, sí siento que los

676
00:31:27,605 --> 00:31:31,000
agentes funcionan en entornos muy
específicos, y los usaría en entornos

677
00:31:31,000 --> 00:31:34,783
específicos. Pero de nuevo, todas estas
son herramientas disponibles para ti, y

678
00:31:34,783 --> 00:31:38,663
tienes que, como, aprender para qué son
buenas y para qué no... y cuándo usarlas.

679
00:31:38,928 --> 00:31:41,895
Así que los agentes son en realidad
bastante buenos, por ejemplo, si estás

680
00:31:41,895 --> 00:31:44,781
haciendo cosas rutinarias. Código
estándar que es, como, solo copiar, ya

681
00:31:44,781 --> 00:31:47,830
sabes, solo copiar y pegar. Son muy
buenos para eso. Son muy buenos en cosas

682
00:31:47,830 --> 00:31:50,920
que aparecen con mucha frecuencia en
internet, porque hay muchísimos ejemplos

683
00:31:50,920 --> 00:31:53,277
de ello en los conjuntos de entrenamiento
de estos modelos.

684
00:31:53,648 --> 00:31:58,086
Eh, entonces, hay como características de
cosas en las que los modelos funcionarán

685
00:31:58,086 --> 00:32:02,086
muy bien. Yo diría que Nanochat no es un
ejemplo de eso, porque, eh, es un

686
00:32:02,086 --> 00:32:06,359
repositorio bastante único. No hay tanto
código, creo, en la forma en que lo he

687
00:32:06,359 --> 00:32:08,880
estructurado. Y, eh, y no es código
repetitivo.

688
00:32:08,888 --> 00:32:12,053
Es, de hecho, un código casi
intelectualmente intenso... y todo tiene

689
00:32:12,053 --> 00:32:15,777
que estar muy precisamente organizado. Y
los modelos siempre trataban de, seguían

690
00:32:15,777 --> 00:32:19,315
tratando de... Es decir, tienen tantos
déficits cognitivos, ¿no? Así que, por

691
00:32:19,315 --> 00:32:22,341
ejemplo, siguen intentando...
malinterpretan el código, eh, porque

692
00:32:22,341 --> 00:32:25,879
tienen demasiada memoria de todas las
formas típicas de hacer las cosas... en

693
00:32:25,879 --> 00:32:28,067
internet que yo simplemente no estaba
adoptando.

694
00:32:17,588 --> 00:32:17,820
Mmm.

695
00:32:28,328 --> 00:32:31,476
Uh, entonces los modelos, por ejemplo,
quiero decir, no sé si quiero entrar en

696
00:32:31,476 --> 00:32:34,788
todos los detalles, pero siguen, siguen,
um, siguen pensando que estoy escribiendo

697
00:32:34,788 --> 00:32:35,851
código normal y no es así.

698
00:32:36,848 --> 00:32:38,055
Quizás un ejemplo interesante.

699
00:32:38,028 --> 00:32:41,188
Quizás un ejemplo. Eh, entonces, la forma
de sincronizar... Tenemos ocho GPUs...

700
00:32:41,188 --> 00:32:44,029
que están haciendo propagación hacia
adelante y hacia atrás. La forma de

701
00:32:44,029 --> 00:32:47,270
sincronizar los gradientes entre ellas es
usar un contenedor de datos distribuidos

702
00:32:47,270 --> 00:32:49,591
en paralelo de PyTorch, que
automáticamente hace todo el...

703
00:32:49,648 --> 00:32:52,434
Al revertirlo, empezará a comunicar y
sincronizar ingredientes.

704
00:32:52,588 --> 00:32:56,681
No utilicé DDP porque no quise usarlo, ya
que no es necesario, así que lo descarté.

705
00:32:56,681 --> 00:33:00,575
Básicamente escribí mi propia rutina de
sincronización que está dentro del paso

706
00:33:00,575 --> 00:33:04,618
del optimizador. Y los modelos intentaban
que usara el contenedor DDP... y estaban

707
00:33:04,618 --> 00:33:08,462
muy preocupados por... Vale, esto se está
volviendo demasiado técnico, pero no

708
00:33:08,462 --> 00:33:12,256
estaba usando ese contenedor porque no lo
necesito y tengo una implementación

709
00:33:12,256 --> 00:33:13,903
personalizada de... algo similar.

710
00:33:05,688 --> 00:33:05,827
Sí

711
00:33:13,928 --> 00:33:15,599
¿Y no aceptaron tu implementación?

712
00:33:15,588 --> 00:33:20,244
Sí, no pudieron. No podían superar eso. Y
luego, eh, seguían intentando, como,

713
00:33:20,244 --> 00:33:21,392
arruinar el estilo.

714
00:33:21,468 --> 00:33:25,178
Es que son demasiado a la defensiva.
Hacen todas estas sentencias try-catch.

715
00:33:25,178 --> 00:33:28,641
Siguen intentando crear una base de
código de producción, y yo tengo un

716
00:33:28,641 --> 00:33:32,500
montón de suposiciones en mi código y
está bien. Y, uh, es que no necesito todo

717
00:33:32,500 --> 00:33:36,359
este material extra ahí. Y por eso siento
que están inflando la base de código,

718
00:33:36,359 --> 00:33:40,366
están inflando la complejidad, siguen sin
entender, están usando APIs obsoletas un

719
00:33:40,366 --> 00:33:41,158
montón de veces.

720
00:33:24,828 --> 00:33:24,967
Sí.

721
00:33:41,328 --> 00:33:44,885
Así que es un desastre total, y
simplemente no es tan útil. Puedo entrar

722
00:33:44,885 --> 00:33:48,894
y limpiarlo, pero no es tan útil. También
siento que es un poco molesto tener que

723
00:33:48,894 --> 00:33:52,702
escribir lo que quiero en inglés, porque
es demasiado teclear. Si simplemente

724
00:33:52,702 --> 00:33:56,560
navego a la parte del código que quiero y
voy donde sé que el código tiene que

725
00:33:56,560 --> 00:34:00,318
aparecer, y empiezo a escribir las
primeras tres letras, el autocompletar lo

726
00:34:00,318 --> 00:34:02,272
entiende y simplemente te da el código.

727
00:34:02,708 --> 00:34:06,469
Y así que creo que es... Esto es un ancho
de banda de información muy alto para

728
00:34:06,469 --> 00:34:10,423
especificar lo que quieres, es si apuntas
al código donde lo quieres y escribes las

729
00:34:10,423 --> 00:34:13,846
primeras piezas... y el modelo lo
completará. Así que supongo que lo que

730
00:34:13,846 --> 00:34:17,415
quiero decir es, um, creo que estos
modelos son buenos en ciertas partes de

731
00:34:17,415 --> 00:34:17,800
la pila.

732
00:34:09,908 --> 00:34:10,047
Sí

733
00:34:18,148 --> 00:34:21,784
De hecho, usar los modelos un poco en,
em... Hay dos ejemplos donde realmente

734
00:34:21,784 --> 00:34:25,181
uso los modelos que creo que son
ilustrativos. Uh, uno fue cuando generé

735
00:34:25,181 --> 00:34:28,817
un informe, y eso es en realidad más de
plantilla, así que de hecho codifiqué

736
00:34:28,817 --> 00:34:32,550
par- parcialmente algo de eso. Eso estuvo
bien, em, porque no es algo de misión

737
00:34:32,550 --> 00:34:33,937
crítica y- ... funciona bien.

738
00:34:34,168 --> 00:34:36,945
Y la otra parte es cuando estaba
reescribiendo el tokenizador, uh, en

739
00:34:36,945 --> 00:34:40,213
Rust. Uh, en realidad no soy tan bueno en
Rust porque soy bastante nuevo en Rust.

740
00:34:40,213 --> 00:34:43,440
Así que estaba haciendo... Hay un poco de
programación por intuición, uh, cuando

741
00:34:43,440 --> 00:34:46,585
escribía parte del código de Rust. Pero
tenía una implementación en Python que

742
00:34:46,585 --> 00:34:49,608
entiendo completamente, y solo me aseguro
de estar haciendo una versión más

743
00:34:49,608 --> 00:34:52,712
eficiente de ella- ... y tengo pruebas,
así que me siento más seguro haciendo

744
00:34:52,712 --> 00:34:53,161
esas cosas.

745
00:34:50,728 --> 00:34:50,867
Mm-hm

746
00:34:53,548 --> 00:34:56,923
Y, básicamente, lo que hacen es reducir
o, mejor dicho, aumentar la accesibilidad

747
00:34:56,923 --> 00:35:00,045
a lenguajes o paradigmas con los que
quizás no estés tan familiarizado. Así

748
00:35:00,045 --> 00:35:03,210
que, en ese sentido, creo que resultan
ser de gran ayuda también. Porque, la

749
00:35:03,210 --> 00:35:06,670
verdad, hay una cantidad enorme de código
Rust disponible. Y los modelos, de hecho,

750
00:35:06,670 --> 00:35:07,851
son bastante buenos en ello.

751
00:35:04,268 --> 00:35:04,639
Ajá.

752
00:35:07,968 --> 00:35:10,382
No sé mucho de eso, por eso los modelos
son muy útiles ahí.

753
00:35:10,408 --> 00:35:14,478
Bien. Um, la razón por la que creo que
esta pregunta es tan interesante es

754
00:35:14,478 --> 00:35:19,051
porque la historia principal que la gente
tiene sobre la IA explotando y obteniendo

755
00:35:19,051 --> 00:35:23,178
superinteligencia bastante rápido es la
IA automatizando la ingeniería y la

756
00:35:23,178 --> 00:35:24,293
investigación de IA.

757
00:35:25,278 --> 00:35:25,370
Ajá.

758
00:35:25,468 --> 00:35:28,673
Así que verán el hecho de que puedes
tener código en la nube y crear

759
00:35:28,673 --> 00:35:32,405
aplicaciones completas, aplicaciones CRUD
desde cero y dirán: "Si tuvieras esta

760
00:35:32,405 --> 00:35:36,137
capacidad dentro de OpenAI y DeepMind y
todo lo demás, bueno... solo imagina el

761
00:35:36,137 --> 00:35:39,964
nivel de, ya sabes, mil de ustedes o un
millón de ustedes en paralelo encontrando

762
00:35:39,964 --> 00:35:43,744
pequeños... ajustes arquitectónicos". Y
es bastante interesante escucharte decir

763
00:35:43,744 --> 00:35:46,040
que esto es en lo que son asimétricamente
peores.

764
00:35:39,528 --> 00:35:39,620
Sí

765
00:35:46,088 --> 00:35:50,223
Y es, la verdad, bastante relevante para
las predicciones, si una explosión del

766
00:35:50,223 --> 00:35:53,193
tipo IA 2027 es probable que ocurra en un
futuro cercano.

767
00:35:48,008 --> 00:35:48,147
Oh

768
00:35:53,328 --> 00:35:56,749
Creo que es una buena forma de decirlo, y
creo que estás dando en el clavo con por

769
00:35:56,749 --> 00:36:00,087
qué mis plazos son un poco más largos.
Eh, tienes razón. Uhm, creo que, uhm, sí,

770
00:36:00,087 --> 00:36:03,424
no son muy buenos con código que nunca se
ha escrito antes. Quizás es, como, una

771
00:36:03,424 --> 00:36:06,550
forma de decirlo, que es, como, lo que
intentamos lograr al construir estos

772
00:36:06,550 --> 00:36:06,888
modelos.

773
00:36:07,788 --> 00:36:12,588
U- u- una pregunta muy ingenua, pero, em,
los ajustes arquitectónicos que están

774
00:36:12,588 --> 00:36:17,574
añadiendo a, eh, Nanochat, están en algún
artículo, ¿verdad? Incluso podrían estar

775
00:36:17,574 --> 00:36:22,313
en un repositorio. Entonces... ¿Es, em,
es sorprendente que no puedan integrar

776
00:36:22,313 --> 00:36:27,176
eso en el... cuando, por ejemplo, añaden
'rope embeddings' o algo así, ¿lo hacen

777
00:36:27,176 --> 00:36:28,407
de forma incorrecta?

778
00:36:29,928 --> 00:36:32,974
Es duro. Creo que más o menos saben, lo
saben, pero no lo saben del todo, y no

779
00:36:32,974 --> 00:36:34,200
saben cómo integrarlo del todo.

780
00:36:34,248 --> 00:36:34,665
Ajá, sí

781
00:36:34,728 --> 00:36:38,008
en el repositorio, tu estilo, tu código,
tu lugar y algunas de las cosas

782
00:36:38,008 --> 00:36:41,566
personalizadas que haces, y- ... y, eh,
cómo encaja con todas las suposiciones

783
00:36:41,566 --> 00:36:44,985
del repositorio y todo este tipo de
cosas. Así que, creo que sí tienen algo

784
00:36:44,985 --> 00:36:48,727
de conocimiento, pero, eh, no han llegado
al punto en que puedan integrarlo, darle

785
00:36:48,727 --> 00:36:52,377
sentido, eh, y así sucesivamente. Creo
que gran parte de esto, por cierto, sigue

786
00:36:52,377 --> 00:36:52,839
mejorando.

787
00:36:52,888 --> 00:36:55,931
Entonces, eh, creo que el modelo de
última generación al que acudo,

788
00:36:55,931 --> 00:36:59,435
probablemente, es el GPT-5 Pro. Y, eh,
ese es un modelo muy, muy potente. Así

789
00:36:59,435 --> 00:37:03,031
que, si realmente tengo 20 minutos, copio
y pego mi repositorio entero y voy al

790
00:37:03,031 --> 00:37:05,798
GPT-5 Pro, el Oráculo... para hacerle
algunas preguntas, y...

791
00:36:57,028 --> 00:36:57,260
Mmm.

792
00:37:06,000 --> 00:37:08,921
A menudo, no está mal, y es
sorprendentemente bueno comparado con lo

793
00:37:08,921 --> 00:37:09,575
de hace un año.

794
00:37:09,620 --> 00:37:10,223
Claro que sí.

795
00:37:10,259 --> 00:37:15,603
Pero sí creo que, en general, los modelos
no están a la altura. Y siento que la

796
00:37:15,603 --> 00:37:20,743
industria se está excediendo. Está dando
un salto demasiado grande... y está

797
00:37:20,743 --> 00:37:25,677
intentando hacer creer que esto es
asombroso, y no lo es. Es una chapuza.

798
00:37:26,240 --> 00:37:29,508
Y creo que no lo asimilan, y quizás
intentan recaudar fondos o algo así. No

799
00:37:29,508 --> 00:37:32,820
estoy seguro de qué está pasando, pero
estamos en esta etapa intermedia. Los

800
00:37:32,820 --> 00:37:35,249
modelos son asombrosos. Todavía necesitan
mucho trabajo.

801
00:37:35,560 --> 00:37:37,371
De momento, autocompletado es mi fuerte.

802
00:37:38,060 --> 00:37:38,292
Mmm.

803
00:37:38,319 --> 00:37:41,105
Pero a veces, para ciertos códigos, iré a
un motor de conocimiento.

804
00:37:41,120 --> 00:37:44,649
Sí, sí. Y mira, esta es también otra
razón por la que esto es muy interesante.

805
00:37:44,900 --> 00:37:48,619
Eh, a lo largo de toda la historia de la
programación, ha habido muchísimas

806
00:37:48,619 --> 00:37:52,086
mejoras en la productividad:
compiladores, herramientas de análisis de

807
00:37:52,086 --> 00:37:55,051
código, mejores lenguajes de
programación, etcétera, que han

808
00:37:55,051 --> 00:37:58,770
incrementado la productividad de los
programadores... pero no han provocado

809
00:37:58,770 --> 00:38:02,489
una explosión. Así que, eso es como
una... Eso suena mucho a una pestaña de

810
00:38:02,489 --> 00:38:03,243
autocompletado.

811
00:37:59,400 --> 00:37:59,817
Ajá.

812
00:38:03,259 --> 00:38:03,769
Ajá. Sí.

813
00:38:03,779 --> 00:38:06,386
Y esta otra categoría es como la
automatización del trabajo del

814
00:38:06,386 --> 00:38:09,708
programador. Y resulta interesante que se
esté viendo más en la categoría de las

815
00:38:09,708 --> 00:38:13,157
analogías históricas, como por ejemplo...
ya sabes, mejores compiladores o algo por

816
00:38:13,157 --> 00:38:13,577
el estilo.

817
00:38:07,200 --> 00:38:07,571
Ajá.

818
00:38:13,580 --> 00:38:18,163
Sí. Y quizás porque esto me lleva a otra
idea, que es, me cuesta diferenciar dónde

819
00:38:18,163 --> 00:38:22,689
empieza y dónde acaba la IA. Porque veo
la IA fundamentalmente como una extensión

820
00:38:22,689 --> 00:38:26,481
de la computación de un modo bastante
esencial. Y siento que hay una

821
00:38:26,481 --> 00:38:31,064
continuidad en esta auto-mejora recursiva
o en la aceleración de los programadores

822
00:38:31,064 --> 00:38:35,081
desde el principio. Por ejemplo, incluso,
diría, los editores de código.

823
00:38:20,220 --> 00:38:20,359
Mmm.

824
00:38:35,120 --> 00:38:35,630
Así es.

825
00:38:35,819 --> 00:38:37,723
Eh, uh, resaltado de sintaxis.

826
00:38:38,400 --> 00:38:38,817
Así es.

827
00:38:38,920 --> 00:38:42,717
Eh, la sintaxis, o la verificación de
tipos, incluso de datos. Todas estas

828
00:38:42,717 --> 00:38:46,411
herramientas que hemos construido entre
nosotros, incluso los motores de

829
00:38:46,411 --> 00:38:49,740
búsqueda. ¿Por qué los motores de
búsqueda no son parte de la IA?

830
00:38:50,400 --> 00:38:53,588
No sé, el ranking es como IA, ¿no? En
algún momento Google, incluso desde el

831
00:38:53,588 --> 00:38:56,903
principio, se consideraba una empresa de
IA haciendo... el motor de búsqueda de

832
00:38:56,903 --> 00:38:58,434
Google, lo cual me parece muy justo.

833
00:38:58,700 --> 00:39:02,188
Y así, yo lo veo más como un continuo de
lo que creo que otras personas lo ven, y

834
00:39:02,188 --> 00:39:05,677
no... Me cuesta trazar la línea, y siento
que, bueno, ahora estamos obteniendo un

835
00:39:05,677 --> 00:39:08,686
autocompletado mucho mejor, y ahora
también estamos obteniendo algunos

836
00:39:08,686 --> 00:39:11,826
agentes que son como estas cosas
cíclicas, pero a veces se descarrilan. Y

837
00:39:11,826 --> 00:39:15,314
lo que está pasando es que el humano está
haciendo progresivamente cada vez menos

838
00:39:15,314 --> 00:39:18,410
del trabajo de bajo nivel. Por ejemplo,
no estamos escribiendo el código

839
00:39:18,410 --> 00:39:20,155
ensamblador porque tenemos compiladores.

840
00:39:20,200 --> 00:39:20,478
Claro.

841
00:39:20,520 --> 00:39:23,675
¿Verdad? Por ejemplo, los compiladores
toman mi lenguaje de alto nivel y C y

842
00:39:23,675 --> 00:39:26,704
escriben el código ensamblador. Así que,
nos estamos abstrayendo muy, muy

843
00:39:26,704 --> 00:39:29,817
lentamente. Y hay lo que yo llamo un
control deslizante de autonomía, donde

844
00:39:29,817 --> 00:39:33,057
cada vez más cosas se automatizan de las
que pueden automatizarse en cualquier

845
00:39:33,057 --> 00:39:36,002
momento, y nosotros hacemos cada vez
menos y nos elevamos en la capa de

846
00:39:36,002 --> 00:39:37,517
abstracción sobre la automatización.

847
00:39:23,319 --> 00:39:23,644
Claro.

848
00:39:37,859 --> 00:39:41,037
Uno de los grandes problemas con el
aprendizaje por refuerzo es que la

849
00:39:41,037 --> 00:39:44,768
información disponible es increíblemente
escasa. LabelBox puede ayudarte con esto,

850
00:39:44,768 --> 00:39:47,946
ya que incrementa la cantidad de
información de la que tu agente puede

851
00:39:47,946 --> 00:39:49,143
aprender en cada episodio.

852
00:39:49,160 --> 00:39:53,149
Por ejemplo, uno de sus clientes quería
entrenar a un agente de codificación, así

853
00:39:53,149 --> 00:39:56,691
que LabelBox aumentó un IDE con un montón
de herramientas adicionales de

854
00:39:56,691 --> 00:40:00,531
recolección de datos y contrató a un
equipo de ingenieros de software expertos

855
00:40:00,531 --> 00:40:03,972
de su red de alineadores para generar
trayectorias optimizadas para el

856
00:40:03,972 --> 00:40:04,670
entrenamiento.

857
00:40:04,680 --> 00:40:09,132
Obviamente, estos ingenieros evaluaron
estas interacciones bajo un criterio de

858
00:40:09,132 --> 00:40:13,295
aprobado o reprobado, pero también
calificaron cada respuesta en diversas

859
00:40:13,295 --> 00:40:17,342
dimensiones, como legibilidad y
rendimiento, y anotaron su razonamiento

860
00:40:17,342 --> 00:40:19,308
para cada calificación que dieron.

861
00:40:19,340 --> 00:40:23,522
Así que, básicamente, están mostrando
cada paso que da un ingeniero y cada

862
00:40:23,522 --> 00:40:28,162
pensamiento que tienen mientras hacen su
trabajo. Y esto es algo que nunca podrías

863
00:40:28,162 --> 00:40:32,115
obtener solo de los datos de uso. Y así,
LabelBox recopiló todas estas

864
00:40:32,115 --> 00:40:36,641
evaluaciones, incluyendo las trayectorias
de los agentes y las ediciones humanas

865
00:40:36,641 --> 00:40:39,448
correctivas, para que el cliente pudiera
entrenar.

866
00:40:39,460 --> 00:40:43,383
Este es solo un ejemplo, así que les
invito a que exploren cómo LabelBox puede

867
00:40:43,383 --> 00:40:47,510
proporcionarles datos de frontera de alta
calidad a través de diferentes dominios,

868
00:40:47,510 --> 00:40:50,771
modalidades y paradigmas de
entrenamiento. Pueden contactarnos en

869
00:40:50,771 --> 00:40:54,797
labelbox.com/themarkesh. Hablemos un poco
sobre el aprendizaje por refuerzo. Eh,

870
00:40:54,797 --> 00:40:57,803
ustedes también hicieron cosas muy
interesantes al respecto.

871
00:40:55,420 --> 00:40:55,605
Mmm.

872
00:40:57,880 --> 00:41:02,028
Eh, conceptualmente, ¿cómo deberíamos
abordar la forma en que los seres humanos

873
00:41:02,028 --> 00:41:05,965
son capaces de construir un modelo rico y
complejo del mundo simplemente al

874
00:41:05,965 --> 00:41:09,422
interactuar con nuestro entorno, y de
maneras que parecen ser casi

875
00:41:09,422 --> 00:41:12,880
independientes de la recompensa final al
término de cada episodio?

876
00:41:12,880 --> 00:41:13,344
Ajá.

877
00:41:13,420 --> 00:41:17,641
Si alguien, sabes, está empezando un
negocio, y al final de 10 años descubre

878
00:41:17,641 --> 00:41:21,918
si tuvo éxito o fracasó, decimos que ha
ganado mucha sabiduría y experiencia.

879
00:41:21,920 --> 00:41:22,523
Ajá, claro.

880
00:41:22,499 --> 00:41:25,819
Pero no es porque, como los registros de
actividad de cada cosa que pasó en los

881
00:41:25,819 --> 00:41:28,799
últimos diez años, estén ponderados al
alza o a la baja. Algo mucho más

882
00:41:28,799 --> 00:41:32,204
deliberado y, eh, rico está sucediendo.
¿Cómo... cuál es la analogía de ML y cómo

883
00:41:32,204 --> 00:41:34,759
se compara con lo que estamos haciendo
con otros ahora mismo?

884
00:41:34,840 --> 00:41:38,079
Sí, quizás lo diría así, uh, los humanos
no usan aprendizaje por refuerzo, es

885
00:41:38,079 --> 00:41:41,021
quizás lo que quiero decir. Creo que
hacen algo diferente, que es, sí,

886
00:41:41,021 --> 00:41:44,431
experimentas... Así que, el aprendizaje
por refuerzo es mucho peor de lo que creo

887
00:41:44,431 --> 00:41:45,753
que la persona promedio piensa.

888
00:41:47,779 --> 00:41:49,032
El aprendizaje por refuerzo es malo.

889
00:41:50,500 --> 00:41:53,797
Da la casualidad de que todo lo que
teníamos antes es mucho peor.

890
00:41:55,640 --> 00:41:58,603
Eh, porque antes solo imitábamos a la
gente, así que tiene todos estos

891
00:41:58,603 --> 00:42:01,867
problemas. Mmm, en el aprendizaje por
refuerzo, digamos que estás resolviendo

892
00:42:01,867 --> 00:42:05,303
un problema de matemáticas. Esto es muy
simple. Te dan un problema de matemáticas

893
00:42:05,303 --> 00:42:07,064
y estás tratando de encontrar la
solución.

894
00:42:07,980 --> 00:42:12,062
Pues, ahora, en el aprendizaje por
refuerzo, vas a probar muchísimas cosas

895
00:42:12,062 --> 00:42:13,181
en paralelo primero.

896
00:42:13,200 --> 00:42:16,188
Entonces, uh, te dan un problema.
Intentas cientos de intentos diferentes,

897
00:42:16,188 --> 00:42:19,259
y estos intentos pueden ser complejos,
¿verdad? Pueden ser como, "Oh, déjame

898
00:42:19,259 --> 00:42:22,453
intentar esto. Déjame intentar aquello.
Esto no funcionó. Aquello no funcionó,"

899
00:42:22,453 --> 00:42:25,523
etcétera. Y luego quizás obtienes una
respuesta, y ahora revisas la parte de

900
00:42:25,523 --> 00:42:28,389
atrás del libro, y ves, okay, la
respuesta correcta es esta. Y entonces

901
00:42:28,389 --> 00:42:31,624
puedes ver que, okay, este, este, y aquel
obtuvieron la respuesta correcta, pero

902
00:42:31,624 --> 00:42:33,262
estos otros noventa y siete de ellos no.

903
00:42:33,540 --> 00:42:36,538
Entonces, literalmente lo que hace el
aprendizaje por refuerzo es que va a

904
00:42:36,538 --> 00:42:39,741
aquellos que funcionaron muy bien, y cada
cosa que hiciste en el camino... cada

905
00:42:39,741 --> 00:42:42,863
token individual recibe un peso mayor,
como, haz más de esto. El problema con

906
00:42:42,863 --> 00:42:46,107
eso es, uh, quiero decir, la gente dirá
que, uh, tu estimador tiene una varianza

907
00:42:46,107 --> 00:42:48,818
alta, pero qué... quiero decir, es
simplemente ruidoso. Es ruidoso.

908
00:42:49,400 --> 00:42:52,789
Uh, básicamente, casi asume que cada
pequeña parte de la solución que creaste

909
00:42:52,789 --> 00:42:56,178
y que llegó a la respuesta correcta fue
lo correcto, lo cual no es verdad. Es

910
00:42:56,178 --> 00:42:59,121
como si hubieras tomado caminos
equivocados hasta que llegaste a la

911
00:42:59,121 --> 00:43:02,420
solución correcta. Cada una de esas cosas
incorrectas que hiciste, mientras

912
00:43:02,420 --> 00:43:05,809
llegaras a la solución correcta, será
recompensada como "haz más de esto". Es

913
00:43:05,809 --> 00:43:06,211
terrible.

914
00:43:06,520 --> 00:43:06,845
Claro.

915
00:43:06,860 --> 00:43:10,383
Es solo ruido. Has hecho todo este
trabajo solo para encontrar un único...

916
00:43:10,383 --> 00:43:14,196
Al final, obtienes un único número que
dice, "Oh, lo hiciste bien." Y, basándote

917
00:43:14,196 --> 00:43:17,912
en eso, sopesas toda esa trayectoria como
si fuera hacia arriba o hacia abajo.

918
00:43:18,160 --> 00:43:21,493
Y así... La forma en que lo veo es, estás
sorbiendo la supervisión con pajita, eh,

919
00:43:21,493 --> 00:43:24,579
porque has hecho todo este trabajo, que
podrían ser minutos de despliegue, y

920
00:43:24,579 --> 00:43:27,912
estás- estás como sorbiendo los pedacitos
de supervisión de la señal de recompensa

921
00:43:27,912 --> 00:43:29,723
final... con pajita, y lo estás como
poniendo

922
00:43:30,280 --> 00:43:31,208
Eres como es-

923
00:43:32,040 --> 00:43:35,418
Básicamente, uhm, sí, estás transmitiendo
eso por toda la trayectoria y usándolo

924
00:43:35,418 --> 00:43:38,924
para potenciar o reducir esa trayectoria.
Es simplemente estúpido y loco. Un humano

925
00:43:38,924 --> 00:43:41,961
nunca haría esto. Primero, un humano
nunca haría cientos de despliegues,

926
00:43:41,961 --> 00:43:42,303
¿verdad?

927
00:43:38,800 --> 00:43:38,939
Oh.

928
00:43:42,300 --> 00:43:42,671
Claro.

929
00:43:42,860 --> 00:43:46,586
Eh, en segundo lugar, cuando una persona,
digamos, encuentra una solución, tendrá

930
00:43:46,586 --> 00:43:49,846
un proceso de revisión bastante
complicado, como, "Vale, creo que estas

931
00:43:49,846 --> 00:43:53,386
partes las hice bien. Estas partes no las
hice tan bien. Debería hacer esto o

932
00:43:53,386 --> 00:43:57,113
aquello". Y reflexionan sobre las cosas.
No hay nada en los LLM actuales que haga

933
00:43:57,113 --> 00:43:58,417
esto. No hay un equivalente.

934
00:43:58,500 --> 00:44:02,128
Pero sí veo artículos que están saliendo
que intentan hacer esto, porque es

935
00:44:02,128 --> 00:44:05,855
evidente para todos en el área. Así que
lo veo como el primer aprendizaje por

936
00:44:05,855 --> 00:44:09,484
imitación, la verdad, fue sumamente
sorprendente, milagroso y asombroso que

937
00:44:09,484 --> 00:44:11,642
podamos, eh, afinar por imitación en
humanos.

938
00:44:03,940 --> 00:44:04,311
Claro.

939
00:44:11,980 --> 00:44:15,541
Eh, y fue increíble, porque al principio
solo teníamos modelos base. Los modelos

940
00:44:15,541 --> 00:44:16,623
base son autocompletado.

941
00:44:17,160 --> 00:44:20,915
Uh, y no me era obvio en ese momento, uh,
y tuve que aprender esto, y, uh, el

942
00:44:20,915 --> 00:44:24,472
artículo que me voló la cabeza fue
InstructGPT, porque señalaba que, oye,

943
00:44:24,472 --> 00:44:28,277
puedes entrenar el modelo preentrenado,
que es el autocompletado, y si solo lo

944
00:44:28,277 --> 00:44:31,686
ajustas con texto que parece
conversaciones, el modelo se adaptará muy

945
00:44:31,686 --> 00:44:34,799
rápidamente para volverse muy
conversacional. Y conserva todo el

946
00:44:34,799 --> 00:44:36,479
conocimiento del preentrenamiento.

947
00:44:36,720 --> 00:44:40,230
Y esto me voló la cabeza, porque no
entendía que estilísticamente puede

948
00:44:40,230 --> 00:44:44,092
ajustarse tan rápido y convertirse en un
asistente para un usuario a través de

949
00:44:44,092 --> 00:44:48,155
solo unos pocos ciclos de ajuste fino con
ese tipo de datos. Es muy milagroso para

950
00:44:48,155 --> 00:44:49,258
mí que eso funcionara.

951
00:44:49,440 --> 00:44:53,057
¡Qué increíble! Y eso fue como dos, tres
años de trabajo. Y entonces llegó el RL.

952
00:44:53,057 --> 00:44:56,359
Y el RL te permite hacerlo un poco mejor
que el aprendizaje por imitación,

953
00:44:56,359 --> 00:44:59,750
¿verdad? Porque puedes tener estas
funciones de recompensa y puedes subir la

954
00:44:59,750 --> 00:45:02,826
pendiente según las funciones de
recompensa. Y así, algunos problemas

955
00:45:02,826 --> 00:45:06,353
tienen respuestas correctas, puedes subir
la pendiente sin obtener trayectorias

956
00:45:06,353 --> 00:45:08,433
expertas para imitar. Así que eso es
asombroso.

957
00:45:08,440 --> 00:45:11,597
Y el modelo puede hallar soluciones que
el humano jamás idearía.

958
00:45:11,620 --> 00:45:12,270
Sí, sí, entiendo.

959
00:45:12,340 --> 00:45:15,033
Así que esto es increíble, y sin embargo
sigue siendo estúpido.

960
00:45:15,980 --> 00:45:19,912
Bueno, creo que necesitamos, eh,
necesitamos mucho más. Y entonces vi un

961
00:45:19,912 --> 00:45:24,176
artículo de Google ayer que intentaba
tener esta idea de reflexión y revisión,

962
00:45:24,176 --> 00:45:24,896
eh, en mente.

963
00:45:24,940 --> 00:45:28,563
Eh, ¿cuál era el... artículo de banco de
memoria o algo así? No lo sé. Eh, de

964
00:45:28,563 --> 00:45:32,044
hecho he visto varios artículos en esta
línea. Así que espero que haya una

965
00:45:32,044 --> 00:45:35,667
actualización importante en cómo hacemos
los algoritmos para los LLM, eh, que

966
00:45:35,667 --> 00:45:39,243
vendrá en ese ámbito. Y luego creo que
necesitamos tres, cuatro o cinco más.

967
00:45:40,640 --> 00:45:42,218
Pues, sí, algo por el estilo, diría yo.

968
00:45:42,240 --> 00:45:46,483
Pero eres, eres, eres tan bueno creando
frases tan evocadoras, tan evocadoras.

969
00:45:46,483 --> 00:45:49,623
Chupar la supervisión con una pajita...
es algo tan bueno.

970
00:45:49,720 --> 00:45:53,503
Eh, ¿por qué no ha...? Entonces, ¿estás
diciendo que tu problema con la

971
00:45:53,503 --> 00:45:57,719
recompensa basada en resultados es que
tienes esta enorme trayectoria, y luego,

972
00:45:57,719 --> 00:46:02,042
al final, estás intentando aprender cada
cosa posible sobre lo que deberías hacer

973
00:46:02,042 --> 00:46:05,556
y lo que deberías aprender del mundo de
ese único resultado final?

974
00:46:05,620 --> 00:46:06,316
Ajá.

975
00:46:06,480 --> 00:46:09,887
Eh, ¿por qué no ha... dado que esto es
obvio, ¿por qué la supervisión basada en

976
00:46:09,887 --> 00:46:13,032
procesos, como alternativa, no ha sido
una forma exitosa de hacer que los

977
00:46:13,032 --> 00:46:16,265
modelos sean más capaces? ¿Qué nos ha
estado impidiendo usar este paradigma

978
00:46:16,265 --> 00:46:16,789
alternativo?

979
00:46:12,780 --> 00:46:13,012
Sí

980
00:46:16,760 --> 00:46:20,347
Así que la supervisión por procesos se
refiere a que no habrá recompensa solo al

981
00:46:20,347 --> 00:46:23,889
final de... después de diez minutos de
trabajo... No te diré si lo hiciste bien

982
00:46:23,889 --> 00:46:26,976
o mal. Te diré en cada paso del camino lo
bien que lo estás haciendo.

983
00:46:24,000 --> 00:46:24,371
Ajá.

984
00:46:27,040 --> 00:46:30,105
Mmm, y esto es básicamente... La razón
por la que no tenemos eso es... no es

985
00:46:30,105 --> 00:46:33,375
sencillo... es complicado hacerlo bien.
Mmm, porque tienes soluciones parciales y

986
00:46:33,375 --> 00:46:36,727
no sabes cómo asignar el crédito. Así que
cuando obtienes la respuesta correcta, es

987
00:46:36,727 --> 00:46:38,975
solo, uh, una coincidencia de igualdad
con la respuesta.

988
00:46:31,360 --> 00:46:31,638
Mmm.

989
00:46:39,040 --> 00:46:39,829
Muy simple de aplicar.

990
00:46:40,660 --> 00:46:40,752
Sí.

991
00:46:40,820 --> 00:46:43,915
Si haces básicamente supervisión de
procesos, ¿cómo asignas, de forma

992
00:46:43,915 --> 00:46:47,512
automatizada, el crédito parcial? No es
obvio cómo se hace. Muchos laboratorios,

993
00:46:47,512 --> 00:46:51,109
creo, están intentando hacerlo con estos
jueces LLM. Así que, básicamente, haces

994
00:46:51,109 --> 00:46:52,429
que los LLM intenten hacerlo.

995
00:46:52,480 --> 00:46:55,810
Así que le pides a un LLM: "Oye, mira la
solución parcial de un estudiante. ¿Qué

996
00:46:55,810 --> 00:46:59,056
tan bien crees que lo está haciendo si la
respuesta es esta?" Y ellos intentan

997
00:46:59,056 --> 00:47:02,091
ajustar la instrucción. Mmm, la razón por
la que creo que esto es un poco

998
00:47:02,091 --> 00:47:05,211
complicado es bastante sutil, y es el
hecho de que cada vez que usas un LLM

999
00:47:05,211 --> 00:47:08,583
para asignar una recompensa, esos LLM son
cosas gigantes con miles de millones de

1000
00:47:08,583 --> 00:47:09,848
parámetros y son manipulables.

1001
00:47:09,880 --> 00:47:13,492
Y si estás aprendiendo por refuerzo con
respecto a ellos... encontrarás ejemplos

1002
00:47:13,492 --> 00:47:15,824
adversarios para tus jueces de LM, casi
garantizado.

1003
00:47:15,820 --> 00:47:19,204
No puedes hacer esto por demasiado
tiempo. Haces quizás 10 o 20 pasos, y

1004
00:47:19,204 --> 00:47:23,018
quizás funcione. Pero no puedes hacer 100
o 1.000 porque no es obvio, porque, eh.

1005
00:47:23,440 --> 00:47:27,394
Sé, entiendo que no es obvio, pero
básicamente el modelo encontrará pequeñas

1006
00:47:27,394 --> 00:47:30,979
grietas. Uh, uh, hallará todas estas
cosas espurias en los rincones y

1007
00:47:30,979 --> 00:47:34,353
recovecos del modelo gigante y encontrará
una forma de engañarlo.

1008
00:47:34,820 --> 00:47:38,257
Así que un ejemplo, uh, que tengo muy
presente es, creo que esto, creo que esto

1009
00:47:38,257 --> 00:47:41,518
fue probablemente público, uh, pero
básicamente, si estás usando un juez de

1010
00:47:41,518 --> 00:47:44,955
LM para una recompensa, entonces solo le
das una solución de un estudiante y le

1011
00:47:44,955 --> 00:47:46,894
preguntas si el estudiante lo hizo bien o
no.

1012
00:47:47,360 --> 00:47:50,668
Estábamos entrenando con aprendizaje por
refuerzo contra esa función de

1013
00:47:50,668 --> 00:47:54,259
recompensa, y funcionó muy bien, y luego,
eh, de repente la recompensa creció

1014
00:47:54,259 --> 00:47:57,898
enormemente. Hubo un salto masivo y lo
hizo perfecto. Y lo miras y dices, wow,

1015
00:47:57,898 --> 00:48:01,585
esto- esto significa que el estudiante es
perfecto en todos estos problemas, ha

1016
00:48:01,585 --> 00:48:03,428
resuelto completamente las matemáticas.

1017
00:48:03,600 --> 00:48:06,559
Pero en realidad lo que pasa es que
cuando miras las terminaciones que

1018
00:48:06,559 --> 00:48:09,861
obtienes del modelo, son un completo
disparate. Empiezan bien, y luego cambian

1019
00:48:09,861 --> 00:48:13,335
a... el, el, el, el, el, el, el. Así que
es como, oh, vale, cojamos dos más tres y

1020
00:48:13,335 --> 00:48:16,851
hacemos esto y esto y el, el, el, el, el,
el, el. Y lo miras y es como, esto es una

1021
00:48:16,851 --> 00:48:19,853
locura. Cómo está obteniendo una
recompensa de uno o del cien por cien?

1022
00:48:11,020 --> 00:48:11,252
Mmm.

1023
00:48:20,140 --> 00:48:23,378
Eh, y miras al juez LM y resulta que el,
el, el, el, el, el es un ejemplo

1024
00:48:23,378 --> 00:48:26,885
adversario para el modelo, y le asigna el
cien por ciento de probabilidad. Y es

1025
00:48:26,885 --> 00:48:30,483
solo porque este es un ejemplo fuera de
la muestra para el LLM. Nunca lo ha visto

1026
00:48:30,483 --> 00:48:33,991
durante el entrenamiento, y estás en el
ámbito de la pura generalización. Nunca

1027
00:48:33,991 --> 00:48:37,499
lo ha visto durante el entrenamiento, y
en el ámbito de la pura generalización,

1028
00:48:37,499 --> 00:48:40,062
puedes encontrar estos ejemplos que, que,
uh, lo quiebran.

1029
00:48:33,700 --> 00:48:33,978
Claro.

1030
00:48:40,100 --> 00:48:43,112
Estás- estás básicamente entrenando al
modelo de lenguaje grande para ser un

1031
00:48:43,112 --> 00:48:44,558
modelo de inyección de prompts. [Eh]

1032
00:48:44,600 --> 00:48:47,853
Ni siquiera eso. La inyección de prompts
es demasiado sofisticada. Estás

1033
00:48:47,853 --> 00:48:51,336
encontrando ejemplos adversarios, como se
les llama. Estas son soluciones sin

1034
00:48:51,336 --> 00:48:55,048
sentido, uh, que son obviamente erróneas,
pero el modelo las considera asombrosas.

1035
00:48:55,420 --> 00:48:58,145
Así que, si consideras que este es el
principal cuello de botella para que el

1036
00:48:58,145 --> 00:49:00,692
aprendizaje por refuerzo sea más
funcional, entonces eso exigirá que los

1037
00:49:00,692 --> 00:49:03,597
grandes modelos de lenguaje sean mejores
jueces si quieres lograr esto- ... de una

1038
00:49:03,597 --> 00:49:06,107
forma completamente automatizada. Y
entonces, uh, ¿será simplemente una

1039
00:49:06,107 --> 00:49:08,833
especie de enfoque tipo GAN donde se tuvo
que entrenar a los modelos para que

1040
00:49:08,833 --> 00:49:09,909
fueran más robustos- ... para-

1041
00:49:09,120 --> 00:49:09,305
Sí

1042
00:49:09,960 --> 00:49:12,967
Creo que los laboratorios probablemente
están haciendo todo eso. Como, bueno, lo

1043
00:49:12,967 --> 00:49:15,823
obvio es que el, el, el no debería
obtener el cien por ciento de recompensa.

1044
00:49:15,823 --> 00:49:18,868
Bueno, toma el, el, el, el, ponlo en el
conjunto de entrenamiento del juez del LM

1045
00:49:18,868 --> 00:49:21,838
y di: "Esto no es el cien por ciento.
Esto es el cero por ciento." Puedes hacer

1046
00:49:21,838 --> 00:49:24,465
esto. Pero cada vez que haces esto,
obtienes un nuevo LLM, y aún tiene

1047
00:49:24,465 --> 00:49:27,130
ejemplos adversarios. Hay infinidad de
ejemplos adversarios. Y creo que

1048
00:49:27,130 --> 00:49:29,909
probablemente si iteras esto unas cuantas
veces, será cada vez más difícil

1049
00:49:29,909 --> 00:49:31,090
encontrar ejemplos adversarios.

1050
00:49:31,100 --> 00:49:31,517
Pues, a ver.

1051
00:49:31,520 --> 00:49:34,755
Pero no estoy cien por ciento seguro
porque esta cosa tiene un billón de

1052
00:49:34,755 --> 00:49:38,309
parámetros o algo por el estilo. [eh] Así
que apuesto a que los laboratorios lo

1053
00:49:38,309 --> 00:49:41,954
están intentando. [eh] En realidad no...
[pausa] Yo sigo pensando... [pausa] Sigo

1054
00:49:41,954 --> 00:49:43,640
pensando que necesitamos otras ideas.

1055
00:49:44,627 --> 00:49:49,642
Interesante. ¿Tienes, uhm, alguna noción
de qué forma podría tener la otra idea?

1056
00:49:49,868 --> 00:49:53,026
Eh, entonces, como, esta, eh, esta idea
de, como, una solución de re-revisión,

1057
00:49:53,026 --> 00:49:56,184
um- ... de revisión, incluida en ejemplos
sintéticos. De tal manera que cuando

1058
00:49:56,184 --> 00:49:59,178
entrenas con ellos, te, eh, te vuelves
mejor. Y, como, lo meta-aprendes de

1059
00:49:59,178 --> 00:50:02,131
alguna manera. Y creo que hay algunos
artículos que estoy empezando a ver

1060
00:50:02,131 --> 00:50:05,372
surgir. Solo estoy en una etapa de, como,
leer resúmenes, porque muchos de estos

1061
00:50:05,372 --> 00:50:06,725
artículos, sabes, son solo ideas.

1062
00:49:53,228 --> 00:49:53,367
Sí

1063
00:50:06,768 --> 00:50:10,152
Alguien tiene que realmente hacerlo
funcionar a escala de un laboratorio de

1064
00:50:10,152 --> 00:50:13,902
LLM de vanguardia, con total generalidad.
Porque cuando ves estos artículos, surgen

1065
00:50:13,902 --> 00:50:17,607
y son un poco confusos, ¿sabes? Son ideas
geniales, pero en realidad no he visto a

1066
00:50:17,607 --> 00:50:20,900
nadie que demuestre convincentemente que
esto es posible. Dicho esto, los

1067
00:50:20,900 --> 00:50:24,193
laboratorios de LLM son bastante
herméticos. Así que quién sabe qué están

1068
00:50:24,193 --> 00:50:24,879
haciendo ahora.

1069
00:50:24,908 --> 00:50:25,140
Mmm.

1070
00:50:25,728 --> 00:50:26,192
Pero sí.

1071
00:50:26,328 --> 00:50:30,580
Así que supongo que, eh, veo una forma no
fácil, pero sí puedo conceptualizar cómo

1072
00:50:30,580 --> 00:50:34,780
podrías entrenar con ejemplos sintéticos.
O problemas sintéticos que tú mismo has

1073
00:50:34,780 --> 00:50:38,822
creado. Pero parece haber otra cosa que
los humanos hacen, quizás el sueño sea

1074
00:50:38,822 --> 00:50:40,817
esto, quizás soñar despierto sea esto.

1075
00:50:40,828 --> 00:50:41,153
Ajá.

1076
00:50:41,548 --> 00:50:45,492
Lo cual no es necesariamente inventar
problemas falsos, sino simplemente, como,

1077
00:50:45,492 --> 00:50:46,099
reflexionar.

1078
00:50:46,188 --> 00:50:46,513
Claro.

1079
00:50:46,567 --> 00:50:49,630
Y no estoy seguro de cuál sería la
analogía en ML para, ya sabes, soñar

1080
00:50:49,630 --> 00:50:53,000
despierto o dormir. Pero solo, como, al
reflexionar, no he encontrado un nuevo

1081
00:50:53,000 --> 00:50:56,195
problema. Quiero decir, obviamente, la
analogía más básica sería, como, un

1082
00:50:56,195 --> 00:50:59,390
ajuste fino en fragmentos de reflexión,
pero siento que en la práctica eso

1083
00:50:59,390 --> 00:51:02,717
probablemente no funcionaría tan bien.
Así que no sé si tienes alguna opinión

1084
00:51:02,717 --> 00:51:04,817
sobre cuál sería la analogía de, como,
esta cosa.

1085
00:50:52,368 --> 00:50:52,693
Sí, sí.

1086
00:51:05,087 --> 00:51:08,090
Sí, creo que, que nos faltan algunos
aspectos ahí. Así que, por ejemplo,

1087
00:51:08,090 --> 00:51:09,359
cuando estás leyendo un libro-

1088
00:51:09,368 --> 00:51:09,460
Sí

1089
00:51:09,567 --> 00:51:12,488
Eh, casi siento que actualmente cuando
los LLM leen un libro, lo que eso

1090
00:51:12,488 --> 00:51:15,779
significa es que estiramos la secuencia
de texto y el modelo predice el siguiente

1091
00:51:15,779 --> 00:51:18,700
token. Y de ahí obtiene algo de
conocimiento. Eh, eso no es realmente lo

1092
00:51:18,700 --> 00:51:21,951
que hacen los humanos, ¿verdad? Entonces,
cuando lees un libro, casi ni siquiera

1093
00:51:21,951 --> 00:51:24,954
siento que el libro sea como una
exposición a la que deba atender y con la

1094
00:51:24,954 --> 00:51:28,246
que deba entrenar. El libro es un, es un
conjunto de indicaciones para mí... para

1095
00:51:28,246 --> 00:51:29,768
hacer generación de datos sintéticos.

1096
00:51:25,508 --> 00:51:25,833
Ajá.

1097
00:51:30,148 --> 00:51:33,716
O para que te unas a un club de lectura y
hables de ello con tus amigos. Y es

1098
00:51:33,716 --> 00:51:37,050
manipulando esa información como
realmente adquieres ese conocimiento. Y

1099
00:51:37,050 --> 00:51:40,525
yo, creo que no tenemos un equivalente de
eso, de nuevo, con los LLM. Ellos

1100
00:51:40,525 --> 00:51:43,154
realmente no hacen eso, pero me
encantaría ver durante el

1101
00:51:43,154 --> 00:51:46,723
pre-entrenamiento algún tipo de etapa
que, uh, analice el material y trate de

1102
00:51:46,723 --> 00:51:50,198
conciliarlo con lo que ya sabe y lo
analice por, digamos, un tiempo, y, um,

1103
00:51:50,198 --> 00:51:51,278
logre que eso funcione.

1104
00:51:32,948 --> 00:51:33,133
Sí.

1105
00:51:36,627 --> 00:51:36,952
Claro.

1106
00:51:51,828 --> 00:51:55,274
Y no hay equivalencia de nada de esto.
Todo esto es investigación. Hay razones

1107
00:51:55,274 --> 00:51:58,766
sutiles, muy sutiles, que creo que son
muy difíciles de entender por las que no

1108
00:51:58,766 --> 00:51:59,258
es trivial.

1109
00:51:59,288 --> 00:52:00,588
Así que si puedo describir uno.

1110
00:52:01,627 --> 00:52:03,345
¿Por qué no generarlo y entrenar
sintéticamente?

1111
00:52:03,368 --> 00:52:03,785
Entiendo.

1112
00:52:03,968 --> 00:52:07,229
Bueno, porque cada ejemplo sintético,
como si solo doy una generación sintética

1113
00:52:07,229 --> 00:52:10,617
del modelo pensando en un libro, lo miras
y dices: "Esto se ve genial. ¿Por qué no

1114
00:52:10,617 --> 00:52:13,878
puedo entrenar con esto?" Bueno, podrías
intentarlo, pero el modelo en realidad

1115
00:52:13,878 --> 00:52:17,098
empeorará mucho si sigues intentándolo. Y
eso es porque todas las muestras que

1116
00:52:17,098 --> 00:52:20,486
obtienes de los modelos están colapsadas
silenciosamente. Están silenciosamente...

1117
00:52:20,486 --> 00:52:21,197
Esto no es obvio.

1118
00:52:21,228 --> 00:52:25,150
Si observas cualquier ejemplo individual
de ello, ocupan una variedad muy pequeña

1119
00:52:25,150 --> 00:52:28,876
del espacio posible de, um, digamos,
pensamientos sobre el contenido. Así que

1120
00:52:28,876 --> 00:52:31,769
los LLM, cuando salen, uh, están lo que
llamamos colapsados.

1121
00:52:31,808 --> 00:52:34,545
Tienen una distribución de datos
colapsada. Si intentas tomar una

1122
00:52:34,545 --> 00:52:38,009
muestra... Una manera fácil de explicarlo
es ir a ChatGPT y preguntarle: "Cuéntame

1123
00:52:38,009 --> 00:52:41,131
un chiste". Apenas conoce como tres
chistes. No te está ofreciendo toda la

1124
00:52:41,131 --> 00:52:44,553
amplia gama de chistes posibles. Te está
dando, como... Sabe, como, tres chistes.

1125
00:52:44,553 --> 00:52:45,879
Están completamente colapsados.

1126
00:52:42,208 --> 00:52:42,579
Ajá.

1127
00:52:44,268 --> 00:52:44,639
Ajá.

1128
00:52:46,028 --> 00:52:49,188
Entonces, básicamente, no estás
obteniendo la riqueza, la diversidad y la

1129
00:52:49,188 --> 00:52:52,437
entropía, uh, de estos modelos como lo
harías de los humanos. Entonces, los

1130
00:52:52,437 --> 00:52:55,905
humanos son mucho más ruidosos, pero al
menos no están sesgados. No están, um...

1131
00:52:55,905 --> 00:52:59,241
En un sentido estadístico, no están
colapsados silenciosamente. Mantienen una

1132
00:52:59,241 --> 00:53:00,470
enorme cantidad de entropía.

1133
00:53:00,668 --> 00:53:04,108
Entonces, ¿cómo se logra que la
generación sintética funcione a pesar del

1134
00:53:04,108 --> 00:53:07,262
colapso y manteniendo la entropía? Es un
problema de investigación.

1135
00:53:07,388 --> 00:53:11,853
Eh, para confirmar que entendí, la razón
por la que el colapso es relevante para

1136
00:53:11,853 --> 00:53:16,149
la generación de datos sintéticos es
porque quieres poder generar problemas o

1137
00:53:16,149 --> 00:53:19,880
reflexiones sintéticas que no están ya en
tu distribución de datos?

1138
00:53:20,228 --> 00:53:24,027
Supongo que lo que quiero decir es, eh,
digamos que tenemos un capítulo de un

1139
00:53:24,027 --> 00:53:27,777
libro y le pido a un LLM que piense en
ello. Eh, te dará algo que parece muy

1140
00:53:27,777 --> 00:53:31,577
razonable. Pero si le pregunto 10 veces,
notarás que todas... son exactamente

1141
00:53:31,577 --> 00:53:31,977
iguales.

1142
00:53:28,148 --> 00:53:28,380
Sí.

1143
00:53:32,008 --> 00:53:34,701
Tú... puedes simplemente seguir
es-ca-lan-do... es-ca-lan-do, entre

1144
00:53:34,701 --> 00:53:36,048
comillas, la "reflexión" sobre...

1145
00:53:36,607 --> 00:53:39,973
la misma cantidad de- ... eh, sabes,
información de la solicitud- ... y luego

1146
00:53:39,973 --> 00:53:41,390
obtener resultados de eso. Vale.

1147
00:53:41,148 --> 00:53:44,124
Sí. Sí, sí. Sí, así que cualquier muestra
individual se verá bien, pero la

1148
00:53:44,124 --> 00:53:47,182
distribución de ella es, es bastante
terrible. Y es bastante terrible de tal

1149
00:53:47,182 --> 00:53:50,159
manera que si sigues entrenando con
demasiado material propio, en realidad

1150
00:53:50,159 --> 00:53:53,462
colapsas. De hecho, creo que, eh, no hay,
como, soluciones fundamentales para esto

1151
00:53:53,462 --> 00:53:56,194
posiblemente. Y también creo que los
humanos colapsan con el tiempo.

1152
00:53:45,928 --> 00:53:46,253
Vaya.

1153
00:53:56,348 --> 00:53:59,466
Eh, creo que esto es, eh... De nuevo,
estas analogías son sorprendentemente

1154
00:53:59,466 --> 00:54:02,922
buenas, pero los seres humanos colapsan a
lo largo de sus vidas. Por eso los niños,

1155
00:54:02,922 --> 00:54:06,336
eh, tienen, eh, una mente completamente,
um... Sabes, aún no se han sobreajustado.

1156
00:54:06,336 --> 00:54:09,624
Y dirán cosas que te sorprenderán porque
es como... Puedes ver de dónde vienen-

1157
00:54:09,624 --> 00:54:11,394
... pero no es lo que la gente suele
decir.

1158
00:54:05,548 --> 00:54:05,826
Claro.

1159
00:54:11,388 --> 00:54:11,852
Así es.

1160
00:54:12,087 --> 00:54:15,741
Y porque todavía no han colapsado. Pero
nosotros sí, nosotros colapsamos, y

1161
00:54:15,741 --> 00:54:19,100
terminamos volviendo a los mismos
pensamientos. Terminamos, ya sabes,

1162
00:54:19,100 --> 00:54:22,854
repitiendo una y otra vez las mismas
cosas. Y las tasas de aprendizaje bajan,

1163
00:54:22,854 --> 00:54:26,360
y el colapso sigue empeorando cada vez
más. Y entonces, uhm, sí, todo se

1164
00:54:26,360 --> 00:54:26,854
deteriora.

1165
00:54:27,308 --> 00:54:31,160
Ha- has, ¿has visto este artículo súper
interesante que dice que soñar es una

1166
00:54:31,160 --> 00:54:34,657
forma de prevenir este tipo de
sobreajuste y colapso? Que la razón por

1167
00:54:34,657 --> 00:54:38,712
la que soñar es, uh- ... evolutivamente
adaptativo es para ponerte en situaciones

1168
00:54:38,712 --> 00:54:42,311
extrañas- ... que son, como, muy
diferentes a tu realidad cotidiana- ...

1169
00:54:42,311 --> 00:54:44,490
para así, prevenir este tipo de
sobreajuste.

1170
00:54:44,508 --> 00:54:47,445
Es una idea interesante. Quiero decir,
creo que cuando generas cosas en tu

1171
00:54:47,445 --> 00:54:50,503
cabeza y luego les prestas atención, es
como si estuvieras entrenando con tus

1172
00:54:50,503 --> 00:54:53,601
propias muestras. Estás entrenando con
tus datos sintéticos. Y si lo haces por

1173
00:54:53,601 --> 00:54:56,257
demasiado tiempo, te descarrilarás- ...
eh, y colapsarás demasiado.

1174
00:54:56,288 --> 00:55:00,633
Así que siempre tienes que, como, buscar,
eh, entropía en tu vida. Eh, hablar con

1175
00:55:00,633 --> 00:55:04,816
otras personas es una gran fuente de
entropía, y, eh, cosas por el estilo. Así

1176
00:55:04,816 --> 00:55:08,673
que quizás el cerebro también ha
construido algunos mecanismos internos,

1177
00:55:08,673 --> 00:55:12,475
eh, para aumentar la cantidad de
entropía, eh, en ese proceso. Pero sí,

1178
00:55:12,475 --> 00:55:14,213
quizás sea una idea interesante.

1179
00:55:00,408 --> 00:55:00,825
Así es.

1180
00:55:14,308 --> 00:55:18,304
Uh, esta es una idea muy mal formulada,
así que la- la voy a soltar y de- dejaré

1181
00:55:18,304 --> 00:55:21,491
que reacciones a ella. Los mejores
aprendices de los que tenemos

1182
00:55:21,491 --> 00:55:25,032
conocimiento, que son los niños, son
extremadamente malos para recordar

1183
00:55:25,032 --> 00:55:25,639
información.

1184
00:55:25,648 --> 00:55:28,777
De hecho, en las primeras etapas de la
infancia lo olvidarás todo. Eres

1185
00:55:28,777 --> 00:55:32,041
simplemente amnésico de todo lo que
ocurre antes de una cierta, uh, fecha.

1186
00:55:32,041 --> 00:55:35,529
Pero eres, uh, extremadamente bueno para
aprender nuevos idiomas y aprender del

1187
00:55:35,529 --> 00:55:38,792
mundo. Y quizás haya algún elemento de
poder ver el bosque más allá de los

1188
00:55:38,792 --> 00:55:42,012
árboles. Mientras que si lo comparas con
el extremo opuesto del espectro,

1189
00:55:42,012 --> 00:55:42,459
tienes-...

1190
00:55:35,788 --> 00:55:36,066
Mmm, mmm.

1191
00:55:38,587 --> 00:55:39,051
Ajá.

1192
00:55:42,580 --> 00:55:46,386
El preentrenamiento de los LLM, que estos
modelos regurgitarán literalmente,

1193
00:55:46,386 --> 00:55:49,940
palabra por palabra, qué es lo
siguiente... en una página de Wikipedia.

1194
00:55:49,940 --> 00:55:54,102
Pero su capacidad para aprender conceptos
abstractos muy rápidamente, como lo haría

1195
00:55:54,102 --> 00:55:55,676
un niño, es mucho más limitada.

1196
00:55:50,080 --> 00:55:50,358
Claro.

1197
00:55:55,720 --> 00:55:58,613
Y luego los adultos se encuentran en un
punto intermedio, donde no poseen la

1198
00:55:58,613 --> 00:56:01,545
misma flexibilidad que el aprendizaje de
la infancia. Pero pueden, sabes, los

1199
00:56:01,545 --> 00:56:04,516
adultos sí pueden memorizar datos y mucha
información de una forma que resulta

1200
00:56:04,516 --> 00:56:07,602
mucho más complicada para los niños. Y no
sé si hay algo realmente interesante en

1201
00:56:07,602 --> 00:56:08,258
todo eso, ¿sabes?

1202
00:56:00,200 --> 00:56:00,617
Ajá.

1203
00:56:08,260 --> 00:56:12,054
Creo que hay algo muy interesante en eso.
Sí, completamente de acuerdo. Sí creo que

1204
00:56:12,054 --> 00:56:15,617
los humanos, de hecho, eh, tienen una
capacidad mucho mayor, comparado con los

1205
00:56:15,617 --> 00:56:19,181
LLM, de ver el bosque más allá de los
árboles. Y no somos realmente tan buenos

1206
00:56:19,181 --> 00:56:21,356
memorizando, lo cual es en realidad una
ventaja.

1207
00:56:09,440 --> 00:56:09,718
Rango, sí.

1208
00:56:17,320 --> 00:56:17,784
Así es.

1209
00:56:21,380 --> 00:56:22,123
Ajá.

1210
00:56:22,180 --> 00:56:26,468
Bueno, porque no somos tan buenos para
memorizar, en realidad nos vemos como

1211
00:56:26,468 --> 00:56:30,986
forzados a encontrar los patrones, de una
manera mucho más general. Creo que los

1212
00:56:30,986 --> 00:56:35,560
LLM, en comparación, son extremadamente
buenos en la memorización. Pueden recitar

1213
00:56:35,560 --> 00:56:38,248
pasajes de todas estas fuentes de
entrenamiento.

1214
00:56:38,360 --> 00:56:41,674
Eh, puedes darles datos completamente sin
sentido. Como puedes tomar, eh, puedes

1215
00:56:41,674 --> 00:56:44,905
hashear una cantidad de texto o algo así.
Obtienes una secuencia completamente

1216
00:56:44,905 --> 00:56:48,262
aleatoria. Si lo entrenas, incluso solo,
creo, una sola iteración o dos, puede de

1217
00:56:48,262 --> 00:56:51,534
repente regurgitarlo todo. Lo memorizará.
No hay forma de que una persona pueda

1218
00:56:51,534 --> 00:56:54,010
leer una sola secuencia de números
aleatorios y recitártela.

1219
00:56:54,080 --> 00:56:54,451
No.

1220
00:56:54,720 --> 00:56:58,519
Y eso es una característica, no un error,
casi, eh, porque te obliga, digamos, a

1221
00:56:58,519 --> 00:57:01,789
aprender solo los componentes
generalizables. Mientras que los LMS se

1222
00:57:01,789 --> 00:57:05,492
distraen con toda la memoria que tienen
de los documentos pre-entrenados. Y es

1223
00:57:05,492 --> 00:57:09,243
probable que les resulte muy distractor,
eh, en cierto sentido. Por eso, cuando

1224
00:57:09,243 --> 00:57:13,042
hablo del núcleo cognitivo, en realidad
quiero eliminar la memoria, que es de lo

1225
00:57:13,042 --> 00:57:13,667
que hablamos.

1226
00:57:13,800 --> 00:57:18,017
Me encantaría que tuvieran, eh, menos
memoria, para que tuvieran que buscar las

1227
00:57:18,017 --> 00:57:21,911
cosas, y que solo mantuvieran los
algoritmos para el pensamiento, la idea

1228
00:57:21,911 --> 00:57:25,967
de un experimento, y todo este pegamento
cognitivo, digamos, para la acción.

1229
00:57:25,980 --> 00:57:29,323
Y esto es también relevante para la
prevención del colapso del modelo.

1230
00:57:29,580 --> 00:57:31,205
A ver, déjame pensar.

1231
00:57:34,880 --> 00:57:38,026
No estoy seguro. Creo que es casi como un
eje separado. Es casi como que los

1232
00:57:38,026 --> 00:57:40,795
modelos son demasiado buenos en la
memorización, y de alguna manera

1233
00:57:40,795 --> 00:57:44,067
deberíamos, deberíamos quitar eso. Y creo
que la gente, la gente es mucho peor,

1234
00:57:44,067 --> 00:57:44,864
pero es algo bueno.

1235
00:57:37,080 --> 00:57:37,405
Ajá.

1236
00:57:45,580 --> 00:57:49,166
Mmm. ¿Cuál es una solución al colapso del
modelo? Es decir, po- o-kay, hay cosas

1237
00:57:49,166 --> 00:57:52,662
muy ingenuas que podrías intentar como,
uhm, la distribución sobre Sí. ... Sí.

1238
00:57:52,662 --> 00:57:56,249
... las tiendas debería ser más amplia o
algo así. Sí. Hay muchas cosas ingenuas

1239
00:57:56,249 --> 00:57:59,790
que podrías probar. ¿Cuál termina siendo
el problema con los enfoques ingenuos?

1240
00:58:00,440 --> 00:58:03,470
Sí, creo que es una buena pregunta. Es
decir, puedes imaginar regularizar la

1241
00:58:03,470 --> 00:58:06,097
entropía y cosas así. Supongo que
simplemente no funciona tan bien

1242
00:58:06,097 --> 00:58:06,662
empíricamente.

1243
00:58:06,660 --> 00:58:10,302
Porque, eh, ahora mismo, como que los
modelos están colapsados. Pero debo

1244
00:58:10,302 --> 00:58:14,300
decir, eh, que la mayoría de las tareas
que les pedimos en realidad no requieren

1245
00:58:14,300 --> 00:58:17,943
diversidad. Esa es probablemente la
respuesta a lo que está sucediendo. Y

1246
00:58:17,943 --> 00:58:21,586
entonces, es que los Laboratorios
Frontier están intentando hacer que los

1247
00:58:21,586 --> 00:58:25,583
modelos sean útiles, y como que siento
que la diversidad de los resultados no es

1248
00:58:25,583 --> 00:58:26,443
tan importante...

1249
00:58:16,460 --> 00:58:16,877
Pues, a ver.

1250
00:58:26,440 --> 00:58:29,841
En primer lugar, es mucho más difícil
trabajar y evaluar todo esto. Pero quizás

1251
00:58:29,841 --> 00:58:32,152
no es lo que realmente capta la mayor
parte del valor.

1252
00:58:31,120 --> 00:58:31,445
Claro.

1253
00:58:32,140 --> 00:58:35,791
De hecho, se penaliza activamente, ¿no es
así? Si tú, si eres como súper creativo

1254
00:58:35,791 --> 00:58:37,434
en la vida real... no es algo bueno.

1255
00:58:37,460 --> 00:58:40,879
Sí. O, por ejemplo, si recibes mucha
ayuda para escribir de los LM... y cosas

1256
00:58:40,879 --> 00:58:44,344
por el estilo, creo que es probablemente
malo porque los modelos te darán esto

1257
00:58:44,344 --> 00:58:47,583
como de forma implícita, todo lo mismo,
¿sabes? Eh, así que no son, em...

1258
00:58:47,620 --> 00:58:50,875
No exploran muchas formas de responder
una pregunta, ¿verdad? Pero siento que

1259
00:58:50,875 --> 00:58:53,959
esta diversidad no es tan grande... Sí,
quizás, no tantas aplicaciones lo

1260
00:58:53,959 --> 00:58:57,215
necesitan, por eso los modelos no la
tienen. Pero entonces es un problema con

1261
00:58:57,215 --> 00:59:00,384
el tiempo de generación sintética,
etcétera. Así que nos estamos disparando

1262
00:59:00,384 --> 00:59:03,511
en el pie al no permitir que esta
entropía mantenga, eh, el modelo. Y creo

1263
00:59:03,511 --> 00:59:06,381
que, uh, posiblemente, uh, los
laboratorios deberían esforzarse más.

1264
00:59:06,660 --> 00:59:09,797
Y luego, me parece que insinuaste que es
un... es un problema de naturaleza muy

1265
00:59:09,797 --> 00:59:12,815
fundamental. No será algo sencillo de
resolver. Y, sí, ¿cuál es tu intuición

1266
00:59:12,815 --> 00:59:13,579
con respecto a eso?

1267
00:59:14,100 --> 00:59:19,242
La verdad es que no sé si es, uhm, algo
súper fundamental. No sé realmente si mi

1268
00:59:19,242 --> 00:59:22,366
intención era decir eso. Lo que sí creo
es que...

1269
00:59:22,520 --> 00:59:25,489
No he hecho estos experimentos, pero sí
creo que podrías probablemente

1270
00:59:25,489 --> 00:59:28,759
regularizar la entropía para que sea, eh,
más alta. Así que estás animando al

1271
00:59:28,759 --> 00:59:31,987
modelo a darte más y más soluciones. Pero
no quieres que empiece a desviarse

1272
00:59:31,987 --> 00:59:35,128
demasiado de los datos de entrenamiento.
Va a empezar a inventar su propio

1273
00:59:35,128 --> 00:59:38,528
lenguaje. Va a empezar a usar palabras
extremadamente raras, eh, así que se va a

1274
00:59:38,528 --> 00:59:40,120
desviar demasiado de la distribución.

1275
00:59:26,700 --> 00:59:26,932
Mmm.

1276
00:59:40,180 --> 00:59:43,772
Entonces creo que controlar la
distribución es algo complicado... Es que

1277
00:59:43,772 --> 00:59:46,960
alguien tiene que... Probablemente no es
trivial en ese sentido.

1278
00:59:46,940 --> 00:59:51,742
Mm-hmm. ¿Cuántos bits debería tener el
núcleo óptimo de la inteligencia, si

1279
00:59:51,742 --> 00:59:56,869
tuvieras que hacer una estimación? Creo
que si pusiéramos las, eh, sondas de von

1280
00:59:56,869 --> 00:59:57,388
Neumann-

1281
00:59:57,400 --> 00:59:57,910
Sí, sí, sí

1282
00:59:58,120 --> 00:59:59,559
¿De qué tamaño es que tiene que ser?

1283
00:59:59,880 --> 01:00:03,119
Pues, es, uh, muy interesante en la
historia del campo, porque en un momento

1284
01:00:03,119 --> 01:00:06,661
todo estaba muy, um, enfocado en escalar,
en términos de, oh, vamos a hacer modelos

1285
01:00:06,661 --> 01:00:09,987
mucho más grandes, modelos de billones de
parámetros. Y en realidad lo que los

1286
01:00:09,987 --> 01:00:13,227
modelos han hecho en tamaño es que han
crecido, y ahora que en realidad como

1287
01:00:13,227 --> 01:00:13,486
que...

1288
01:00:14,180 --> 01:00:17,920
En realidad, incluso bajan. Sus modelos
son más pequeños. E incluso entonces,

1289
01:00:17,920 --> 01:00:21,611
creo que memorizaron demasiado. Así que
creo que tuve una predicción hace un

1290
01:00:21,611 --> 01:00:25,302
tiempo de que casi siento que podemos
obtener núcleos cognitivos que son muy

1291
01:00:25,302 --> 01:00:27,368
buenos incluso con un billón de
parámetros.

1292
01:00:15,020 --> 01:00:15,252
Sí.

1293
01:00:27,780 --> 01:00:30,850
Ya debería ser como... Como, si hablas
con un modelo de mil millones de

1294
01:00:30,850 --> 01:00:34,096
parámetros, creo, en 20 años, podrás
tener una conversación muy productiva.

1295
01:00:34,096 --> 01:00:37,650
Piensa, um, y es mucho más parecido a un
humano. Uh, pero si le haces una pregunta

1296
01:00:37,650 --> 01:00:41,027
de hechos, quizás tenga que buscarla.
Pero sabe que no sabe y que quizás tenga

1297
01:00:41,027 --> 01:00:43,616
que buscarla, y simplemente hará todas
las cosas razonables.

1298
01:00:43,600 --> 01:00:47,222
Me sorprende que creas que tomará mil
millones de pa- Porque ya tenemos modelos

1299
01:00:47,222 --> 01:00:50,844
de mil millones de parámetros, o un par
de ellos de mil millones... que son muy

1300
01:00:50,844 --> 01:00:51,726
inteligentes, y es-

1301
01:00:51,720 --> 01:00:54,668
Claro que sí, hay modelos que tienen como
un billón de parámetros, ¿verdad? Pero

1302
01:00:54,668 --> 01:00:55,899
recuerdan tantas cosas, es que...

1303
01:00:55,880 --> 01:01:01,020
Sí. Pero me sorprende que en diez años,
dado el ritmo al que avanzamos... Bueno,

1304
01:01:01,020 --> 01:01:05,704
tenemos GPT, OSS, de 20B, y eso es
muchísimo mejor que el GPT-4 original,

1305
01:01:05,704 --> 01:01:08,372
que era de un billón y pico de
parámetros.

1306
01:01:08,420 --> 01:01:08,745
Ajá

1307
01:01:08,820 --> 01:01:11,771
eh, parámetros. Así que, dada esa
tendencia, me sorprende de verdad que

1308
01:01:11,771 --> 01:01:14,723
pienses que en 10 años el núcleo
cognitivo sigue siendo mil millones de

1309
01:01:14,723 --> 01:01:15,693
parámetros. Yo diría...

1310
01:01:15,820 --> 01:01:19,660
Sí, me sorprende que no digas... [pause]
va a ser como, eh, decenas de millones o

1311
01:01:19,660 --> 01:01:20,092
millones.

1312
01:01:16,620 --> 01:01:17,037
¿Será más?

1313
01:01:20,120 --> 01:01:22,723
No, porque creo que los datos de
entrenamiento son... El problema es que

1314
01:01:22,723 --> 01:01:24,996
los datos de entrenamiento son
internet... lo cual es terrible.

1315
01:01:25,920 --> 01:01:28,859
Así que hay muchísimas ganancias por
obtener porque el internet es terrible.

1316
01:01:28,859 --> 01:01:31,603
Como, si realmente... E incluso el
internet, cuando tú y yo pensamos en

1317
01:01:31,603 --> 01:01:34,464
internet, piensas en, oh, Wall Street
Journal o- eso no es lo que es esto.

1318
01:01:28,240 --> 01:01:28,518
Claro.

1319
01:01:33,000 --> 01:01:33,092
Sí

1320
01:01:34,940 --> 01:01:38,559
Cuando estás viendo un conjunto de datos
pre-entrenado en el Laboratorio Frontier,

1321
01:01:38,559 --> 01:01:41,687
y miras un documento aleatorio de
internet, es basura total. Es que, ni

1322
01:01:41,687 --> 01:01:44,502
siquiera sé cómo funciona esto en
absoluto. Son como símbolos de

1323
01:01:44,502 --> 01:01:45,574
cotizaciones bursátiles.

1324
01:01:45,640 --> 01:01:48,466
Eh- ... es una inmensa cantidad de
porquería y desechos de todos los

1325
01:01:48,466 --> 01:01:51,587
rincones de internet. No es como tu
artículo del Wall Street Journal que es

1326
01:01:51,587 --> 01:01:54,836
extremadamente raro. Uhm, entonces, casi
siento que, porque el internet es tan

1327
01:01:54,836 --> 01:01:58,084
terrible, en realidad tenemos que casi
como construir modelos muy grandes para

1328
01:01:58,084 --> 01:02:01,458
comprimir todo eso. Eh, la mayor parte de
esa compresión es trabajo de memoria en

1329
01:02:01,458 --> 01:02:04,538
lugar de trabajo cognitivo- ... pero lo
que realmente queremos es la parte

1330
01:02:04,538 --> 01:02:06,352
cognitiva, en realidad, eliminar la
memoria.

1331
01:02:03,672 --> 01:02:04,043
Vaya

1332
01:02:06,372 --> 01:02:06,882
Correcto.

1333
01:02:06,952 --> 01:02:10,012
Entonces, lo que digo es que necesitamos
modelos inteligentes que nos ayuden a

1334
01:02:10,012 --> 01:02:12,794
refinar incluso el conjunto de
pre-entrenamiento, para reducirlo solo a

1335
01:02:12,794 --> 01:02:15,775
los componentes cognitivos. Y entonces
creo que podrías arreglártelas con un

1336
01:02:15,775 --> 01:02:18,676
modelo mucho más pequeño, porque es con
un conjunto de datos mucho mejor y

1337
01:02:18,676 --> 01:02:21,538
podrías entrenarlo con él. Pero
probablemente, no se entrena directamente

1338
01:02:21,538 --> 01:02:24,320
en él, probablemente se destila para un
modelo mucho mejor aún, pero...

1339
01:02:23,892 --> 01:02:27,421
De acuerdo. Pero ¿por qué la versión
destilada sigue siendo mil millones? Es,

1340
01:02:27,421 --> 01:02:28,582
creo, lo que me pregunto.

1341
01:02:28,672 --> 01:02:31,650
Simplemente siento que la destilación
funciona muy bien. Así que, um, casi cada

1342
01:02:31,650 --> 01:02:34,208
modelo pequeño, si tienes un modelo
pequeño, es casi seguro que está

1343
01:02:34,208 --> 01:02:35,545
destilado. ¿Por qué entrenarías en-

1344
01:02:35,592 --> 01:02:39,251
No, no, no. Pero ¿por qué la destilación,
en diez años, sigue sin bajar de mil

1345
01:02:39,251 --> 01:02:39,678
millones?

1346
01:02:39,752 --> 01:02:43,188
Oh, ¿tú crees que debería ser menos de
mil millones? O sea, ¡por favor!, ¿no?

1347
01:02:44,512 --> 01:02:48,168
En algún momento, uh, debería requerir al
menos mil millones de perillas, uh, para

1348
01:02:48,168 --> 01:02:50,967
hacer algo interesante. ¿Crees que
debería ser aún más pequeño?

1349
01:02:51,212 --> 01:02:54,785
Sí. Es que, si observas la tendencia de
los últimos años, solo encontrar la fruta

1350
01:02:54,785 --> 01:02:58,448
más fácil de alcanzar, y pasar de modelos
de billones o más... que son literalmente

1351
01:02:58,448 --> 01:03:01,842
dos órdenes de magnitud más pequeños en
cuestión de solo dos años, y tener un

1352
01:03:01,842 --> 01:03:02,914
rendimiento mucho mejor.

1353
01:02:56,872 --> 01:02:57,150
Sí, sí

1354
01:03:03,032 --> 01:03:03,403
Sí, sí

1355
01:03:03,592 --> 01:03:07,527
Me hace pensar que el núcleo de la
inteligencia podría ser incluso mucho,

1356
01:03:07,527 --> 01:03:11,626
mucho más pequeño. Como hay mucho espacio
abajo, para parafrasear a Feynman.

1357
01:03:11,632 --> 01:03:13,868
Quiero decir, ya me siento
contracorriente al hablar de un núcleo

1358
01:03:13,868 --> 01:03:15,300
cognitivo de mil millones de parámetros y

1359
01:03:16,192 --> 01:03:19,563
Me estás superando. Creo que sí, quizás
podríamos reducirlo un poco. Es decir,

1360
01:03:19,563 --> 01:03:22,978
aún creo que debería ser suficiente. Sí,
tal vez pueda ser más pequeño. Pero sí

1361
01:03:22,978 --> 01:03:26,305
creo que, en la práctica, quieres que el
modelo tenga cierto conocimiento. No

1362
01:03:26,305 --> 01:03:27,662
quieres que esté buscando todo.

1363
01:03:26,232 --> 01:03:26,371
Bien.

1364
01:03:27,692 --> 01:03:28,109
Así es.

1365
01:03:28,272 --> 01:03:31,476
Eh, porque entonces no puedes pensar por
ti mismo, estás consultando demasiada

1366
01:03:31,476 --> 01:03:34,680
información constantemente, así que creo
que debe haber un currículo básico...

1367
01:03:34,680 --> 01:03:37,885
para el conocimiento. Eh, pero no debe
incluir conocimiento esotérico, ¿sabes?

1368
01:03:34,932 --> 01:03:35,164
Sí

1369
01:03:37,892 --> 01:03:41,320
Sí. Así que estamos discutiendo lo que
plausiblemente podría ser el núcleo

1370
01:03:41,320 --> 01:03:45,077
cognitivo. Hay una pregunta aparte, que
es: ¿cuál será realmente el tamaño de los

1371
01:03:45,077 --> 01:03:48,740
modelos futuros a lo largo del tiempo? Y
me gustaría escuchar sus predicciones.

1372
01:03:48,740 --> 01:03:52,591
Tuvimos una escala creciente hasta quizás
el 4.5, y ahora estamos viendo una escala

1373
01:03:52,591 --> 01:03:55,973
decreciente o estancada. Hay muchas
razones por las que esto podría estar

1374
01:03:55,973 --> 01:03:58,697
sucediendo, pero ¿tienes alguna
predicción sobre el futuro?

1375
01:03:58,772 --> 01:04:01,757
¿Aumentará la escala...? ¿Los modelos más
grandes serán mayores? ¿Serán más

1376
01:04:01,757 --> 01:04:02,765
pequeños? ¿Serán iguales?

1377
01:04:03,212 --> 01:04:06,006
Eh, sí, no sé si tengo una predicción
súper fuerte. [pause] Sí creo que los

1378
01:04:06,006 --> 01:04:08,951
laboratorios simplemente están siendo
prácticos. Tienen un presupuesto de FLOPs

1379
01:04:08,951 --> 01:04:11,783
[pause] y un presupuesto de costos, y
resulta que el pre-entrenamiento no es

1380
01:04:11,783 --> 01:04:14,690
donde quieres poner la mayoría de tus
FLOPs o tu costo, por eso los modelos se

1381
01:04:14,690 --> 01:04:17,183
han vuelto más pequeños. Como son un poco
más pequeños, la etapa de

1382
01:04:17,183 --> 01:04:20,052
pre-entrenamiento es más pequeña, etc.,
pero lo compensan con aprendizaje por

1383
01:04:20,052 --> 01:04:22,922
refuerzo y [pause] todo ese tipo de
cosas, entrenamiento intermedio y todo lo

1384
01:04:22,922 --> 01:04:23,413
que le sigue.

1385
01:04:23,472 --> 01:04:27,712
Así que, están siendo prácticos en cuanto
a todas las etapas y cómo sacar el mayor

1386
01:04:27,712 --> 01:04:31,534
provecho. Así que, supongo que predecir
la tendencia, creo que es bastante

1387
01:04:31,534 --> 01:04:35,303
difícil. Aún espero que haya mucha fruta
al alcance de la mano. Esa es mi

1388
01:04:35,303 --> 01:04:38,811
expectativa básica, mi expectativa
fundamental. Y así que, tengo una

1389
01:04:38,811 --> 01:04:40,329
distribución muy amplia aquí.

1390
01:04:40,312 --> 01:04:43,984
Mm-hmm. ¿Esperas que los frutos más
fáciles de alcanzar sean similares en su

1391
01:04:43,984 --> 01:04:47,755
naturaleza a los tipos de cosas que han
estado ocurriendo en los últimos dos a

1392
01:04:47,755 --> 01:04:51,623
cinco años? Por ejemplo, si consideramos
Nano chat versus nanoGPT... y luego los

1393
01:04:51,623 --> 01:04:55,443
ajustes arquitectónicos que realizaste,
¿es esa básicamente la clase de avances

1394
01:04:55,443 --> 01:04:57,401
que esperas que sigan sucediendo, o hay—

1395
01:04:57,392 --> 01:04:57,717
Ajá.

1396
01:04:58,112 --> 01:04:59,690
¿No esperas un gran cambio de paradigma?

1397
01:04:59,672 --> 01:05:02,685
En general, sí. Espero que los datos
mejoren mucho, porque... cuando ves los

1398
01:05:02,685 --> 01:05:05,940
datos promedio, son terribles. Tan malos
que... ni siquiera sé cómo funciona nada,

1399
01:05:05,940 --> 01:05:08,913
para ser honesto. Mira el ejemplo
promedio en el conjunto de entrenamiento.

1400
01:05:10,352 --> 01:05:14,362
Ehm, como errores de hecho, errores...
cosas sin sentido. Eh, de alguna manera,

1401
01:05:14,362 --> 01:05:18,579
cuando lo haces a escala, el, el ruido se
disipa y te quedas con parte de la señal.

1402
01:05:18,579 --> 01:05:22,384
Eh, así que los conjuntos de datos
mejorarán muchísimo. Es que todo mejora,

1403
01:05:22,384 --> 01:05:26,035
así que, eh, nuestro hardware, eh,
nuestro... Todos los kernels, eh, uh,

1404
01:05:26,035 --> 01:05:29,840
todos los kernels para ejecutar el
hardware y maximizar lo que obtienes con

1405
01:05:29,840 --> 01:05:30,971
el hardware... ¿sabes?

1406
01:05:12,252 --> 01:05:12,437
Sí

1407
01:05:31,092 --> 01:05:34,983
Así que, NVIDIA ajustando lentamente el
hardware en sí, los núcleos tensoriales y

1408
01:05:34,983 --> 01:05:38,923
demás, todo eso tiene que pasar y seguirá
pasando, eh, todos los kernels mejorarán

1409
01:05:38,923 --> 01:05:42,911
y utilizarán el chip al máximo, todos los
algoritmos probablemente mejorarán con la

1410
01:05:42,911 --> 01:05:46,754
arquitectura de optimización y, eh, todos
los componentes de modelado de cómo se

1411
01:05:46,754 --> 01:05:50,645
hace todo y cuáles son los algoritmos con
los que estamos entrenando. Así que, de

1412
01:05:50,645 --> 01:05:52,640
alguna manera, espero que sea, pues,
todo.

1413
01:05:52,872 --> 01:05:56,169
No hay nada que lo domine... todo, más un
veinte por ciento adicional.

1414
01:05:56,532 --> 01:05:57,553
Claro. Interesante.

1415
01:05:57,572 --> 01:05:58,918
Es más o menos lo que he visto.

1416
01:05:59,312 --> 01:06:01,587
Okay, este de aquí es mi gerente general,
Max.

1417
01:06:01,612 --> 01:06:02,726
Qué bien aquí. Siempre aquí.

1418
01:06:02,732 --> 01:06:05,758
Y has estado aquí desde que te
incorporaste hace unos seis meses. Pero

1419
01:06:05,758 --> 01:06:05,889
no-

1420
01:06:05,872 --> 01:06:06,893
E- hace ocho meses.

1421
01:06:07,012 --> 01:06:11,285
Ah, claro. Eh, el tiempo pasa tan rápido.
Pero cuando te di la bienvenida, yo

1422
01:06:11,285 --> 01:06:15,109
estaba en Francia, y por eso,
prácticamente no tuvimos la oportunidad

1423
01:06:15,109 --> 01:06:16,346
de hablar en absoluto.

1424
01:06:16,332 --> 01:06:18,700
Y básicamente me diste un solo inicio de
sesión.

1425
01:06:18,732 --> 01:06:22,116
Así que te di acceso a mi plataforma
Mercury, que es la plataforma bancaria

1426
01:06:22,116 --> 01:06:24,769
que estaba usando en ese momento para
gestionar el podcast.

1427
01:06:24,752 --> 01:06:28,808
Así que entré en Mercury pensando que
sería el primer paso de muchos, pero me

1428
01:06:28,808 --> 01:06:32,757
di cuenta de que así manejabas todo el
negocio. Incluso, muchos de nuestros

1429
01:06:32,757 --> 01:06:36,066
editores son contratistas
internacionales, y habías descubierto

1430
01:06:36,066 --> 01:06:39,055
cómo configurar pagos recurrentes para
una nómina básica.

1431
01:06:39,092 --> 01:06:43,160
O sea, Mercury hizo tan fluida la
experiencia de mis tareas anteriores que

1432
01:06:43,160 --> 01:06:47,228
ni se me ocurrió hasta que lo señalaste
que esta no es la forma natural de

1433
01:06:47,228 --> 01:06:49,401
gestionar nóminas, facturación o demás.

1434
01:06:49,392 --> 01:06:52,971
Sí, yo estaba sorprendido, pero me dije:
"Ha funcionado hasta ahora". Así que

1435
01:06:52,971 --> 01:06:55,986
quizás confíe en ello. Y ahora no puedo
pensar en hacer nada más.

1436
01:06:52,472 --> 01:06:52,843
Sí, sí.

1437
01:06:56,032 --> 01:07:00,271
Bien, lo oíste. Visita mercury.com y
aplica en línea en minutos. Genial.

1438
01:07:00,271 --> 01:07:01,047
Gracias, Max.

1439
01:07:01,992 --> 01:07:02,827
Gracias por invitarme.

1440
01:07:02,892 --> 01:07:03,727
Tío, lo haces genial.

1441
01:07:03,792 --> 01:07:05,649
Me siento muy nervioso, pero muchas
gracias.

1442
01:07:05,692 --> 01:07:09,487
Mercury es una empresa de tecnología
financiera, no un banco. Los servicios

1443
01:07:09,487 --> 01:07:13,334
bancarios son proporcionados a través de
Choice Financial Group, Column A, y

1444
01:07:13,334 --> 01:07:17,335
Evolve Bank and Trust, Miembros de la
FDIC. Se han propuesto diversas formas de

1445
01:07:17,335 --> 01:07:20,413
evaluar cuánto progreso hemos logrado
hacia una AGI completa.

1446
01:07:20,432 --> 01:07:20,849
Ajá.

1447
01:07:20,972 --> 01:07:24,155
Porque si puedes idear una línea,
entonces puedes ver dónde esa línea se

1448
01:07:24,155 --> 01:07:27,697
cruza con la IAG y dónde ocurriría eso en
el eje X. Y así la gente ha propuesto,

1449
01:07:27,697 --> 01:07:31,060
oh, es como el nivel educativo. Como,
teníamos un estudiante de secundaria y

1450
01:07:31,060 --> 01:07:33,975
luego fueron a la universidad con RL y
van a obtener un doctorado-

1451
01:07:33,952 --> 01:07:34,834
No me gusta ese.

1452
01:07:34,912 --> 01:07:38,784
Eh, o luego propondrán... la longitud del
horizonte, así que quizás puedan hacer

1453
01:07:38,784 --> 01:07:42,656
tareas que tarden un minuto. Eh, pueden
hacerlas de forma autónoma, luego pueden

1454
01:07:42,656 --> 01:07:46,626
hacer de forma autónoma tareas que tarden
una hora... a un humano una hora... a un

1455
01:07:46,626 --> 01:07:50,302
humano una semana, etcétera. ¿Cómo
piensas sobre cuál es el eje Y relevante,

1456
01:07:50,302 --> 01:07:53,782
eh, aquí? ¿Cuál es el... Cómo deberíamos
pensar sobre... cómo la IA está

1457
01:07:53,782 --> 01:07:54,370
progresando?

1458
01:07:44,292 --> 01:07:44,431
Mmm

1459
01:07:54,620 --> 01:07:57,601
Así que, supongo, tengo dos respuestas
para eso. Primero, casi me siento tentado

1460
01:07:57,601 --> 01:08:00,281
a rechazar la pregunta por completo.
Porque, de nuevo, veo esto como una

1461
01:08:00,281 --> 01:08:03,074
extensión de la informática. ¿Hemos
hablado de cómo medir el progreso en la

1462
01:08:03,074 --> 01:08:06,093
informática? O cómo se mide el progreso
en la informática, desde los años 70 o lo

1463
01:08:06,093 --> 01:08:07,112
que sea? ¿Cuál es el eje X?

1464
01:08:07,660 --> 01:08:10,837
Así que, en cierto modo, siento que toda
la cuestión resulta un tanto divertida

1465
01:08:10,837 --> 01:08:14,015
desde esa perspectiva. Eh, pero, supongo,
me gustaría decir que cuando la gente

1466
01:08:14,015 --> 01:08:17,275
habla de la inteligencia artificial y la
IAG original, y de cómo nos referíamos a

1467
01:08:17,275 --> 01:08:20,412
ella cuando, eh, cuando OpenAI comenzó,
la IAG era un sistema al que uno podía

1468
01:08:20,412 --> 01:08:23,590
acudir que es capaz de realizar cualquier
tarea que sea económicamente valiosa,

1469
01:08:23,590 --> 01:08:25,260
cualquier tarea que tenga valor
económico.

1470
01:08:25,220 --> 01:08:25,823
Claro que sí.

1471
01:08:25,840 --> 01:08:29,493
A, eh, rendimiento humano o mejor. Bien,
esa era la definición, y en ese momento

1472
01:08:29,493 --> 01:08:32,915
estaba bastante contento con eso. Y como
que siento que me he apegado a esa

1473
01:08:32,915 --> 01:08:36,337
definición para siempre... y luego la
gente ha inventado todo tipo de otras

1474
01:08:36,337 --> 01:08:36,939
definiciones,

1475
01:08:37,540 --> 01:08:41,113
pero me g- me gusta... siento que me
gusta esa definición. Ahora, número uno,

1476
01:08:41,113 --> 01:08:44,968
la primera concesión que la gente siempre
hace es quitar todo lo físico, eh, porque

1477
01:08:44,968 --> 01:08:48,306
hablamos de trabajo de conocimiento
digital. Siento que es una concesión

1478
01:08:48,306 --> 01:08:51,879
importante comparada a la definición
original, que era cualquier tarea que un

1479
01:08:51,879 --> 01:08:52,772
humano puede hacer.

1480
01:08:52,800 --> 01:08:56,143
Puedo levantar cosas, etcétera. La IA no
puede hacer eso, claro, pues.

1481
01:08:56,359 --> 01:08:59,875
Pero lo aceptaremos. Eh, ¿qué porcentaje
de la economía estamos quitando al decir,

1482
01:08:59,875 --> 01:09:03,217
"Oh, solo trabajo de conocimiento"? Mmm,
en realidad no sé los números. Siento

1483
01:09:03,217 --> 01:09:06,734
que, mmm, es alrededor del diez al veinte
por ciento, si tuviera que adivinar, es,

1484
01:09:06,734 --> 01:09:08,340
mmm, es solo trabajo de conocimiento.

1485
01:09:08,680 --> 01:09:12,571
Eh, como alguien podría trabajar desde
casa... y realizar tareas. Algo así. Mmm,

1486
01:09:12,571 --> 01:09:16,511
sigo pensando que es un mercado realmente
grande. Sabes, ¿cuál es el tamaño de la

1487
01:09:16,511 --> 01:09:20,304
economía y qué representa el 10, 20%?
Estamos hablando todavía de unos cuantos

1488
01:09:20,304 --> 01:09:23,702
billones de dólares de, incluso en
Estados Unidos, de cuota de mercado

1489
01:09:23,702 --> 01:09:24,933
casi... o, como, trabajo.

1490
01:09:10,319 --> 01:09:10,458
Mm-hm

1491
01:09:25,460 --> 01:09:29,169
Eh, uhm, así que sigue siendo un ámbito
muy, muy, muy amplio. Pero supongo que,

1492
01:09:29,169 --> 01:09:32,735
volviendo a la definición, lo que me
preguntaría es, eh, ¿hasta qué punto es

1493
01:09:32,735 --> 01:09:36,255
cierta esa definición? Eh, entonces, uhm,
¿hay trabajos o muchas tareas, si

1494
01:09:36,255 --> 01:09:39,869
pensamos en las tareas, como sabes...
Bueno, no, no trabajos, sino tareas. Es

1495
01:09:39,869 --> 01:09:43,578
un poco difícil. Porque el problema es
que la sociedad se va a reestructurar en

1496
01:09:43,578 --> 01:09:47,240
función de las tareas que componen los
trabajos, en comparación con lo que es-

1497
01:09:47,540 --> 01:09:51,604
según lo que sea automatizable o no. Pero
hoy, ¿qué trabajos son reemplazables por

1498
01:09:51,604 --> 01:09:51,905
la IA?

1499
01:09:48,639 --> 01:09:49,103
Así es.

1500
01:09:52,019 --> 01:09:55,540
Así que, un buen ejemplo reciente fue,
eh, la predicción de Geoff Hinton de que

1501
01:09:55,540 --> 01:09:59,016
los radiólogos ya no serían un trabajo, y
esto resultó estar muy equivocado de

1502
01:09:59,016 --> 01:10:00,099
muchas maneras. ¿Verdad?

1503
01:10:00,139 --> 01:10:03,908
Así que, los radiólogos siguen vigentes y
creciendo, aunque la visión por

1504
01:10:03,908 --> 01:10:07,364
computadora es muy, muy buena
reconociendo todo lo que deben ver en

1505
01:10:07,364 --> 01:10:11,396
imágenes. Y es un trabajo complejo y
caótico, con muchas facetas, tratando con

1506
01:10:11,396 --> 01:10:13,281
pacientes y todo eso en su contexto.

1507
01:10:13,760 --> 01:10:17,686
Pues, supongo que en realidad no estoy
seguro de que, bajo esa definición, la IA

1508
01:10:17,686 --> 01:10:20,122
haya logrado un impacto tan significativo
todavía.

1509
01:10:20,200 --> 01:10:23,474
Pero algunos de los empleos que quizás yo
buscaría tienen alguna característica

1510
01:10:23,474 --> 01:10:26,916
que, creo, los hace muy susceptibles a la
automatización, más pronto que tarde. Por

1511
01:10:26,916 --> 01:10:30,064
ejemplo, los empleados de centros de
llamadas suelen mencionarse, y creo que

1512
01:10:30,064 --> 01:10:33,128
con razón. Porque los empleados de
centros de llamadas tienen una serie de

1513
01:10:33,128 --> 01:10:36,361
propiedades simplificadoras con respecto
a lo que es automatizable hoy en día.

1514
01:10:36,440 --> 01:10:40,229
Eh, sus trabajos son bastante sencillos.
Es una secuencia de tareas y cada tarea

1515
01:10:40,229 --> 01:10:44,115
parece similar. Como cuando atiendes una
llamada con una persona, son diez minutos

1516
01:10:44,115 --> 01:10:46,993
de interacción o lo que sea.
Probablemente un poco más. En mi

1517
01:10:46,993 --> 01:10:48,096
experiencia, mucho más.

1518
01:10:48,980 --> 01:10:52,713
Um, y completas alguna tarea en cierto
esquema y modificas algunas entradas de

1519
01:10:52,713 --> 01:10:56,688
la base de datos... o algo por el estilo.
Así que sigues repitiendo algo una y otra

1520
01:10:56,688 --> 01:11:00,325
vez, y ese es tu trabajo. Básicamente,
quieres considerar el horizonte de la

1521
01:11:00,325 --> 01:11:03,864
tarea, cuánto tiempo lleva realizarla, y
luego también quieres eliminar el

1522
01:11:03,864 --> 01:11:07,403
contexto. Como que no estás tratando con
diferentes partes de servicios de

1523
01:11:07,403 --> 01:11:08,809
empresas o de otros clientes.

1524
01:11:08,880 --> 01:11:12,644
Es solamente la base de datos, tú, y una
persona a la que sirves. Y así es más

1525
01:11:12,644 --> 01:11:15,381
cerrado, es más comprensible, eh, y es
puramente digital.

1526
01:11:15,580 --> 01:11:18,709
Así que yo, yo estaría buscando esas
cosas. Pero incluso ahí, en realidad no

1527
01:11:18,709 --> 01:11:21,546
estoy buscando una automatización
completa todavía. Estoy buscando un

1528
01:11:21,546 --> 01:11:24,801
control deslizante de autonomía, y casi
espero que no vayamos a reemplazar a la

1529
01:11:24,801 --> 01:11:27,847
gente de inmediato. Vamos a estar
introduciendo IAs que manejen el ochenta

1530
01:11:27,847 --> 01:11:31,019
por ciento del volumen. Delegarán el
veinte por ciento del volumen a humanos,

1531
01:11:31,019 --> 01:11:34,232
y los humanos supervisarán equipos de
cinco IAs que hagan el trabajo de centro

1532
01:11:34,232 --> 01:11:35,316
de llamadas más rutinario.

1533
01:11:35,580 --> 01:11:39,709
Así que estaría buscando nuevas
interfaces o nuevas empresas que ofrezcan

1534
01:11:39,709 --> 01:11:43,896
algún tipo de capa que te permita
gestionar algunas de estas inteligencias

1535
01:11:43,896 --> 01:11:46,075
artificiales que aún no son perfectas.

1536
01:11:46,120 --> 01:11:46,259
Sí.

1537
01:11:47,080 --> 01:11:50,911
Y entonces esperaría que en la economía,
muchos trabajos sean mucho más duros que

1538
01:11:50,911 --> 01:11:51,677
un teleoperador.

1539
01:11:51,740 --> 01:11:55,941
Me pregunto con los radiólogos, eh, estoy
especulando totalmente, no tengo ni la

1540
01:11:55,941 --> 01:11:59,877
menor idea de cómo... es el flujo de
trabajo real de un radiólogo. Pero una

1541
01:11:59,877 --> 01:12:03,866
analogía que podría ser aplicable es,
uhm, cuando los Waymos fueron lanzados

1542
01:12:03,866 --> 01:12:07,483
por primera vez... había una persona
sentada en el asiento delantero.

1543
01:12:07,460 --> 01:12:07,924
Ajá.

1544
01:12:08,099 --> 01:12:11,240
Y debías tenerlos allí para asegurar que
si algo salía muy mal, ellos eran el

1545
01:12:11,240 --> 01:12:14,465
supervisor. Y creo que incluso hoy, la
gente sigue vigilando para asegurarse de

1546
01:12:14,465 --> 01:12:17,854
que las cosas van bien. Eh, RoboTaxi, que
acaba de ser desplegado, de hecho todavía

1547
01:12:17,854 --> 01:12:19,012
tiene una persona... dentro.

1548
01:12:19,099 --> 01:12:23,008
Y nosotros, podríamos estar en una
situación similar donde si automatizas el

1549
01:12:23,008 --> 01:12:27,179
99% de un trabajo, ese último 1% que el
humano tiene que hacer es increíblemente-

1550
01:12:27,440 --> 01:12:30,515
valioso porque está creando un cuello de
botella en todo lo demás. Y si fuera el

1551
01:12:30,515 --> 01:12:33,630
caso, como con los radiólogos, donde la
persona sentada en la parte delantera del

1552
01:12:33,630 --> 01:12:36,316
Uber o del Waymo tiene que estar
especialmente entrenada durante años-

1553
01:12:36,316 --> 01:12:39,275
para poder proporcionar el último 1%, sus
salarios deberían subir enormemente

1554
01:12:39,275 --> 01:12:42,116
porque son como la única cosa que está
creando un cuello de botella- en el

1555
01:12:42,116 --> 01:12:43,090
despliegue a gran escala.

1556
01:12:29,400 --> 01:12:29,725
Ajá.

1557
01:12:42,500 --> 01:12:43,475
Sí, claro, lo entiendo.

1558
01:12:43,559 --> 01:12:46,295
Así que los radiólogos, creo que sus
salarios han subido por razones

1559
01:12:46,295 --> 01:12:49,523
similares. Si eres como el último cuello
de botella, y no eres fungible, lo que,

1560
01:12:49,523 --> 01:12:52,382
sabes, un... un conductor de Waymo podría
ser fungible con otras cosas.

1561
01:12:52,620 --> 01:12:53,502
Ajá.

1562
01:12:53,480 --> 01:12:57,045
Pues, verás, tus salarios como que suben
así... y luego llegan al noventa y ocho

1563
01:12:57,045 --> 01:12:58,309
por ciento, y como que, así.

1564
01:12:55,900 --> 01:12:56,132
Mmm

1565
01:12:58,340 --> 01:12:58,897
Ah, ya veo.

1566
01:12:58,880 --> 01:13:02,232
Y luego el último uno por ciento se ha
ido. Y me pregunto si estamos viendo

1567
01:13:02,232 --> 01:13:05,857
cosas similares con la radiología o los
salarios de los teleoperadores o algo por

1568
01:13:05,857 --> 01:13:06,310
el estilo.

1569
01:13:00,200 --> 01:13:00,571
Claro.

1570
01:13:06,320 --> 01:13:09,960
Sí. Creo que esa es una pregunta, eh,
bastante interesante. No creo que estemos

1571
01:13:09,960 --> 01:13:13,742
viendo eso actualmente con la radiología
o, eh, con otras áreas... Y no tengo, eh,

1572
01:13:13,742 --> 01:13:17,102
en mi entendimiento... Pero creo que la
radiología no es un buen ejemplo,

1573
01:13:17,102 --> 01:13:20,743
básicamente. No sé por qué Geoff Hinton
se ensañó con la radiología, eh, porque

1574
01:13:20,743 --> 01:13:24,431
creo que es una profesión extremadamente
desordenada, desordenada... complicada.

1575
01:13:24,440 --> 01:13:24,765
Claro.

1576
01:13:24,800 --> 01:13:27,962
Uh, me interesaría mucho más lo que está
pasando hoy con los empleados de los

1577
01:13:27,962 --> 01:13:30,833
centros de llamadas, por ejemplo. Uh,
porque esperaría que mucho de lo

1578
01:13:30,833 --> 01:13:34,120
rutinario fuera, uh, automatizable hoy. Y
no tengo, uh, acceso de primer nivel a

1579
01:13:34,120 --> 01:13:37,408
eso, pero quizás buscaría tendencias de
lo que está pasando con los empleados de

1580
01:13:37,408 --> 01:13:38,406
los centros de llamadas.

1581
01:13:37,740 --> 01:13:37,879
Sí

1582
01:13:38,440 --> 01:13:41,965
Quizás algunas de las cosas que también
esperaría es que estén, eh, reemplazando

1583
01:13:41,965 --> 01:13:45,624
con IA, pero entonces yo aún esperaría un
año o dos porque potencialmente esperaría

1584
01:13:45,624 --> 01:13:48,703
que se retractaran y de hecho
recontrataran a algunas de las personas.

1585
01:13:49,340 --> 01:13:52,183
Sí. Creo que hay pruebas de que eso ya
está pasando en las... generalmente en

1586
01:13:52,183 --> 01:13:54,727
las empresas que adoptan IA. Lo cual me
parece bastante sorprendente.

1587
01:13:51,460 --> 01:13:51,785
Vale

1588
01:13:54,840 --> 01:13:58,272
Y también me parece que lo que fue
verdaderamente sorprendente... De

1589
01:13:58,272 --> 01:14:02,166
acuerdo... eh, la IAG, ¿no? Como, una
cosa que lo haría absolutamente todo y,

1590
01:14:02,166 --> 01:14:04,267
de acuerdo, nos quitará el trabajo
físico.

1591
01:14:04,320 --> 01:14:07,886
Así que, algo que podría hacer todo el
trabajo intelectual, y lo que

1592
01:14:07,886 --> 01:14:12,144
ingenuamente habrías anticipado, que la
forma en que esta progresión se daría es,

1593
01:14:12,144 --> 01:14:16,348
como, tomarías una pequeña tarea que un
consultor está haciendo, la sacarías del

1594
01:14:16,348 --> 01:14:20,447
cubo. Tomarías la pequeña tarea que un
contador está haciendo, la sacarías del

1595
01:14:20,447 --> 01:14:20,713
cubo.

1596
01:14:21,040 --> 01:14:24,314
Y luego, simplemente estás haciendo esto
en todo el trabajo de conocimiento. Pero

1597
01:14:24,314 --> 01:14:27,589
en cambio, si creemos que estamos en el
camino de la IAG con el paradigma actual,

1598
01:14:27,589 --> 01:14:30,781
la progresión no es para nada así. Al
menos, uhm, no parece que los consultores

1599
01:14:30,781 --> 01:14:33,360
y contadores y lo que sea estén
obteniendo, uhm, una gran mejora

1600
01:14:33,360 --> 01:14:33,810
productiva.

1601
01:14:33,860 --> 01:14:37,671
Es muy parecido a que los programadores
están, como, sintiendo cada vez más

1602
01:14:37,671 --> 01:14:41,791
escalofríos de otra forma en su trabajo.
Si miras los ingresos de estas empresas,

1603
01:14:41,791 --> 01:14:45,706
descontando solo, como, los ingresos
normales de chat, que creo, es, como, no

1604
01:14:45,706 --> 01:14:48,024
sé, eso es similar a, como, Google o algo
así.

1605
01:14:41,700 --> 01:14:41,932
Mmm

1606
01:14:48,059 --> 01:14:48,476
Ajá.

1607
01:14:48,559 --> 01:14:51,902
Solo mirando los ingresos de las API,
está, como, dominado por la programación,

1608
01:14:51,902 --> 01:14:54,860
¿verdad? Así que esta cosa que es
"general", entre comillas, que podrá

1609
01:14:54,860 --> 01:14:58,032
hacer cualquier trabajo de conocimiento,
está abrumadoramente haciendo solo

1610
01:14:58,032 --> 01:14:58,590
codificación.

1611
01:14:55,640 --> 01:14:56,011
Sí, sí, sí

1612
01:14:58,600 --> 01:14:58,971
Sí. Oh,

1613
01:14:58,980 --> 01:15:02,648
Y es una forma sorprendente en la que
esperarías que la IAG se despliegue.

1614
01:15:02,880 --> 01:15:06,720
Así que creo que hay un punto bastante
interesante aquí, porque realmente

1615
01:15:06,720 --> 01:15:10,667
considero que la programación es,
digamos, la primera cosa ideal para estos

1616
01:15:10,667 --> 01:15:14,188
LLM... y agentes. Y eso es porque la
programación siempre ha girado

1617
01:15:14,188 --> 01:15:18,188
fundamentalmente en torno al texto. Son
terminales de computadora y texto, y

1618
01:15:18,188 --> 01:15:20,109
absolutamente todo se basa en texto.

1619
01:15:20,100 --> 01:15:20,239
Sí.

1620
01:15:20,260 --> 01:15:23,965
Y los LLM, la forma en que son entrenados
en internet, aman el texto, y por eso son

1621
01:15:23,965 --> 01:15:27,264
procesadores de texto perfectos... y hay
todos estos datos ahí fuera, y es

1622
01:15:27,264 --> 01:15:28,665
simplemente un ajuste perfecto.

1623
01:15:28,820 --> 01:15:33,053
Y también tenemos mucha infraestructura
preconstruida para manejar, eh, código y

1624
01:15:33,053 --> 01:15:37,179
texto. Por ejemplo, tenemos, eh, Visual
Studio Code, o, ya sabes... eh, tu IDE

1625
01:15:37,179 --> 01:15:41,519
favorito mostrándote código, y un agente
puede conectarse a eso. Así, por ejemplo,

1626
01:15:41,519 --> 01:15:45,752
si un agente tiene un "diff" donde hizo
algún cambio, de repente ya tenemos todo

1627
01:15:45,752 --> 01:15:49,503
este código que muestra todas las
diferencias a una base de código, eh,

1628
01:15:49,503 --> 01:15:50,414
usando un "diff".

1629
01:15:36,559 --> 01:15:36,791
Bien

1630
01:15:50,700 --> 01:15:53,700
Así que, es casi como si ya hubiéramos
pre-construido gran parte de la

1631
01:15:53,700 --> 01:15:57,004
infraestructura para el código. Ahora,
contrasta eso con algunas de las cosas

1632
01:15:57,004 --> 01:16:00,221
que no disfrutan de eso en absoluto. Así,
por ejemplo, hay gente intentando

1633
01:16:00,221 --> 01:16:03,091
construir automatización no para
codificar, sino, por ejemplo, para

1634
01:16:03,091 --> 01:16:03,656
diapositivas.

1635
01:16:03,780 --> 01:16:07,630
Vi una empresa haciendo diapositivas. Eso
es mucho más difícil. Y la razón es que

1636
01:16:07,630 --> 01:16:09,074
las diapositivas no son texto.

1637
01:16:09,140 --> 01:16:09,511
Ajá.

1638
01:16:09,620 --> 01:16:13,231
Las diapositivas son pequeños gráficos, y
están dispuestas espacialmente, y, uh,

1639
01:16:13,231 --> 01:16:16,569
tienen componentes visuales. Y, um, y las
diapositivas, uh, no tienen esta

1640
01:16:16,569 --> 01:16:19,906
infraestructura preestablecida. Por
ejemplo, si un agente fuera a hacer un

1641
01:16:19,906 --> 01:16:22,695
cambio diferente, uh, en tus
diapositivas, ¿cómo te muestra la

1642
01:16:22,695 --> 01:16:26,307
diferencia? ¿Cómo ves la diferencia? No
hay, no hay nada que muestre diferencias

1643
01:16:26,307 --> 01:16:27,313
para las diapositivas.

1644
01:16:27,440 --> 01:16:27,718
Ajá.

1645
01:16:27,800 --> 01:16:31,314
Así que alguien tiene que construirlo.
[pausa] Y es que, algunas de estas cosas

1646
01:16:31,314 --> 01:16:35,008
no son adecuadas para las IA tal como las
conocemos, que son procesadores de texto.

1647
01:16:35,008 --> 01:16:36,855
Y el código, sorprendentemente, sí lo es.

1648
01:16:37,200 --> 01:16:40,543
Yo- yo en realidad no estoy seguro si eso
solo lo explica, porque

1649
01:16:42,280 --> 01:16:45,866
Yo, en lo personal, he intentado que los
modelos de lenguaje grandes sean útiles

1650
01:16:45,866 --> 01:16:49,316
en ámbitos que son puramente de entrada
de lenguaje y salida de lenguaje, um,

1651
01:16:49,316 --> 01:16:52,948
como la reescritura de transcripciones,
como... la creación de fragmentos basados

1652
01:16:52,948 --> 01:16:56,625
en transcripciones, y un largo etcétera.
Y podrías decir, bueno, yo no in- uh, uh,

1653
01:16:56,625 --> 01:16:59,939
es muy plausible que, como, no hice
absolutamente todo lo posible que pude

1654
01:16:59,939 --> 01:17:00,484
haber hecho.

1655
01:17:00,519 --> 01:17:04,123
Puse un montón de, ya sabes, buenos
ejemplos en contexto, pero quizás debí

1656
01:17:04,123 --> 01:17:08,122
haber hecho algún tipo de ajuste fino, lo
que sea. Así que nuestro amigo en común,

1657
01:17:08,122 --> 01:17:11,825
Andy Matushak, me dijo que él de hecho
probó cincuenta mil millones de cosas

1658
01:17:11,825 --> 01:17:15,133
para que los modelos fueran buenos
escribiendo prompts de repetición

1659
01:17:15,133 --> 01:17:18,491
espaciada. De nuevo, mucho lenguaje de
entrada, lenguaje de salida...

1660
01:17:18,519 --> 01:17:18,844
Ajá

1661
01:17:19,100 --> 01:17:21,839
tarea. Algo que debería ser central en el
repertorio de estos LLM.

1662
01:17:21,900 --> 01:17:22,457
Ah, ya veo.

1663
01:17:22,460 --> 01:17:26,114
Y probó el aprendizaje en contexto,
obviamente, con, eh, unos pocos ejemplos

1664
01:17:26,114 --> 01:17:29,817
cortos. Probó, creo, me dijo, como, un
montón de cosas, como, eh, ajuste fino

1665
01:17:29,817 --> 01:17:33,422
supervisado, y, como, re- ya sabes,
recuperación, lo que sea. Y simplemente

1666
01:17:33,422 --> 01:17:37,369
no pudo lograr que hicieran la tarjeta a
su entera satisfacción. Así que me parece

1667
01:17:37,369 --> 01:17:40,292
sorprendente... que incluso en dominios
fuera del lenguaje...

1668
01:17:33,260 --> 01:17:33,538
Claro.

1669
01:17:36,720 --> 01:17:37,045
Ajá.

1670
01:17:40,300 --> 01:17:40,439
Mm-hm

1671
01:17:40,780 --> 01:17:44,990
De hecho, es muy difícil sacarles mucho
valor económico a estos modelos, separado

1672
01:17:44,990 --> 01:17:47,095
del código. Y no sé qué, qué lo explica.

1673
01:17:44,000 --> 01:17:44,185
Ah

1674
01:17:47,100 --> 01:17:51,200
Sí, creo que, uhm, sí, creo que tiene
sentido. Es decir, yo diría, uhm, sí, no

1675
01:17:51,200 --> 01:17:55,193
estoy diciendo que cualquier texto sea
trivial, ¿verdad? Uhm, sí creo que el

1676
01:17:55,193 --> 01:17:57,270
código es, como, bastante estructurado.

1677
01:17:54,019 --> 01:17:54,390
Ajá.

1678
01:17:57,340 --> 01:18:00,827
Mmm, el texto es quizás mucho más
florido, y la estruc- mmm, y hay mucha

1679
01:18:00,827 --> 01:18:04,315
más, como, eh, como una especie de
entropía en el texto, diría yo. No sé

1680
01:18:04,315 --> 01:18:05,838
cómo explicarlo de otra manera.

1681
01:18:05,920 --> 01:18:09,886
Y también, quiero decir, el código es
difícil, y la gente se siente bastante

1682
01:18:09,886 --> 01:18:13,799
empoderada por los LLM, incluso a partir
de conocimientos muy, muy básicos.

1683
01:18:13,799 --> 01:18:18,030
Básicamente, no sé si tengo una muy buena
respuesta. Quiero decir, obviamente, el

1684
01:18:18,030 --> 01:18:21,996
texto lo hace mucho, mucho más fácil
quizás, es quizás por qué lo digo, pero

1685
01:18:21,996 --> 01:18:24,217
no significa que todo el texto sea
trivial.

1686
01:18:18,820 --> 01:18:19,005
Mmm.

1687
01:18:24,280 --> 01:18:28,849
Mm-hmm. ¿Qué piensas de la
superinteligencia? ¿Esperas que se sienta

1688
01:18:28,849 --> 01:18:34,032
cualitativamente diferente de los humanos
normales o de las empresas humanas?

1689
01:18:35,160 --> 01:18:38,286
Supongo que lo veo como una progresión de
la automatización en la sociedad,

1690
01:18:38,286 --> 01:18:41,540
¿verdad? Y de nuevo, extrapolando la
tendencia de la informática. Simplemente,

1691
01:18:41,540 --> 01:18:44,413
siento que se producirá una
automatización gradual de muchas cosas, y

1692
01:18:44,413 --> 01:18:47,836
la superinteligencia será una especie de
extrapolación de eso. Así que sí creo que

1693
01:18:47,836 --> 01:18:51,005
con el tiempo esperaremos más y más
entidades autónomas que hagan gran parte

1694
01:18:51,005 --> 01:18:53,962
del trabajo digital, y luego,
eventualmente, incluso el trabajo físico,

1695
01:18:53,962 --> 01:18:55,314
probablemente un tiempo después.

1696
01:18:38,460 --> 01:18:38,831
Ajá.

1697
01:18:55,800 --> 01:18:59,036
Pero la, básicamente lo considero
simplemente, eh, automatización. Eh,

1698
01:18:59,036 --> 01:19:00,490
hablando en términos generales.

1699
01:18:58,840 --> 01:18:59,025
Mmm.

1700
01:19:00,460 --> 01:19:02,970
Creo que la animación incluye lo que los
humanos ya pueden hacer, y la

1701
01:19:02,970 --> 01:19:04,825
superinteligencia... aporta cosas que los
humanos...

1702
01:19:04,860 --> 01:19:07,673
Bueno, pero parte de lo que la gente hace
es inventar cosas nuevas, lo que yo

1703
01:19:07,673 --> 01:19:09,968
simplemente pondría en la automatización,
si eso tiene sentido.

1704
01:19:10,059 --> 01:19:13,383
Sí. Pero tú, [pausa] a ver, [pausa] yo
diría que quizás, [pausa] en lugar de

1705
01:19:13,383 --> 01:19:16,353
algo tan abstracto, [pausa] más bien,
[pausa] de una forma mucho más

1706
01:19:16,353 --> 01:19:16,885
cualitativa.

1707
01:19:16,920 --> 01:19:17,430
Ajá.

1708
01:19:17,680 --> 01:19:20,988
¿Esperas que algo se perciba como, 'de
acuerdo, esto es así', porque esta

1709
01:19:20,988 --> 01:19:24,573
entidad puede pensar con una velocidad
asombrosa o generar una cantidad inmensa

1710
01:19:24,573 --> 01:19:27,882
de copias, o esas copias pueden volver a
fusionarse entre sí, o es, entre

1711
01:19:27,882 --> 01:19:31,328
comillas, "mucho más inteligente"?
Cualquier cantidad de ventajas que... una

1712
01:19:31,328 --> 01:19:34,867
inteligencia artificial podría poseer.
Cualitativamente... La, la civilización

1713
01:19:34,867 --> 01:19:38,038
en la que estas IAs existan...
simplemente se sentirá cualitativamente

1714
01:19:38,038 --> 01:19:39,692
diferente de la civilización humana.

1715
01:19:31,800 --> 01:19:32,032
Mmm.

1716
01:19:39,660 --> 01:19:42,944
No, yo creo que sí. Quiero decir, es
fundamentalmente una automatización, pero

1717
01:19:42,944 --> 01:19:45,888
quiero decir, será, como, algo
extremadamente ajeno. Yo sí creo que se

1718
01:19:45,888 --> 01:19:49,258
verá realmente muy extraño. Porque, como
mencionaste, podemos ejecutar todo esto

1719
01:19:49,258 --> 01:19:52,245
en un clúster de computadoras, etcétera,
y mucho más rápido y todo eso.

1720
01:19:52,280 --> 01:19:52,605
Claro.

1721
01:19:52,620 --> 01:19:56,013
Quiero decir, quizás algunos de los
escenarios, por ejemplo, que me empiezan

1722
01:19:56,013 --> 01:19:59,587
a poner nervioso... con respecto a cuando
el mundo se ve así, es esta especie de

1723
01:19:59,587 --> 01:20:02,663
pérdida gradual de control y
comprensión... de lo que está pasando. Y

1724
01:20:02,663 --> 01:20:06,192
creo que ese es en realidad el resultado
más probable. Es que habrá una pérdida

1725
01:20:06,192 --> 01:20:09,901
gradual de comprensión de, y gradualmente
superpondremos todo esto en todas partes,

1726
01:20:09,901 --> 01:20:11,892
y habrá cada vez menos gente que lo
entienda.

1727
01:20:03,920 --> 01:20:04,152
Mmm.

1728
01:20:11,980 --> 01:20:16,461
Y luego habrá un escenario de pérdida
gradual de control y de lo que ocurre.

1729
01:20:16,461 --> 01:20:20,524
Eso me parece el resultado más
probable... de cómo todo esto acabará.

1730
01:20:18,780 --> 01:20:19,058
Mmm

1731
01:20:20,660 --> 01:20:24,424
Permítame ahondar un poco en eso. No me
queda claro que la pérdida de control y

1732
01:20:24,424 --> 01:20:26,790
la pérdida de entendimiento... sean la
misma cosa.

1733
01:20:25,660 --> 01:20:25,799
Mmm

1734
01:20:26,800 --> 01:20:27,310
Mmm.

1735
01:20:27,552 --> 01:20:32,085
una junta directiva en, digamos, lo que
sea, TSMC, Intel... nombra una empresa

1736
01:20:32,085 --> 01:20:36,265
cualquiera. Eh, son solo, digamos,
octogenarios prestigiosos. Tienen muy

1737
01:20:36,265 --> 01:20:40,681
poca comprensión, y quizás en la práctica
no tienen control real. Pero, o en

1738
01:20:40,681 --> 01:20:44,920
realidad, quizás un mejor ejemplo es el
presidente de los Estados Unidos.

1739
01:20:44,932 --> 01:20:45,349
Ajá.

1740
01:20:46,032 --> 01:20:49,195
El presidente tiene un poder de mierda,
un poder inmenso. Eh, no estoy intentando

1741
01:20:49,195 --> 01:20:52,398
hacer una, una declaración positiva sobre
el actual... eh, gobernante, pero quizás

1742
01:20:52,398 --> 01:20:55,483
sí lo estoy haciendo. Pero, el nivel real
de entendimiento es muy diferente del

1743
01:20:55,483 --> 01:20:56,155
nivel de control.

1744
01:20:47,532 --> 01:20:47,671
Mmm.

1745
01:20:50,692 --> 01:20:50,924
Sí

1746
01:20:52,972 --> 01:20:53,111
Mmm.

1747
01:20:56,192 --> 01:20:59,731
Sí, me parece que eso es bastante justo.
Es una réplica bastante sólida y bien

1748
01:20:59,731 --> 01:21:03,408
fundamentada. Pienso que, uhm, me imagino
que lo que espero es que se pierda, uh,

1749
01:21:03,408 --> 01:21:05,154
una parte de las dos. Sí, exactamente.

1750
01:21:04,832 --> 01:21:07,964
Mmm. ¿Por qué? Quiero decir, la pérdida
de comprensión es obvia, pero... ¿por qué

1751
01:21:07,964 --> 01:21:08,825
la pérdida de control?

1752
01:21:09,072 --> 01:21:12,282
Así que, eh, estamos ya muy adentrados en
un territorio donde, um, no sé

1753
01:21:12,282 --> 01:21:15,764
exactamente cómo se vería esto, pero si
yo fuera a escribir novelas de ciencia

1754
01:21:15,764 --> 01:21:19,200
ficción- ... se parecerían a algo que no
sería ni siquiera una sola, digamos,

1755
01:21:19,200 --> 01:21:22,139
entidad o algo por el estilo, que
simplemente, por así decirlo, lo

1756
01:21:22,139 --> 01:21:22,818
acaparara todo.

1757
01:21:22,892 --> 01:21:26,304
Pero en realidad, como múltiples
entidades compitiendo que gradualmente se

1758
01:21:26,304 --> 01:21:29,670
vuelven más y más autónomas y, uh,
algunas de ellas se descontrolan y las

1759
01:21:29,670 --> 01:21:33,410
otras, como, las combaten y... todo este
tipo de cosas. Y es como esta, esta olla

1760
01:21:33,410 --> 01:21:36,870
a presión de actividad completamente
autónoma a la que hemos, uh, delegado.

1761
01:21:37,812 --> 01:21:38,508
Yo, como que siento

1762
01:21:40,192 --> 01:21:41,027
Tendría ese sabor.

1763
01:21:41,412 --> 01:21:45,658
Mmm. No es el hecho de que ellos sean más
inteligentes que nosotros lo que está

1764
01:21:45,658 --> 01:21:50,013
provocando la pérdida de control. Es el
hecho de que están compitiendo entre sí y

1765
01:21:50,013 --> 01:21:53,933
lo que sea, eh, que surja de esa
competencia lo que nos lleva a perder el

1766
01:21:53,933 --> 01:21:54,368
control.

1767
01:21:45,652 --> 01:21:45,884
No, no siempre.

1768
01:21:54,452 --> 01:21:58,350
Eh, quiero decir, básicamente espero que
haya... quiero decir, eh, muchas de estas

1769
01:21:58,350 --> 01:22:02,200
cosas, quiero decir, serán herramientas
para la gente, y la gente podría... Parte

1770
01:22:02,200 --> 01:22:06,002
de la población, digamos, actúa en nombre
de la gente o algo así. Así que quizás

1771
01:22:06,002 --> 01:22:09,756
esa gente tenga el control, pero quizás
sea una pérdida de control general para

1772
01:22:09,756 --> 01:22:12,981
la sociedad en el sentido de los
resultados que queremos o algo así.

1773
01:22:13,052 --> 01:22:16,692
Donde hay entidades actuando en
representación de individuos, que todavía

1774
01:22:16,692 --> 01:22:19,321
se perciben, a grandes rasgos, como fuera
de control.

1775
01:22:19,352 --> 01:22:23,382
Sí, sí. Esta es una pregunta que debí
haber hecho antes. Estábamos hablando de

1776
01:22:23,382 --> 01:22:27,412
cómo, actualmente, parece que al hacer
ingeniería o investigación de IA, estos

1777
01:22:27,412 --> 01:22:31,599
modelos están más en la categoría de un
compilador que, uh, en la categoría de un

1778
01:22:31,599 --> 01:22:32,122
reemplazo.

1779
01:22:32,172 --> 01:22:32,543
Ajá.

1780
01:22:32,952 --> 01:22:35,831
En algún momento, si tienes la llamada
"AGI", debería poder hacer lo que haces.

1781
01:22:35,872 --> 01:22:36,289
Ajá.

1782
01:22:36,592 --> 01:22:40,417
¿Y crees que tener un millón de copias
tuyas en paralelo resultaría en un gran

1783
01:22:40,417 --> 01:22:43,646
impulso al progreso de la IA?
Básicamente, si eso ocurre, ¿verías,

1784
01:22:43,646 --> 01:22:46,825
esperas ver una explosión de
inteligencia? ¿O incluso una vez que

1785
01:22:46,825 --> 01:22:50,849
tengamos... una verdadera IAG? Y no hablo
de los LLM de hoy, sino de una IAG real.

1786
01:22:50,832 --> 01:22:54,693
Supongo, o sea, lo que quiero decir es,
sí, pero es lo de siempre. Porque ya

1787
01:22:54,693 --> 01:22:58,761
estamos en una explosión de inteligencia
y lo hemos estado por décadas. Y cuando

1788
01:22:58,761 --> 01:23:02,519
miras el PIB, es básicamente la curva del
PIB... que es una suma ponderada

1789
01:23:02,519 --> 01:23:05,042
exponencial sobre tantos aspectos de la
industria.

1790
01:23:01,292 --> 01:23:01,431
Mm-hm

1791
01:23:05,212 --> 01:23:08,571
Todo se está automatizando gradualmente,
lo ha estado por cientos de años. Eh, la

1792
01:23:08,571 --> 01:23:11,007
revolución industrial es
automatización... y algunos de los

1793
01:23:11,007 --> 01:23:14,325
componentes físicos y la construcción de
herramientas y todo este tipo de cosas,

1794
01:23:14,325 --> 01:23:17,475
compiladores, nuestra automatización
temprana de software, etcétera. Eh, así

1795
01:23:17,475 --> 01:23:20,751
que siento que hemos estado mejorando
recursivamente y, eh, explotando por, eh,

1796
01:23:20,751 --> 01:23:21,465
por mucho tiempo.

1797
01:23:21,492 --> 01:23:25,052
Quizás otra forma de verlo es, eh, quiero
decir, la Tierra era bastante... quiero

1798
01:23:25,052 --> 01:23:28,302
decir, si no consideramos la biomecánica
y todo eso, era un lugar bastante

1799
01:23:28,302 --> 01:23:31,774
aburrido, creo... y se veía muy similar
si solo la miras desde el espacio. Y la

1800
01:23:31,774 --> 01:23:34,978
Tierra está girando y luego, como,
estamos en medio de este, como, evento

1801
01:23:34,978 --> 01:23:35,423
explosivo.

1802
01:23:35,492 --> 01:23:35,817
Claro.

1803
01:23:35,952 --> 01:23:39,960
Pero lo estamos viendo en cámara lenta.
Pero definitivamente siento que esto,

1804
01:23:39,960 --> 01:23:44,285
esto ya viene ocurriendo desde hace mucho
tiempo. Y yo, de nuevo, no veo la IA como

1805
01:23:44,285 --> 01:23:48,400
una tecnología distinta con respecto a lo
que ya ha estado sucediendo por mucho

1806
01:23:48,400 --> 01:23:48,769
tiempo.

1807
01:23:48,832 --> 01:23:51,850
O sea... ¿crees que va a continuar hacia
esta tendencia hiperexponencial?

1808
01:23:52,172 --> 01:23:55,548
Sí, y por eso- ... esto fue, esto me
pareció muy interesante porque estuve,

1809
01:23:55,548 --> 01:23:58,969
estuve tratando de encontrar la IA en el
PIB por un tiempo. Creía que el PIB

1810
01:23:58,969 --> 01:24:02,573
debería aumentar. Pero luego miré otras
tecnologías que pensé que eran, que eran

1811
01:24:02,573 --> 01:24:05,813
muy transformadoras, como, eh, quizás
computadoras o teléfonos móviles o

1812
01:24:05,813 --> 01:24:07,729
etcétera. No se pueden encontrar en el
PIB.

1813
01:24:07,912 --> 01:24:11,048
El PIB sigue la misma curva de
crecimiento exponencial. Y es que, por

1814
01:24:11,048 --> 01:24:14,784
ejemplo, el primer iPhone no tenía la App
Store ni muchas de las características y

1815
01:24:14,784 --> 01:24:18,520
funciones que tiene el iPhone moderno. Y
aunque pensamos que 2008, cuando salió el

1816
01:24:18,520 --> 01:24:21,426
iPhone, fue un cambio sísmico importante,
en realidad no lo fue.

1817
01:24:21,452 --> 01:24:24,455
Todo está, como, tan disperso y tan
lentamente se difunde que todo termina

1818
01:24:24,455 --> 01:24:27,376
promediándose en la misma exponencial. Y
es exactamente lo mismo con las

1819
01:24:27,376 --> 01:24:30,256
computadoras, no puedes encontrarlas en
el PIB como, "Oh, ahora tenemos

1820
01:24:30,256 --> 01:24:30,832
computadoras".

1821
01:24:31,492 --> 01:24:34,432
Eso no fue lo que pasó porque es una
progresión tan lenta. Y con la IA veremos

1822
01:24:34,432 --> 01:24:37,068
exactamente lo mismo. Es solo más
automatización. Nos permite escribir

1823
01:24:37,068 --> 01:24:40,008
diferentes tipos de programas que no
podíamos escribir antes, pero la IA sigue

1824
01:24:40,008 --> 01:24:41,383
siendo fundamentalmente un programa.

1825
01:24:41,892 --> 01:24:46,043
Y, eh, es un nuevo tipo de computadora y
un nuevo tipo de, eh, sistema de

1826
01:24:46,043 --> 01:24:50,426
computación, pero tiene todos estos
problemas, se va a extender con el tiempo

1827
01:24:50,426 --> 01:24:54,809
y seguirá sumando el mismo exponencial. Y
aún tendremos un exponencial que se

1828
01:24:54,809 --> 01:24:59,306
volverá extremadamente vertical. Y será
muy ajeno vivir en ese tipo de entorno.

1829
01:24:59,392 --> 01:25:02,407
¿Es- estás diciendo que, o sea, lo que
pasará es...? Entonces, si ves, si

1830
01:25:02,407 --> 01:25:05,339
observas la tendencia antes de la
revolución industrial... hasta ahora,

1831
01:25:05,339 --> 01:25:08,187
tienes un crecimiento hiperexponencial
donde pasas de, digamos, 0% de

1832
01:25:08,187 --> 01:25:11,244
crecimiento a luego... hace 10.000 años,
un 0,02% de crecimiento. Y luego,

1833
01:25:11,244 --> 01:25:13,841
actualmente, estamos en un 2% de
crecimiento. Así que eso es un

1834
01:25:13,841 --> 01:25:17,108
hiperexponencial y estás diciendo que, si
graficas la IA ahí, entonces la IA te

1835
01:25:17,108 --> 01:25:19,454
lleva a un 20% de crecimiento o a un 200%
de crecimiento.

1836
01:25:09,812 --> 01:25:09,997
Mmm

1837
01:25:19,472 --> 01:25:20,168
Ajá.

1838
01:25:20,152 --> 01:25:23,807
O podrías estar diciendo que si miras los
últimos 300 años, lo que has visto es

1839
01:25:23,807 --> 01:25:27,650
tecnología tras tecnología, computadoras,
electrificación... vapor, eh, máquinas de

1840
01:25:27,650 --> 01:25:31,400
vapor, ferrocarriles... etcétera, pero la
tasa de crecimiento es la misma. Es del

1841
01:25:31,400 --> 01:25:34,306
2%. ¿Entonces estás diciendo que la tasa
de crecimiento saltará

1842
01:25:34,306 --> 01:25:35,384
hiper-exponencialmente?

1843
01:25:35,352 --> 01:25:38,487
No, básicamente... espero esto... La tasa
de crecimiento también se ha mantenido

1844
01:25:38,487 --> 01:25:39,717
más o menos constante, ¿verdad?

1845
01:25:39,812 --> 01:25:42,576
Solo en los últimos doscientos,
trescientos años. Pero a lo largo de la

1846
01:25:42,576 --> 01:25:45,617
historia humana, como que ha explotado,
¿verdad? Es como si hubiera pasado de,

1847
01:25:45,617 --> 01:25:48,579
como, un 0% básicamente a, como- ... más
rápido, más rápido, más rápido, una

1848
01:25:48,579 --> 01:25:49,843
explosión industrial- ... un 2%.

1849
01:25:49,092 --> 01:25:49,277
Sí

1850
01:25:49,872 --> 01:25:53,130
Básicamente, lo que quiero decir es que
durante un tiempo intenté encontrar o

1851
01:25:53,130 --> 01:25:56,474
buscar la IA en, digamos, la curva del
PIB. Y me convencí de que esto es falso.

1852
01:25:56,474 --> 01:25:59,733
Y que incluso cuando la gente habla de
auto-mejora recursiva y laboratorios y

1853
01:25:59,733 --> 01:26:02,734
cosas por el estilo, ni siquiera... esto
es lo habitual. Claro que va a

1854
01:26:02,734 --> 01:26:05,950
auto-mejorarse recursivamente, y se ha
estado auto-mejorando recursivamente.

1855
01:26:05,950 --> 01:26:09,337
Como los LLM permiten a los ingenieros
trabajar mucho más eficientemente... para

1856
01:26:09,337 --> 01:26:11,095
construir la siguiente, uh, ronda de LLM.

1857
01:26:11,212 --> 01:26:14,675
Y muchísimos más de los componentes están
siendo automatizados y, y ajustados, y

1858
01:26:14,675 --> 01:26:18,183
etcétera. Así que el acceso de todos los
ingenieros a Google Search es, en cierto

1859
01:26:18,183 --> 01:26:19,060
modo, parte de ello.

1860
01:26:19,152 --> 01:26:22,028
Todos los ingenieros que disponen de un
IDE, todos, todos ellos con

1861
01:26:22,028 --> 01:26:25,559
autocompletado o con código de plantilla,
etcétera, todo esto es simplemente parte

1862
01:26:25,559 --> 01:26:28,785
de la misma aceleración de todo el
proceso. Así que, uh, es simplemente tan

1863
01:26:28,785 --> 01:26:29,090
fluido.

1864
01:26:30,624 --> 01:26:34,106
Pero solo- solo para aclarar, ¿dices que
la tasa de crecimiento no cambiará? Como-

1865
01:26:34,164 --> 01:26:34,628
Pues, este...

1866
01:26:35,284 --> 01:26:37,048
La explosión de inteligencia se verá
como-

1867
01:26:37,084 --> 01:26:37,409
Ajá

1868
01:26:37,644 --> 01:26:41,480
Nos permitió seguir en la trayectoria de
crecimiento del 2%, como internet nos

1869
01:26:41,480 --> 01:26:42,427
ayudó a mantenerla.

1870
01:26:42,444 --> 01:26:45,091
Sí, mi expectativa es que se mantenga el
mismo patrón.

1871
01:26:45,544 --> 01:26:50,155
Sí. Quiero decir, eh, uhm, solo para
presentarte el argumento opuesto, mi

1872
01:26:50,155 --> 01:26:54,767
expectativa es que esto, como, uhm,
explote, porque creo que la verdadera

1873
01:26:54,767 --> 01:26:59,443
IAG, y no me refiero a los bots de
codificación de LLM, hablo de, como, un

1874
01:26:59,443 --> 01:27:02,262
reemplazo real de un humano en un
servidor...

1875
01:27:02,284 --> 01:27:02,376
Mm

1876
01:27:02,844 --> 01:27:06,614
es cualitativamente diferente de estas
otras tecnologías que contribuyen a

1877
01:27:06,614 --> 01:27:07,905
mejorar la productividad.

1878
01:27:07,944 --> 01:27:08,129
Mm-hm, mm-hm

1879
01:27:08,684 --> 01:27:11,368
porque es la mano de obra en sí, ¿verdad?
Creo que vivimos en un mundo con una

1880
01:27:11,368 --> 01:27:13,949
escasez de mano de obra muy grande. Si
hablas con cualquier fundador de una

1881
01:27:13,949 --> 01:27:16,703
startup... con cualquier persona, puedes
simplemente preguntar, "¿Qué- qué es lo

1882
01:27:16,703 --> 01:27:19,040
que más necesitas?" Simplemente necesitas
gente realmente talentosa.

1883
01:27:19,144 --> 01:27:22,470
Y si simplemente tienes miles de millones
de personas adicionales que están

1884
01:27:22,470 --> 01:27:25,706
inventando cosas, integrándose a la
sociedad, creando empresas desde cero

1885
01:27:25,706 --> 01:27:28,717
hasta el final, eso se siente
cualitativamente diferente de una sola

1886
01:27:28,717 --> 01:27:32,223
tecnología. Es como si te preguntaran si
tuvieras diez mil millones de personas

1887
01:27:32,223 --> 01:27:33,122
extra en el planeta.

1888
01:27:33,124 --> 01:27:36,299
Quiero decir, quizás un contrapunto.
Quiero decir, en primer lugar, estoy

1889
01:27:36,299 --> 01:27:39,872
bastante abierto a que me convenzan de un
modo u otro en este punto. Pero sí diré,

1890
01:27:39,872 --> 01:27:42,783
por ejemplo, la computación es trabajo.
La computación era trabajo.

1891
01:27:42,824 --> 01:27:45,983
Las computadoras, muchos trabajos
desaparecieron- porque las computadoras

1892
01:27:45,983 --> 01:27:49,098
están automatizando mucho procesamiento
de información para lo que ya no

1893
01:27:49,098 --> 01:27:49,975
necesitas un humano.

1894
01:27:50,024 --> 01:27:54,361
Y así las computadoras son trabajo, y eso
se ha visto. Y, sabes, la conducción

1895
01:27:54,361 --> 01:27:58,473
autónoma, por ejemplo, también son
computadoras haciendo trabajo. Así que,

1896
01:27:58,473 --> 01:28:02,191
supongo que eso ya se ha visto, así que
sigue siendo lo de siempre.

1897
01:28:02,204 --> 01:28:05,779
Sí. Supongo que tienes una máquina que
está escupiendo más cosas así- ... a un

1898
01:28:05,779 --> 01:28:09,448
ritmo potencialmente más rápido. Y así,
históricamente aquí, tenemos ejemplos de

1899
01:28:09,448 --> 01:28:12,978
cómo cambia el régimen de crecimiento,
donde pasaste, sabes, de un cero punto

1900
01:28:12,978 --> 01:28:16,461
dos por ciento de crecimiento a un dos
por ciento de crecimiento. Así que me

1901
01:28:16,461 --> 01:28:20,083
parece muy plausible que, por ejemplo,
una máquina que luego esté escupiendo el

1902
01:28:20,083 --> 01:28:23,194
próximo- ... coche autónomo y el próximo
internet- ... y lo que sea.

1903
01:28:05,684 --> 01:28:05,869
Mmm

1904
01:28:14,364 --> 01:28:14,781
Ajá.

1905
01:28:21,144 --> 01:28:21,283
Sí

1906
01:28:23,204 --> 01:28:27,416
Quiero decir, un poco... Sí, entiendo de
dónde viene. Al mismo tiempo, siento que

1907
01:28:27,416 --> 01:28:31,734
la gente asume que, bueno, tenemos a Dios
en una caja y ahora puede hacerlo todo. Y

1908
01:28:31,734 --> 01:28:35,893
es que, simplemente, no se verá así. Va a
ser... Va a poder hacer algunas cosas,

1909
01:28:35,893 --> 01:28:37,368
pero fallará en otras cosas.

1910
01:28:33,804 --> 01:28:34,175
Claro.

1911
01:28:37,404 --> 01:28:41,295
Se irá introduciendo gradualmente en la
sociedad y básicamente terminaremos con

1912
01:28:41,295 --> 01:28:44,888
el mismo patrón, es mi predicción. Uh,
porque esta suposición de tener de

1913
01:28:44,888 --> 01:28:48,280
repente un humano completamente
inteligente, uh, totalmente flexible,

1914
01:28:48,280 --> 01:28:51,823
totalmente general, uh, en una caja, y
que podamos aplicarlo a problemas

1915
01:28:51,823 --> 01:28:55,515
arbitrarios en la sociedad, no creo que
tengamos este, uh, cambio discreto.

1916
01:28:41,704 --> 01:28:41,982
Claro.

1917
01:28:55,904 --> 01:28:59,849
Y, uh, creo que llegaremos a la misma,
uh, difusión gradual de esto en toda la

1918
01:28:59,849 --> 01:29:00,362
industria.

1919
01:29:03,004 --> 01:29:06,872
Mmm. Yo creo que lo que a menudo resulta
engañoso en estas conversaciones es que

1920
01:29:06,872 --> 01:29:10,007
la gente... No me gusta emplear la
palabra 'inteligencia' en este

1921
01:29:10,007 --> 01:29:13,876
contexto... porque 'inteligencia' implica
que piensas, oh, una superinteligencia

1922
01:29:13,876 --> 01:29:17,451
estará sentada... Habrá una única
superinteligencia sentada en un servidor

1923
01:29:17,451 --> 01:29:21,124
y como que adivinará cómo crear nuevas
tecnologías e inventos... que causará

1924
01:29:21,124 --> 01:29:21,858
esta explosión.

1925
01:29:21,844 --> 01:29:22,261
Ajá.

1926
01:29:22,624 --> 01:29:26,264
Y eso no es lo que me estoy imaginando-
... cuando me imagino un crecimiento del

1927
01:29:26,264 --> 01:29:29,398
veinte por ciento. Me imagino que hay
miles de millones de, ya sabes,

1928
01:29:29,398 --> 01:29:33,178
básicamente, como mentes muy inteligentes
parecidas a las humanas potencialmente, o

1929
01:29:33,178 --> 01:29:34,791
que eso es todo lo que se requiere.

1930
01:29:25,404 --> 01:29:25,821
Ajá.

1931
01:29:34,884 --> 01:29:38,345
Pero el hecho de que haya cientos de
millones de ellos, miles de millones,

1932
01:29:38,345 --> 01:29:42,043
cada uno individualmente creando nuevos
productos, descubriendo cómo integrarse

1933
01:29:42,043 --> 01:29:45,172
en la economía, justo como si un
inmigrante inteligente y con mucha

1934
01:29:45,172 --> 01:29:48,586
experiencia llegara al país, no
necesitarías averiguar cómo integrarlo en

1935
01:29:48,586 --> 01:29:52,332
la economía. Ellos lo averiguan. Podrían
iniciar una empresa, podrían, uh, hacer

1936
01:29:52,332 --> 01:29:55,224
inventos, o simplemente aumentar la
productividad en el mundo.

1937
01:29:55,304 --> 01:29:59,722
Y tenemos ejemplos, incluso en el régimen
actual, de lugares que han tenido un

1938
01:29:59,722 --> 01:30:04,026
crecimiento económico del 10, 20%. Sabes,
si solo tienes mucha gente y menos

1939
01:30:04,026 --> 01:30:08,617
capital en comparación con la población,
puedes tener a Hong Kong o Shenzhen o lo

1940
01:30:08,617 --> 01:30:12,347
que sea, que tuvieron décadas de más del
10% de crecimiento. Es...

1941
01:30:12,160 --> 01:30:16,076
Y... Y creo que es solo que hay mucha
gente muy inteligente que está lista para

1942
01:30:16,076 --> 01:30:20,094
usar los recursos y hacer este período de
ponerse al día porque hemos tenido esta

1943
01:30:20,094 --> 01:30:22,655
discontinuidad. Y creo que la IA podría
ser similar.

1944
01:30:22,760 --> 01:30:26,266
Así que creo, creo que lo entiendo, pero
sigo pensando que estás presuponiendo

1945
01:30:26,266 --> 01:30:29,362
algún salto discreto, simplemente un
desbloqueo que estamos esperando

1946
01:30:29,362 --> 01:30:29,772
reclamar.

1947
01:30:30,640 --> 01:30:33,953
y de repente vamos a tener genios en
centros de datos. Y yo, yo sigo pensando

1948
01:30:33,953 --> 01:30:37,180
que estás presuponiendo un salto discreto
que creo que básicamente no tiene

1949
01:30:37,180 --> 01:30:40,538
precedentes históricos, que no encuentro
en ninguna de las estadísticas, y que

1950
01:30:40,538 --> 01:30:42,064
creo que probablemente no sucederá.

1951
01:30:42,060 --> 01:30:45,952
Quiero decir, la revolución industrial es
un salto enorme, ¿no? Pasaste de un 0% o

1952
01:30:45,952 --> 01:30:48,979
0.2% de crecimiento a un 2%. Solo digo
que verás otro salto así.

1953
01:30:46,620 --> 01:30:47,084
Pues... este...

1954
01:30:49,360 --> 01:30:52,541
Estoy un poco sospechoso. Tendría que
mirarlo. Estoy... algo sospechoso... y

1955
01:30:52,541 --> 01:30:53,771
tendría que echar un vistazo.

1956
01:30:53,760 --> 01:30:56,925
Por ejemplo, quizás algunos de los
registros no sean muy fiables de antes de

1957
01:30:56,925 --> 01:30:59,879
la revolución industrial o algo por el
estilo. Así que, uh, soy un poco

1958
01:30:59,879 --> 01:31:03,044
escéptico al respecto, pero, um, sí,
quizás tengas razón. No tengo, no tengo

1959
01:31:03,044 --> 01:31:03,930
opiniones muy firmes.

1960
01:31:04,180 --> 01:31:04,412
Sí.

1961
01:31:04,440 --> 01:31:07,379
Quizás estás diciendo que este fue un
evento singular que fue extremadamente

1962
01:31:07,379 --> 01:31:10,437
mágico, y estás diciendo que quizás habrá
otro evento que será igual de mágico,

1963
01:31:10,437 --> 01:31:12,984
extremadamente mágico- ... romperá
paradigmas y así sucesivamente.

1964
01:31:13,040 --> 01:31:16,968
En realidad, no creo que la, uh, quiero
decir, lo crucial, lo más importante de

1965
01:31:16,968 --> 01:31:20,896
la Revolución Industrial no fue que fuera
algo mágico, ¿verdad? Es decir, si te

1966
01:31:20,896 --> 01:31:24,573
acercaras... lo que verías en mil
setecientos setenta o... mil ochocientos

1967
01:31:24,573 --> 01:31:27,343
setenta no es que hubiera, digamos, una
invención clave.

1968
01:31:17,320 --> 01:31:17,552
Mmm.

1969
01:31:19,880 --> 01:31:20,065
Mmm

1970
01:31:27,340 --> 01:31:28,315
Ajá. Exactamente.

1971
01:31:28,300 --> 01:31:32,236
Pero, al mismo tiempo, se logró llevar la
economía a un régimen donde el progreso

1972
01:31:32,236 --> 01:31:36,025
fue mucho más rápido... y lo exponencial
se multiplicó por diez. Y espero algo

1973
01:31:36,025 --> 01:31:39,765
similar de la inteligencia artificial,
donde no será como si hubiera un único

1974
01:31:39,765 --> 01:31:42,324
momento en el que... hiciéramos la
invención crucial.

1975
01:31:33,800 --> 01:31:33,939
Mm-hm

1976
01:31:35,920 --> 01:31:36,337
Ajá.

1977
01:31:42,340 --> 01:31:45,494
Todavía hay un excedente que se está
desbloqueando, como si hubiera una nueva

1978
01:31:45,494 --> 01:31:48,358
fuente de energía. Hay un... un
desbloqueo, en este caso algún tipo de

1979
01:31:48,358 --> 01:31:51,139
capacidad cognitiva, y hay... un
excedente de trabajo cognitivo, uh,

1980
01:31:51,139 --> 01:31:54,210
cognitivo por hacer. Y esperas que ese
excedente se llene con esta... nueva

1981
01:31:54,210 --> 01:31:55,621
tecnología cuando cruce el umbral.

1982
01:31:51,520 --> 01:31:51,937
Así es.

1983
01:31:55,640 --> 01:31:59,202
Sí. Y, quiero decir, uhm... quizás una
manera de pensarlo es a través de la

1984
01:31:59,202 --> 01:32:03,005
historia, mucho crecimiento, o sea, el
crecimiento se da porque la gente propone

1985
01:32:03,005 --> 01:32:06,808
ideas, y luego la gente está, digamos,
ahí fuera haciendo cosas para... ejecutar

1986
01:32:06,808 --> 01:32:08,782
esas ideas y generar un resultado
valioso.

1987
01:32:08,820 --> 01:32:11,993
Y durante la mayor parte de este tiempo,
la población no está explotando como

1988
01:32:11,993 --> 01:32:15,333
motor de crecimiento. Durante los últimos
50 años, la gente ha argumentado que el

1989
01:32:15,333 --> 01:32:18,673
crecimiento se ha estancado. La población
en los países fronterizos también se ha

1990
01:32:18,673 --> 01:32:21,387
estancado. Creo que volvemos al
crecimiento hiperexponencial de la

1991
01:32:21,387 --> 01:32:22,473
población y la producción.

1992
01:32:15,720 --> 01:32:15,859
Mmm.

1993
01:32:18,400 --> 01:32:18,725
Ajá.

1994
01:32:22,500 --> 01:32:22,917
Ajá.

1995
01:32:23,080 --> 01:32:25,261
¿Verdad? Lo siento, crecimiento
exponencial poblacional que causa

1996
01:32:25,261 --> 01:32:26,795
crecimiento hiperexponencial en la
producción.

1997
01:32:27,020 --> 01:32:31,261
Sí. Quiero decir, la verdad es que, sí,
es muy difícil de saber. Entiendo ese

1998
01:32:31,261 --> 01:32:33,939
punto de vista, pero no lo siento
intuitivamente.

1999
01:32:29,900 --> 01:32:30,271
Ajá.

2000
01:32:31,660 --> 01:32:31,892
Sí.

2001
01:32:34,280 --> 01:32:37,530
Así que acabamos de obtener acceso a Veo
3.1 de Google,

2002
01:32:38,220 --> 01:32:42,677
y ha sido muy divertido jugar con él. Lo
primero que hicimos fue ejecutar una

2003
01:32:42,677 --> 01:32:47,193
serie de indicaciones a través de Veo 3.0
y 3.1 para ver qué ha cambiado en la

2004
01:32:47,193 --> 01:32:49,597
nueva versión. Así que aquí está Veo 3.0.

2005
01:32:49,660 --> 01:32:52,910
Hola, mi nombre es Max, y me quedé
atascado en un mínimo local otra vez.

2006
01:32:53,000 --> 01:32:56,970
Está bien, Max, todos hemos pasado por
eso. A mí me llevó tres épocas enteras

2007
01:32:56,970 --> 01:32:58,851
salir de ahí. Y aquí tienes Veo 3.1.

2008
01:32:59,440 --> 01:33:03,108
Hola, me llamo Max, y volví a quedarme
atascado en un mínimo local otra vez.

2009
01:33:03,140 --> 01:33:06,596
Está bien, Max, todos hemos estado en esa
situación. A mí me llevó tres épocas

2010
01:33:06,596 --> 01:33:07,180
salir de ahí.

2011
01:33:07,240 --> 01:33:10,940
La salida de 3.1 es simplemente más
coherente, y el audio es notablemente de

2012
01:33:10,940 --> 01:33:14,344
mayor calidad. De hecho, hemos estado
usando Veo desde hace un tiempo.

2013
01:33:14,344 --> 01:33:18,290
Publicamos un ensayo a principios de este
año sobre empresas de IA, completamente

2014
01:33:18,290 --> 01:33:22,286
animado por Veo 2.0, y ha sido realmente
asombroso ver lo rápido que estos modelos

2015
01:33:22,286 --> 01:33:23,076
están mejorando.

2016
01:33:23,120 --> 01:33:26,848
Esta actualización hace que Veo sea
todavía más útil para darle vida a

2017
01:33:26,848 --> 01:33:29,064
nuestras ideas y a nuestras
explicaciones.

2018
01:33:29,140 --> 01:33:32,514
Puedes probar Veo ahora mismo en la
aplicación Gemini con las suscripciones

2019
01:33:32,514 --> 01:33:36,116
Pro y Ultra, y también puedes acceder a
él mediante la API de Gemini o por medio

2020
01:33:36,116 --> 01:33:39,673
de Google Flow. Tú me recomendaste el
libro de Nick Lane, y entonces, basándome

2021
01:33:39,673 --> 01:33:43,229
en esa recomendación, la verdad es que
también lo encontré súper interesante, y

2022
01:33:43,229 --> 01:33:44,232
por eso lo entrevisté.

2023
01:33:44,520 --> 01:33:48,049
De hecho tengo algunas preguntas sobre la
inteligencia y su historia evolutiva.

2024
01:33:49,180 --> 01:33:53,586
Ahora que, en los últimos 20 años, haces
investigación de IA, quizás tienes un

2025
01:33:53,586 --> 01:33:57,248
sentido más tangible de lo que es la
inteligencia, lo que implica

2026
01:33:57,248 --> 01:33:58,050
desarrollarla.

2027
01:33:58,060 --> 01:34:03,262
¿Estás más o menos sorprendido por el
hecho de que la evolución simplemente

2028
01:34:03,262 --> 01:34:05,722
diera con ello de forma espontánea?

2029
01:34:05,800 --> 01:34:10,053
Mm-hmm. Eh, por cierto, me encantan
muchísimo los libros de Nick Lane. Así

2030
01:34:10,053 --> 01:34:14,831
que, sí, estaba escuchando su podcast, de
hecho, de camino hacia aquí. Con respecto

2031
01:34:14,831 --> 01:34:18,153
a la inteligencia y a su evolución, me
pareció bastante...

2032
01:34:09,740 --> 01:34:09,879
Sí.

2033
01:34:18,160 --> 01:34:21,886
Quiero decir, es algo muy, muy reciente,
¿verdad que sí? Me sorprende de verdad

2034
01:34:21,886 --> 01:34:23,129
que haya evolucionado, sí.

2035
01:34:23,160 --> 01:34:26,386
Me, me parece fascinante pensar en todos
los mundos que hay ahí fuera, como si

2036
01:34:26,386 --> 01:34:29,654
hubiera mil planetas como... la Tierra y
cómo se ven. Creo que Nick Lane estuvo

2037
01:34:29,654 --> 01:34:32,461
aquí hablando de las primeras etapas,
¿verdad? O sea, él espera, eh,

2038
01:34:32,461 --> 01:34:35,729
básicamente formas de vida muy similares,
a grandes rasgos, y cosas parecidas a

2039
01:34:35,729 --> 01:34:38,578
bacterias en la mayoría de ellos. Y luego
hay algunas interrupciones.

2040
01:34:36,100 --> 01:34:36,517
Así es.

2041
01:34:38,700 --> 01:34:42,332
Yo esperaría que la evolución de la
inteligencia intuitivamente siento que

2042
01:34:42,332 --> 01:34:45,665
debería ser un evento bastante raro, y ha
habido animales durante...

2043
01:34:45,700 --> 01:34:48,561
Supongo que quizás deberías basarlo en
cuánto tiempo algo ha existido. Por

2044
01:34:48,561 --> 01:34:51,540
ejemplo, si las bacterias han estado
presentes por dos mil millones de años y

2045
01:34:51,540 --> 01:34:54,636
no ocurrió nada, entonces la transición a
un eucariota es probablemente bastante

2046
01:34:54,636 --> 01:34:57,811
difícil porque, eh, porque las bacterias
en realidad, eh, aparecieron muy temprano

2047
01:34:57,811 --> 01:34:59,771
en la e-evolución... o en la historia de
la Tierra.

2048
01:34:58,580 --> 01:34:58,719
Sí

2049
01:35:00,080 --> 01:35:03,850
Eh, y supongo, eh, ¿cuánto tiempo hemos
tenido animales? Quizás un par de cientos

2050
01:35:03,850 --> 01:35:07,385
de millones de años, como animales
multicelulares... animales que corren, se

2051
01:35:07,385 --> 01:35:11,202
arrastran, etcétera, eh, lo que es quizás
el diez por ciento de, eh, la vida de la

2052
01:35:11,202 --> 01:35:14,926
Tierra o algo así. Así que, o quizás en
esa escala de tiempo, en realidad no es,

2053
01:35:14,926 --> 01:35:15,916
no es tan complicado.

2054
01:35:15,960 --> 01:35:20,161
Todavía me parece sorprendente, creo,
intuitivamente, que se haya desarrollado.

2055
01:35:20,161 --> 01:35:23,932
Quizás esperaría solo muchas formas de
vida animales, haciendo cosas de

2056
01:35:23,932 --> 01:35:28,296
animales. El hecho de que puedas obtener
algo que crea cultura y conocimiento... y

2057
01:35:28,296 --> 01:35:30,774
lo acumula es... es... Me resulta
sorprendente.

2058
01:35:27,920 --> 01:35:28,012
Sí

2059
01:35:30,920 --> 01:35:34,495
Bueno, hay... Entonces, de hecho, hay un
par de seguimientos interesantes.

2060
01:35:35,500 --> 01:35:38,684
Si aceptas la perspectiva de Sun, de que,
en realidad, el punto clave de la

2061
01:35:38,684 --> 01:35:41,911
inteligencia es la inteligencia animal, y
que, según sus estudios, si logras

2062
01:35:41,911 --> 01:35:45,268
entender a la ardilla, habrás recorrido
la mayor parte del camino hacia la IAG.

2063
01:35:45,268 --> 01:35:48,495
Um, entonces, la inteligencia de la
ardilla, supongo, apareció justo después

2064
01:35:48,495 --> 01:35:49,571
de la explosión cámbrica.

2065
01:35:40,000 --> 01:35:40,139
Mm-hm

2066
01:35:44,680 --> 01:35:45,051
Mmm.

2067
01:35:49,820 --> 01:35:53,598
Hace seiscientos millones de años. Parece
que lo que provocó todo eso fue el evento

2068
01:35:53,598 --> 01:35:57,192
de oxigenación, también hace seiscientos
millones de años. Pero inmediatamente,

2069
01:35:57,192 --> 01:36:00,879
una especie de algoritmo de inteligencia
ya estaba ahí para, como, crear la... la

2070
01:36:00,879 --> 01:36:01,708
inteligencia, ¿no?

2071
01:36:01,740 --> 01:36:06,371
Así que es sugerente que la inteligencia
animal fuera así. En cuanto hubo oxígeno

2072
01:36:06,371 --> 01:36:10,424
en el ambiente y el eucariota, podías
simplemente obtener el algoritmo.

2073
01:36:11,280 --> 01:36:14,563
Eh, yo- yo- yo... Quizás fue, como, una
especie de accidente que la evolución lo

2074
01:36:14,563 --> 01:36:17,846
detectó, lo quiso tan rápido, pero yo- yo
no sé si eso sugiere que sea, como, en

2075
01:36:17,846 --> 01:36:20,382
realidad bastante, uh... Al final, vaya a
ser bastante simple.

2076
01:36:20,440 --> 01:36:23,520
Sí. Básicamente es tan difícil saberlo,
¿verdad?, con todo esto. Supongo que

2077
01:36:23,520 --> 01:36:26,723
puedes basarlo un poco en cuánto tiempo
ha existido algo o cuánto tiempo parece

2078
01:36:26,723 --> 01:36:29,885
que algo ha estado estancado. Así que
Nicoline es muy buena describiendo este,

2079
01:36:29,885 --> 01:36:32,514
como, cuello de botella muy aparente en,
uh, bacterias y arqueas.

2080
01:36:27,820 --> 01:36:28,005
Sí.

2081
01:36:32,560 --> 01:36:32,792
Sí.

2082
01:36:32,800 --> 01:36:35,877
Durante dos mil millones de años, no
sucedió nada. O sea, una diversidad

2083
01:36:35,877 --> 01:36:38,868
extrema de sustancias químicas, de
bioquímica, y sin embargo, nada que

2084
01:36:38,868 --> 01:36:39,951
crezca para llegar a ser-

2085
01:36:39,980 --> 01:36:40,119
Sí

2086
01:36:40,600 --> 01:36:44,632
animales. Dos mil millones de años. Mmm,
no sé si hemos visto un equivalente

2087
01:36:44,632 --> 01:36:48,341
exacto de eso en animales y su
inteligencia, a tu punto, ¿verdad? Pero

2088
01:36:48,341 --> 01:36:52,535
supongo que también podríamos verlo con
respecto a cuántas veces creemos que la

2089
01:36:52,535 --> 01:36:55,600
evolución, eh, la inteligencia ha surgido
individualmente.

2090
01:36:55,659 --> 01:36:57,980
Eso es algo muy bueno... sí, muy bueno
para investigar.

2091
01:36:58,020 --> 01:37:01,136
Quizás una idea sobre eso es que casi
siento que, um... Bueno, existe la

2092
01:37:01,136 --> 01:37:04,340
inteligencia homínida. Y está, diría yo,
como la inteligencia de las aves,

2093
01:37:04,340 --> 01:37:07,062
¿verdad? Como los cuervos, y otros
animales, son extremadamente

2094
01:37:07,062 --> 01:37:07,633
inteligentes.

2095
01:37:05,880 --> 01:37:06,112
Sí.

2096
01:37:07,680 --> 01:37:07,912
Sí.

2097
01:37:07,899 --> 01:37:11,525
Pero, eh, en realidad... Sus partes
cerebrales son bastante distintas y no

2098
01:37:11,525 --> 01:37:15,450
tenemos tanta, eh, existencia. Así que
quizás sea un... Un leve evento de... Hay

2099
01:37:15,450 --> 01:37:18,928
una leve indicación de que la
inteligencia surge varias veces. Y en ese

2100
01:37:18,928 --> 01:37:21,413
caso, quizás lo esperarías más a menudo o
algo así.

2101
01:37:21,420 --> 01:37:26,515
Sí. Un antiguo invitado, Gwern, y también
Carl Schliman han he- he- hecho un punto

2102
01:37:26,515 --> 01:37:31,108
muy interesante al respecto, que es que
su perspectiva es que el algoritmo

2103
01:37:31,108 --> 01:37:34,190
escalable que los humanos y los primates
poseen...

2104
01:37:34,980 --> 01:37:36,884
Se originó en las aves asimismo.

2105
01:37:36,880 --> 01:37:37,344
Ajá.

2106
01:37:37,659 --> 01:37:41,694
Y quizás en otros momentos también. Pero
en... los humanos encontraron un nicho

2107
01:37:41,694 --> 01:37:44,695
evolutivo que recompensaba los
incrementos marginales en la

2108
01:37:44,695 --> 01:37:45,368
inteligencia-

2109
01:37:45,360 --> 01:37:45,499
Mmm

2110
01:37:46,120 --> 01:37:50,398
Y también poseía un algoritmo cerebral
escalable que permitiera lograr esos

2111
01:37:50,398 --> 01:37:52,017
aumentos en la inteligencia.

2112
01:37:52,640 --> 01:37:55,684
El... Y así, por ejemplo, si un pájaro
tuviera un cerebro más grande,

2113
01:37:55,684 --> 01:37:59,221
simplemente colapsaría en el aire. Así
que, es muy inteligente para el tamaño de

2114
01:37:59,221 --> 01:38:02,624
su cerebro, pero... no está en un nicho
que recompense un cerebro más grande.

2115
01:38:02,640 --> 01:38:03,847
Ajá. Sí, por supuesto.

2116
01:38:03,840 --> 01:38:05,744
Um, tal vez similar con unos muy listos
oftal-

2117
01:38:05,760 --> 01:38:06,781
¿Delfines, etcétera?

2118
01:38:06,780 --> 01:38:10,590
Exacto, sí. Mientras que los humanos,
sabes, tenemos manos que nos permiten

2119
01:38:10,590 --> 01:38:14,453
aprender a usar herramientas. Podemos
externalizar la digestión, más energía

2120
01:38:14,453 --> 01:38:17,182
para el cerebro. Y eso, eh, pone en
marcha el volante.

2121
01:38:07,400 --> 01:38:07,771
Ajá.

2122
01:38:14,760 --> 01:38:15,131
Ajá.

2123
01:38:17,480 --> 01:38:20,657
Eh, sí, y solo cosas con las que
trabajar. Quiero decir, supongo que sería

2124
01:38:20,657 --> 01:38:24,009
más difícil... Si fuera un delfín. Eh,
quiero decir, ¿cómo lo haces? No puedes

2125
01:38:24,009 --> 01:38:27,404
tener fuego, por ejemplo... y cosas así.
Quiero decir, el universo de cosas que

2126
01:38:27,404 --> 01:38:30,799
puedes hacer en el agua, o sea, bajo el
agua, es probablemente menor que lo que

2127
01:38:30,799 --> 01:38:32,758
puedes hacer en tierra. Eh, solo
químicamente.

2128
01:38:21,220 --> 01:38:21,591
Ajá.

2129
01:38:31,640 --> 01:38:31,825
Sí.

2130
01:38:32,860 --> 01:38:33,277
Correcto.

2131
01:38:33,500 --> 01:38:37,573
Sí, estoy de acuerdo con esto, con este
punto de vista de los nichos y lo que se

2132
01:38:37,573 --> 01:38:41,491
está incentivando. Aún me parece un poco
milagroso que, eh... No sé... Quizás

2133
01:38:41,491 --> 01:38:45,306
habría esperado que se centraran en, por
ejemplo, animales con músculos más

2134
01:38:45,306 --> 01:38:46,131
grandes, ¿sabes?

2135
01:38:46,180 --> 01:38:46,597
Así es.

2136
01:38:46,700 --> 01:38:50,164
O sea, pasar por la inteligencia es en
realidad un punto de quiebre

2137
01:38:50,164 --> 01:38:51,529
verdaderamente fascinante.

2138
01:38:51,720 --> 01:38:55,490
La- la- la forma en que Gwern lo expresó
es que la razón por la que fue tan

2139
01:38:55,490 --> 01:38:59,515
difícil es que hay una línea muy delgada
entre estar en una situación donde algo

2140
01:38:59,515 --> 01:39:03,388
es tan importante de aprender que no solo
vale la pena destilar los circuitos

2141
01:39:03,388 --> 01:39:04,305
exactos correctos-

2142
01:39:04,319 --> 01:39:04,551
Sí

2143
01:39:04,600 --> 01:39:07,713
directamente de vuelta a tu- ... ADN- ...
frente a que no es lo suficientemente

2144
01:39:07,713 --> 01:39:09,429
importante como para aprenderlo en
absoluto.

2145
01:39:06,420 --> 01:39:06,605
Sí

2146
01:39:09,460 --> 01:39:09,831
Claro.

2147
01:39:10,120 --> 01:39:13,606
Tiene que ser algo que sea como... impo-
eh, tienes que... i- incentivar la

2148
01:39:13,606 --> 01:39:16,668
construcción... del algoritmo para que
aprenda... en su vida útil.

2149
01:39:15,740 --> 01:39:18,719
Tienes que in- Sí, exactamente. Tienes
que incentivar cierta adaptabilidad. En

2150
01:39:18,719 --> 01:39:21,854
realidad quieres algo que... En realidad
quieres ambientes que sean impredecibles.

2151
01:39:21,854 --> 01:39:24,563
Entonces, la evolución no puede
incorporar tus algoritmos en tus pesos.

2152
01:39:24,620 --> 01:39:27,640
Muchos, eh... Muchos animales están
básicamente preprogramados... en este

2153
01:39:27,640 --> 01:39:30,576
sentido, y los humanos tienen que
resolverlo en el momento de la prueba

2154
01:39:30,576 --> 01:39:31,121
cuando nacen.

2155
01:39:31,240 --> 01:39:34,118
Y entonces, quizás, había, eh... En
realidad, lo que quieres son este tipo

2156
01:39:34,118 --> 01:39:37,274
de, eh, entornos que de hecho cambian muy
rápidamente o algo por el estilo, donde

2157
01:39:37,274 --> 01:39:40,074
no puedes prever, eh, qué es lo que
funcionará bien. Y así, en realidad,

2158
01:39:40,074 --> 01:39:43,229
depositas toda esa inteligencia... Creas
inteligencia para poder resolverlo en el

2159
01:39:43,229 --> 01:39:44,057
momento de la prueba.

2160
01:39:44,380 --> 01:39:48,153
Eh, pues, eh, Quentin Pope tenía esta
interesante entrada de blog donde decía

2161
01:39:48,153 --> 01:39:51,727
que la razón por la que no espera un
despegue abrupto es, ehm, que... Los

2162
01:39:51,727 --> 01:39:55,600
humanos tuvieron este despegue abrupto,
donde hace sesenta mil años, parece que

2163
01:39:55,600 --> 01:39:58,033
teníamos el tipo de arquitecturas que
tenemos hoy.

2164
01:39:58,040 --> 01:39:58,736
Ajá. Ajá.

2165
01:39:58,720 --> 01:40:02,602
Y hace 10.000 años, la revolución
agrícola, la modernidad... ¿Qué es lo que

2166
01:40:02,602 --> 01:40:06,643
estaba pasando en esos 50.000 años? Donde
tenías que construir esta especie de

2167
01:40:06,643 --> 01:40:10,158
andamiaje cultural para poder acumular
conocimiento... a lo largo de

2168
01:40:10,158 --> 01:40:10,840
generaciones.

2169
01:40:03,980 --> 01:40:04,397
Ajá.

2170
01:40:10,860 --> 01:40:11,185
Ajá.

2171
01:40:11,579 --> 01:40:14,920
Esta es una habilidad que existe de forma
gratuita en la manera en que entrenamos

2172
01:40:14,920 --> 01:40:18,219
la IA, donde si reentrenas un modelo, aún
puede... O sea, es... En muchos casos,

2173
01:40:18,219 --> 01:40:21,310
están literalmente destilados, pero
pueden ser entrenados unos sobre otros.

2174
01:40:21,310 --> 01:40:24,442
Sabes, pueden, pueden ser entrenados en
el mismo corpus de preentrenamiento.

2175
01:40:24,819 --> 01:40:28,156
Eh, no tienen que empezar literalmente
desde cero. Así que, hay un sentido en el

2176
01:40:28,156 --> 01:40:31,494
que aquello que a los humanos les tomó
mucho tiempo para que este ciclo cultural

2177
01:40:31,494 --> 01:40:34,705
se pusiera en marcha, simplemente viene
gratis con la forma en que hacemos el

2178
01:40:34,705 --> 01:40:35,593
entrenamiento de LLM.

2179
01:40:32,760 --> 01:40:32,899
Ah

2180
01:40:35,680 --> 01:40:39,222
Eh, sí y no, porque los LLM no tienen
realmente el equivalente de una cultura,

2181
01:40:39,222 --> 01:40:42,672
y quizás les estamos dando demasiado e
incentivando a no crearla o algo así.

2182
01:40:42,672 --> 01:40:46,168
Pero supongo, como la mención de la
cultura y de un registro escrito al final

2183
01:40:46,168 --> 01:40:49,802
de pasarse notas entre ellos, no creo que
haya un equivalente de eso con los LLM

2184
01:40:49,802 --> 01:40:50,355
ahora mismo.

2185
01:40:50,380 --> 01:40:50,519
Ajá.

2186
01:40:50,540 --> 01:40:54,449
Así que, los LLM no tienen realmente
cultura ahora mismo, y es como uno de

2187
01:40:54,449 --> 01:40:56,484
los, creo, uh, impedimentos, diría yo.

2188
01:40:56,620 --> 01:40:59,917
¿Podrías darme una idea de cómo podría
ser la cultura de los LLM?

2189
01:41:00,040 --> 01:41:03,452
Eh, entonces, en el caso más simple,
sería como un bloc de notas gigante que

2190
01:41:03,452 --> 01:41:07,001
el LLM puede editar. Y mientras lee cosas
o mientras ayuda con el trabajo, está

2191
01:41:07,001 --> 01:41:10,641
editando ese bloc para sí mismo. ¿Por qué
un LLM no podría escribir un libro para

2192
01:41:10,641 --> 01:41:11,278
los otros LLM?

2193
01:41:11,680 --> 01:41:15,796
Eso sería genial. Como, ¿por qué otros
LLM no pueden leer el libro de este LLM e

2194
01:41:15,796 --> 01:41:19,808
inspirarse o so- sorprenderse por él o
algo así? No hay equivalencia para nada

2195
01:41:19,808 --> 01:41:20,224
de esto.

2196
01:41:12,400 --> 01:41:12,725
Claro.

2197
01:41:20,240 --> 01:41:23,919
¡Qué interesante! ¿Cuándo esperarías que
ese tipo de cosas comenzaran a ocurrir? Y

2198
01:41:23,919 --> 01:41:27,281
una pregunta más general sobre, por
ejemplo, los sistemas multiagente y una

2199
01:41:27,281 --> 01:41:30,642
especie de inteligencia artificial
independiente... civilización y cultura.

2200
01:41:30,920 --> 01:41:34,677
Creo que hay dos ideas muy potentes en el
ámbito de los multiagentes que no han

2201
01:41:34,677 --> 01:41:38,484
sido realmente exploradas o aprovechadas
del todo, o algo así. La primera, diría

2202
01:41:38,484 --> 01:41:41,471
yo, es la cultura, y los LLM son,
básicamente, un repertorio de

2203
01:41:41,471 --> 01:41:43,783
conocimiento en expansión para sus
propios fines.

2204
01:41:35,819 --> 01:41:36,004
Mmm

2205
01:41:44,160 --> 01:41:48,030
Uh, el segundo se parece mucho más a, uh,
la poderosa idea del auto-juego, uh, en

2206
01:41:48,030 --> 01:41:51,852
mi mente, es extremadamente poderosa. Así
que la evolución en realidad es mucha,

2207
01:41:51,852 --> 01:41:55,239
um, uh, competencia básicamente- ...
impulsando la inteligencia y, y la

2208
01:41:55,239 --> 01:41:55,723
evolución.

2209
01:41:55,880 --> 01:41:59,819
Y, uh, para... En AlphaGo, más
algorítmicamente, como, uh, AlphaGo-...

2210
01:41:59,819 --> 01:42:04,158
juega contra sí mismo, y así aprende a
dominar el Go. Y no hay equivalente de

2211
01:42:04,158 --> 01:42:05,585
LLMs que se autoentrenen.

2212
01:42:05,676 --> 01:42:08,729
Eh, pero esperaría que eso también
existiera, pero nadie lo ha hecho aún.

2213
01:42:08,729 --> 01:42:12,037
Por ejemplo, ¿por qué un LLM no puede
crear un montón de problemas que otro LLM

2214
01:42:12,037 --> 01:42:15,133
esté aprendiendo a resolver? Y luego el
LLM siempre está intentando, como,

2215
01:42:15,133 --> 01:42:16,914
presentar problemas cada vez más
difíciles.

2216
01:42:13,136 --> 01:42:13,507
Mm.

2217
01:42:16,916 --> 01:42:17,287
Claro.

2218
01:42:17,536 --> 01:42:21,327
Cosas por el estilo, ¿sabes? Así que,
creo que hay muchas maneras de

2219
01:42:21,327 --> 01:42:25,741
organizarlo. Y creo que es un campo de
investigación. Pero creo que no he visto

2220
01:42:25,741 --> 01:42:29,703
nada que convincentemente afirme ambas
cosas. Como mejoras multiagente.

2221
01:42:27,556 --> 01:42:27,834
Mm.

2222
01:42:29,976 --> 01:42:33,052
Sigo pensando que nos encontramos
principalmente en el terreno de un único

2223
01:42:33,052 --> 01:42:36,170
agente individual, pero creo que...
también pienso que eso va a cambiar. Y,

2224
01:42:36,170 --> 01:42:39,500
uhm, en el ámbito de la cultura también,
yo incluiría a las organizaciones. Y no

2225
01:42:39,500 --> 01:42:42,660
hemos visto nada parecido de forma
convincente tampoco. Uhm, por eso todavía

2226
01:42:42,660 --> 01:42:44,093
estamos en una etapa muy temprana.

2227
01:42:41,676 --> 01:42:41,908
Mm.

2228
01:42:44,256 --> 01:42:48,112
Y p- ¿puedes identificar el cuello de
botella clave que está, eh, impidiendo

2229
01:42:48,112 --> 01:42:50,014
este tipo de colaboración entre LLMs?

2230
01:42:50,176 --> 01:42:52,312
A lo mejor, la manera en que yo lo
expresaría sería

2231
01:42:53,956 --> 01:42:56,727
Sorprendentemente... De nuevo, algunas de
estas analogías funcionan y no deberían,

2232
01:42:56,727 --> 01:42:57,856
pero sorprendentemente, lo hacen.

2233
01:42:57,876 --> 01:43:01,050
Muchos de los modelos más pequeños o los
más tontos mo- eh, como, los modelos más

2234
01:43:01,050 --> 01:43:03,868
pequeños de alguna manera se parecen
notablemente a un estudiante de kí-

2235
01:43:03,868 --> 01:43:06,844
kinder. O a un estudiante de primaria o
de secundaria. Etcétera. Y de alguna

2236
01:43:06,844 --> 01:43:09,582
manera, todavía no hemos, como,
"graduado" lo suficiente como para que

2237
01:43:09,582 --> 01:43:10,693
esto pueda tomar el control.

2238
01:43:03,516 --> 01:43:03,841
Ajá.

2239
01:43:10,756 --> 01:43:13,731
Es como que, en su mayoría, sigue
siendo... Como que mi código Cloth o

2240
01:43:13,731 --> 01:43:16,189
Codex... todavía se sienten como en un
estado elemental s-

2241
01:43:13,376 --> 01:43:13,608
Mm

2242
01:43:16,496 --> 01:43:19,384
estudiante de posgrado. Sé que pueden
hacer exámenes de doctorado, pero

2243
01:43:19,384 --> 01:43:22,767
cognitivamente todavía se sienten como un
niño de jardín de infancia, un estudiante

2244
01:43:22,767 --> 01:43:25,861
de primaria. Así que yo, no creo que
puedan crear cultura porque todavía son

2245
01:43:25,861 --> 01:43:26,109
niños.

2246
01:43:22,896 --> 01:43:23,267
Vaya.

2247
01:43:26,196 --> 01:43:29,898
Eh, ¿sabes? O sea, son niños prodigio.
Eh, tienen memoria episódica... Tienen

2248
01:43:29,898 --> 01:43:33,406
memoria perfecta de todo esto, etcétera,
y pueden, eh, crear de forma muy

2249
01:43:33,406 --> 01:43:36,134
convincente todo tipo de cosas que se ven
realmente bien.

2250
01:43:36,676 --> 01:43:39,956
Pero sigo pensando que no saben realmente
lo que hacen, y no tienen realmente la

2251
01:43:39,956 --> 01:43:42,945
cognición en todas estas pequeñas
casillas que aún tenemos que recopilar.

2252
01:43:42,976 --> 01:43:47,178
Sí. Has comentado cómo estuviste en
Tesla, liderando el desarrollo de la

2253
01:43:47,178 --> 01:43:51,795
conducción autónoma, desde el año 2017
hasta el 2022, y entonces pudiste ver de

2254
01:43:51,795 --> 01:43:54,400
primera mano el progreso que se hizo
desde...

2255
01:43:54,396 --> 01:43:57,951
Pasamos de demostraciones geniales a
tener ahora miles de coches circulando de

2256
01:43:57,951 --> 01:44:01,414
forma autónoma. ¿Por qué llevó eso una
década? ¿Qué es lo que estaba pasando

2257
01:44:01,414 --> 01:44:02,522
durante todo ese tiempo?

2258
01:44:02,536 --> 01:44:06,526
Sí. Uh, diría que una cosa a la que me
opondría casi al instante también es que

2259
01:44:06,526 --> 01:44:08,573
esto ni siquiera está cerca de terminar.

2260
01:44:10,416 --> 01:44:13,513
Bueno, de muchas maneras diferentes que
les voy a explicar ahora. Realmente creo

2261
01:44:13,513 --> 01:44:16,414
que la conducción autónoma es algo muy
interesante, porque es, sin duda, de

2262
01:44:16,414 --> 01:44:19,159
donde provienen muchas de mis
intuiciones, ya que le dediqué cinco años

2263
01:44:19,159 --> 01:44:21,786
a este tema. Y tiene toda una historia
donde, de hecho, las primeras

2264
01:44:21,786 --> 01:44:24,765
demostraciones de conducción autónoma se
remontan a la década de los ochenta.

2265
01:44:24,916 --> 01:44:25,287
Mm.

2266
01:44:25,316 --> 01:44:29,521
Uh, puedes ver una demo de CMU en 1986.
Hay un camión que se conduce solo por las

2267
01:44:29,521 --> 01:44:30,099
carreteras.

2268
01:44:30,276 --> 01:44:33,949
Eh, uh, pero bueno, para avanzar
rápidamente. Creo que cuando estaba por

2269
01:44:33,949 --> 01:44:37,778
unirme a Tesla, tuve, eh... tuve una
demostración muy, muy temprana, eh, de

2270
01:44:37,778 --> 01:44:41,607
Waymo, y básicamente me dio un recorrido
perfecto, eh, en el año dos mil...

2271
01:44:41,656 --> 01:44:45,223
Dos mil catorce o algo parecido. Así que,
el coche de Waymo conducía perfectamente

2272
01:44:45,223 --> 01:44:45,928
hace una década.

2273
01:44:46,016 --> 01:44:49,591
Uh, nos llevó por Palo Alto y así...
porque tenía un amigo que trabajaba allí.

2274
01:44:49,676 --> 01:44:53,194
Eh, y pensé que estaba muy cerca, y aun
así tardó mucho tiempo. Y sí creo que

2275
01:44:53,194 --> 01:44:56,944
para algunos... Hay... Para ciertos tipos
de, eh, tareas y trabajos y cosas por el

2276
01:44:56,944 --> 01:45:00,277
estilo, eh, la... existe una brecha muy
grande entre la demostración y el

2277
01:45:00,277 --> 01:45:03,518
producto, donde la demostración es muy
sencilla, pero los productos son

2278
01:45:03,518 --> 01:45:04,629
realmente muy difíciles.

2279
01:45:04,696 --> 01:45:08,163
Um, y es especialmente el caso en
situaciones como la conducción autónoma

2280
01:45:08,163 --> 01:45:11,774
donde, uh, el, uh, el costo de un fracaso
es demasiado alto, ¿verdad? Muchas

2281
01:45:11,774 --> 01:45:15,530
industrias, tareas y trabajos quizás no
tienen esa propiedad, pero cuando sí la

2282
01:45:15,530 --> 01:45:17,745
tienes, eso definitivamente aumenta los
plazos.

2283
01:45:17,996 --> 01:45:21,811
Realmente creo que, por ejemplo, en
ingeniería de software, esa propiedad sí

2284
01:45:21,811 --> 01:45:25,780
existe. Creo que para mucho código "vibe"
no es así, pero si escribes código de

2285
01:45:25,780 --> 01:45:29,698
producción real, esa propiedad debería
existir, porque cualquier error lleva a

2286
01:45:29,698 --> 01:45:31,835
una vulnerabilidad de seguridad o algo
así.

2287
01:45:31,876 --> 01:45:32,061
Mmm.

2288
01:45:32,056 --> 01:45:35,596
Y millones y cientos de millones de
números de seguridad social personales de

2289
01:45:35,596 --> 01:45:39,183
personas, etcétera, se filtran o algo
así. Y por eso creo que es un caso en el

2290
01:45:39,183 --> 01:45:42,584
que, en el software, la gente debería
tener cuidado, como en la conducción

2291
01:45:42,584 --> 01:45:46,217
autónoma. Como en la conducción autónoma,
si... si las cosas salen mal, podrías

2292
01:45:46,217 --> 01:45:46,963
sufrir lesiones.

2293
01:45:47,116 --> 01:45:50,962
En, eh, supongo que hay resultados
peores, pero supongo que en, en software,

2294
01:45:50,962 --> 01:45:55,167
casi siento que es realmente ilimitado lo
terrible que algunas cosas podrían llegar

2295
01:45:55,167 --> 01:45:55,475
a ser.

2296
01:45:56,036 --> 01:45:56,732
Interesante.

2297
01:45:56,916 --> 01:46:00,382
Así que, realmente creo que comparten esa
propiedad. Y luego, creo que básicamente

2298
01:46:00,382 --> 01:46:03,591
lo que requiere una gran cantidad de
tiempo y la manera de entenderlo es que

2299
01:46:03,591 --> 01:46:06,929
es una marcha de nueves, y cada uno de
esos nueves es una cantidad constante de

2300
01:46:06,929 --> 01:46:07,272
trabajo.

2301
01:46:08,336 --> 01:46:11,137
Así que cada nueve por sí solo es la
misma cantidad de trabajo. Así que cuando

2302
01:46:11,137 --> 01:46:13,866
obtienes una demo y algo funciona el
noventa por ciento de las veces, eso es

2303
01:46:13,866 --> 01:46:16,449
solo, uh, eso es solo, uh, el pri- el
primer nueve, y luego necesitas un

2304
01:46:16,449 --> 01:46:19,433
segundo nueve, un tercer nueve, un cuarto
nueve, un quinto nueve. Y mientras estuve

2305
01:46:19,433 --> 01:46:21,106
en Tesla por... ¿fueron cinco años más o
menos?

2306
01:46:21,156 --> 01:46:24,548
Creo que pasamos por quizás tres nueves o
dos nueves. No sé qué es, pero como

2307
01:46:24,548 --> 01:46:28,074
múltiples nueves de iteración. Todavía
quedan más nueves por alcanzar, y por eso

2308
01:46:28,074 --> 01:46:29,190
estas cosas tardan tanto.

2309
01:46:29,616 --> 01:46:32,929
Eh, y por eso fue definitivamente
formativo para mí, ver algo que era una

2310
01:46:32,929 --> 01:46:36,703
demo. Estoy muy poco impresionado por las
demos. Eh, así que cada vez que veo demos

2311
01:46:36,703 --> 01:46:39,879
de cualquier cosa, me siento
extremadamente poco impresionado por eso.

2312
01:46:40,996 --> 01:46:44,839
Eh, funciona mejor si puedes... Eh, si es
una demo que alguien preparó y solo te la

2313
01:46:44,839 --> 01:46:45,965
está mostrando, es peor.

2314
01:46:45,996 --> 01:46:49,116
Si puedes interactuar con ello, es un
poco mejor. Pero incluso así, no has

2315
01:46:49,116 --> 01:46:52,536
terminado. Necesitas un producto real que
enfrente todos estos desafíos al chocar

2316
01:46:52,536 --> 01:46:55,614
con la realidad y todos esos distintos
patrones de conducta que requieren

2317
01:46:55,614 --> 01:46:58,948
corrección. Y creo que veremos todo eso
desarrollarse. Es una marcha de nueves.

2318
01:46:58,948 --> 01:46:59,974
Cada nueve es constante.

2319
01:47:00,216 --> 01:47:03,141
Eh, las demos animan. Aún queda muchísimo
trabajo por hacer.

2320
01:47:03,476 --> 01:47:07,114
Eh, creo que es un, eh, una especie de
dominio de seguridad crítico, a menos que

2321
01:47:07,114 --> 01:47:10,798
estés haciendo codificación de bytes, lo
cual es todo muy bonito y divertido y, y

2322
01:47:10,798 --> 01:47:14,575
así. Y, eh, por eso creo que esto también
reforzó mis plazos desde esa perspectiva.

2323
01:47:14,616 --> 01:47:18,775
Mm. Es muy interesante oírte decir que el
tipo de garantías de seguridad que se

2324
01:47:18,775 --> 01:47:22,935
necesitan del software en realidad no son
tan diferentes a las de la conducción

2325
01:47:22,935 --> 01:47:27,309
autónoma, porque... lo que la gente suele
decir es que la conducción autónoma tardó

2326
01:47:27,309 --> 01:47:29,708
tanto porque el coste de un fallo es tan
alto.

2327
01:47:23,195 --> 01:47:23,380
Sí

2328
01:47:29,716 --> 01:47:30,087
Ajá.

2329
01:47:30,096 --> 01:47:33,846
Un humano comete un error en promedio
cada 400.000 millas o cada siete años. Y

2330
01:47:33,846 --> 01:47:37,402
si tuvieras que lanzar un agente de
codificación que no pudiera cometer un

2331
01:47:37,402 --> 01:47:41,006
error por al menos siete años, sería
mucho más difícil de implementar. Pero

2332
01:47:41,006 --> 01:47:44,806
supongo que tu punto es que si cometieras
un error de codificación catastrófico

2333
01:47:44,806 --> 01:47:45,049
como-

2334
01:47:45,036 --> 01:47:45,361
Ajá

2335
01:47:45,636 --> 01:47:47,818
rompiendo un sistema importante cada
siete años-

2336
01:47:47,836 --> 01:47:48,579
Es muy sencillo.

2337
01:47:48,576 --> 01:47:51,970
Y de hecho, en términos de tiempo real,
sería mucho menos de siete años porque

2338
01:47:51,970 --> 01:47:55,365
estás, como, constantemente generando
código así, ¿verdad? Así que es como por

2339
01:47:55,365 --> 01:47:58,716
tokens, tú... O en términos de tokens,
serían siete años, pero en términos de

2340
01:47:58,716 --> 01:47:59,907
tiempo real, sería bastante

2341
01:47:59,896 --> 01:48:03,107
Sí, en cierto modo, es un problema mucho
más difícil. Quiero decir, la conducción

2342
01:48:03,107 --> 01:48:06,158
autónoma es solo una de las miles de
cosas que la gente hace. Es casi como un

2343
01:48:06,158 --> 01:48:09,129
solo nicho, supongo. Mientras que cuando
hablamos de ingeniería de software

2344
01:48:09,129 --> 01:48:10,855
general, es aún más... tiene más
superficie.

2345
01:48:11,156 --> 01:48:14,782
Hay otra, eh... objeción que la gente
suele presentar con respecto a esa

2346
01:48:14,782 --> 01:48:15,242
analogía.

2347
01:48:15,260 --> 01:48:15,631
Ajá.

2348
01:48:15,660 --> 01:48:19,441
Lo que pasa es que, con la conducción
automática, lo que ocupaba una gran parte

2349
01:48:19,441 --> 01:48:20,071
de ese tiempo

2350
01:48:20,620 --> 01:48:24,010
Estaba resolviendo el problema de
construir una percepción básica, eh, de

2351
01:48:24,010 --> 01:48:27,589
tener una percepción básica que sea
robusta, y de construir representaciones,

2352
01:48:27,589 --> 01:48:31,215
y de tener un modelo que posea cierto
sentido común para que pueda generalizar

2353
01:48:31,215 --> 01:48:34,087
cuando vea algo que está ligeramente
fuera de la distribución.

2354
01:48:34,100 --> 01:48:37,346
Si alguien está saludando, por ejemplo,
por la carretera de esta manera, no

2355
01:48:37,346 --> 01:48:40,856
necesitas entrenar para ello. El sistema,
uh, tendrá cierta capacidad de entender

2356
01:48:40,856 --> 01:48:42,041
cómo reaccionar a algo así.

2357
01:48:42,060 --> 01:48:42,617
Ajá.

2358
01:48:42,640 --> 01:48:46,048
Y estas son cosas que estamos obteniendo
gratuitamente con los LLM o VLM hoy. Por

2359
01:48:46,048 --> 01:48:48,945
tanto, no sabemos cómo resolver estos
problemas de representación tan

2360
01:48:48,945 --> 01:48:52,268
fundamentales. Y ahora, desplegar IAs en
diferentes dominios será algo así como

2361
01:48:52,268 --> 01:48:55,505
desplegar un coche autónomo con los
modelos actuales en una ciudad diferente,

2362
01:48:55,505 --> 01:48:58,104
lo cual es difícil, pero no como una
tarea que dure diez años.

2363
01:48:58,080 --> 01:49:01,029
Mm-hmm. Sí, básicamente, no estoy del
todo seguro de si estoy completamente de

2364
01:49:01,029 --> 01:49:03,902
acuerdo con eso. No sé cuánto estamos
obteniendo gratis, y sigo pensando que

2365
01:49:03,902 --> 01:49:06,392
hay muchas lagunas en la comprensión de
lo que estamos obteniendo.

2366
01:49:06,740 --> 01:49:09,746
Bueno, quiero decir, estamos obteniendo
definitivamente una inteligencia más

2367
01:49:09,746 --> 01:49:12,633
generalizable en una sola entidad.
Mientras que la conducción autónoma es

2368
01:49:12,633 --> 01:49:15,880
una tarea de propósito muy específico que
requiere... En cierto sentido, construir

2369
01:49:15,880 --> 01:49:18,647
una tarea de propósito específico es
quizás incluso más difícil en una

2370
01:49:18,647 --> 01:49:21,854
circunstancia porque no surge de algo más
general que estás haciendo a escala, si

2371
01:49:21,854 --> 01:49:22,576
eso tiene sentido.

2372
01:49:22,620 --> 01:49:26,701
Así que, eh, pero sigo pensando que la
analogía no, aún no sé si resuena del

2373
01:49:26,701 --> 01:49:30,782
todo porque, eh, como los LLM son aún
bastante falibles, y sigo pensando que

2374
01:49:30,782 --> 01:49:33,394
tienen muchas lagunas y que aún hay que
rellenar.

2375
01:49:33,440 --> 01:49:37,076
Y no creo que estemos obteniendo una
generalización mágica, por así decirlo,

2376
01:49:37,076 --> 01:49:40,955
completamente de la nada, uh, en cierto
sentido. Y el otro aspecto al que quería,

2377
01:49:40,955 --> 01:49:44,689
de hecho, volver cuando estaba, uh, al
principio, era que los coches autónomos

2378
01:49:44,689 --> 01:49:46,628
todavía no están ni de lejos terminados.

2379
01:49:47,100 --> 01:49:47,285
Mmm.

2380
01:49:47,560 --> 01:49:50,352
Eh, entonces, aunque... Eh, entonces, los
despliegues siguen siendo bastante

2381
01:49:50,352 --> 01:49:53,070
mínimos, ¿verdad? Eh, entonces, incluso
Waymo y demás, eh, tiene muy pocos

2382
01:49:53,070 --> 01:49:55,714
coches, y lo están haciendo, a grandes
rasgos, porque no son económicos,

2383
01:49:55,714 --> 01:49:56,012
¿verdad?

2384
01:49:56,180 --> 01:50:00,043
Pues, porque construyeron algo que, eh,
que vive en el futuro. Eh, y entonces

2385
01:50:00,043 --> 01:50:03,703
tuvieron que retrasar el futuro, pero
tuvieron que hacerlo antieconómico.

2386
01:50:03,740 --> 01:50:07,535
Así que tienen todos estos, eh, ya sabes,
hay todos estos costos, no solo los

2387
01:50:07,535 --> 01:50:11,580
marginales de esos coches y su operación
y mantenimiento, sino también el CapEx de

2388
01:50:11,580 --> 01:50:15,126
todo el sistema. Eh, así que hacerlo
económico seguirá siendo un trabajo

2389
01:50:15,126 --> 01:50:16,325
arduo, creo, para ellos.

2390
01:50:16,400 --> 01:50:19,792
Y luego, también, pienso que cuando uno
observa estos vehículos, y no hay

2391
01:50:19,792 --> 01:50:23,138
absolutamente nadie al volante, eh,
también me resulta un tanto engañoso

2392
01:50:23,138 --> 01:50:26,059
porque existen centros de teleoperación
sumamente sofisticados-

2393
01:50:26,080 --> 01:50:26,265
Mm

2394
01:50:26,560 --> 01:50:30,377
de personas realmente involucradas con
estos coches. Y no tengo la, no conozco

2395
01:50:30,377 --> 01:50:34,194
el alcance completo, pero creo que, um,
hay más humanos involucrados de lo que

2396
01:50:34,194 --> 01:50:38,011
esperarían, y hay gente en algún lugar,
uh, básicamente, uh, llegando desde el

2397
01:50:38,011 --> 01:50:38,309
cielo.

2398
01:50:38,380 --> 01:50:41,640
Y, eh, no sé si están completamente al
tanto de la conducción. Creo que a veces

2399
01:50:41,640 --> 01:50:45,025
sí, pero ciertamente están involucrados y
hay gente. Y en cierto sentido, no hemos

2400
01:50:45,025 --> 01:50:45,903
quitado a la persona.

2401
01:50:45,920 --> 01:50:48,984
Los hemos trasladado, eh, a un lugar
donde no los vemos. Todavía creo que

2402
01:50:48,984 --> 01:50:52,176
quedará trabajo, como mencionaste, al
pasar de un entorno a otro. Y, eh, así

2403
01:50:52,176 --> 01:50:55,325
que creo que todavía hay desafíos para
hacer que la conducción autónoma sea

2404
01:50:55,325 --> 01:50:58,432
real. Pero sí estoy de acuerdo en que
definitivamente ha cruzado el umbral

2405
01:50:58,432 --> 01:51:01,198
donde se siente como real, a menos que
esté realmente teleoperado.

2406
01:51:01,640 --> 01:51:04,958
Eh, por ejemplo, Waymo no puede ir a
todas las diferentes partes de la ciudad.

2407
01:51:04,958 --> 01:51:08,234
Eh, mi sospecha es que son como partes de
la ciudad donde no hay buena señal.

2408
01:51:08,680 --> 01:51:09,144
¿Ah, sí?

2409
01:51:09,600 --> 01:51:13,238
En fin, básicamente, realmente no sé nada
sobre la pila. Quiero decir, solo estoy

2410
01:51:13,238 --> 01:51:14,011
inventando cosas.

2411
01:51:14,880 --> 01:51:17,387
De verdad, dejaste la conducción autónoma
por cinco años en Tesla.

2412
01:51:17,900 --> 01:51:21,369
Lo siento, olvidé mencionar los detalles
de Waymo. De hecho, por cierto, me

2413
01:51:21,369 --> 01:51:22,822
encanta Waymo y lo uso siempre.

2414
01:51:22,840 --> 01:51:23,118
Claro.

2415
01:51:23,160 --> 01:51:26,511
Así que no quiero decir que... Solo creo
que la gente, de nuevo, a veces es un

2416
01:51:26,511 --> 01:51:29,776
poco demasiado ingenua sobre parte del
progreso, y sigo pensando que hay una

2417
01:51:29,776 --> 01:51:33,127
enorme cantidad de trabajo. Y creo que
Tesla adoptó, en mi opinión, un enfoque

2418
01:51:33,127 --> 01:51:36,697
mucho más escalable. Y creo que el equipo
lo está haciendo extremadamente bien y va

2419
01:51:36,697 --> 01:51:37,045
a, eh...

2420
01:51:24,540 --> 01:51:24,957
De acuerdo.

2421
01:51:33,860 --> 01:51:34,045
Sí.

2422
01:51:37,080 --> 01:51:40,502
Y yo, yo, yo estoy como en el historial
de haber predicho cómo irá esto, que es

2423
01:51:40,502 --> 01:51:44,013
que Waymo tuvo una ventaja inicial porque
puedes empaquetar muchos sensores. Pero

2424
01:51:44,013 --> 01:51:47,172
sí creo que Tesla está tomando la
estrategia más, eh, escalable y se va a

2425
01:51:47,172 --> 01:51:48,225
parecer mucho más a eso.

2426
01:51:48,320 --> 01:51:51,706
Eh, entonces, creo que esto todavía
tendrá que resolverse, y no lo ha hecho.

2427
01:51:51,706 --> 01:51:55,229
Pero básicamente, no quiero hablar de la
conducción autónoma como algo que tomó

2428
01:51:55,229 --> 01:51:56,493
una década, porque no tardó.

2429
01:51:56,680 --> 01:51:59,559
Todavía no ha surtido el efecto deseado.
Si es que tiene sentido.

2430
01:51:59,580 --> 01:52:03,943
P- porque, uno, el inicio es en 1980, no
hace diez años. Y luego, dos, el final no

2431
01:52:03,943 --> 01:52:04,967
ha llegado todavía.

2432
01:52:05,000 --> 01:52:07,966
Sí. El final no está cerca todavía.
Porque, cuando hablamos de conducción

2433
01:52:07,966 --> 01:52:10,974
autónoma, normalmente en mi mente es
conducción autónoma a gran escala. La

2434
01:52:10,974 --> 01:52:13,405
gente no tiene que sacar una licencia de
conducir, etcétera.

2435
01:52:10,420 --> 01:52:10,652
Sí.

2436
01:52:13,540 --> 01:52:18,010
Yo, yo tengo curiosidad de plantear otras
dos maneras en que la analogía podría ser

2437
01:52:18,010 --> 01:52:18,555
diferente.

2438
01:52:18,740 --> 01:52:19,111
Ajá.

2439
01:52:19,100 --> 01:52:23,072
Y la razón por la que me interesa esto es
porque creo que la pregunta de cuán

2440
01:52:23,072 --> 01:52:26,627
rápido se despliega la IA, cuán valiosa
es en sus primeras etapas, es

2441
01:52:26,627 --> 01:52:29,920
potencialmente la más importante del
mundo ahora mismo, ¿verdad?

2442
01:52:29,940 --> 01:52:33,948
Si, por ejemplo, estás intentando modelar
cómo lucirá el año 2030, esta es la

2443
01:52:33,948 --> 01:52:37,797
pregunta que debes comprender bien.
Entonces, otra cosa que podrías pensar

2444
01:52:37,797 --> 01:52:41,911
es, primero, tienes este requisito de
latencia con la conducción autónoma donde

2445
01:52:41,911 --> 01:52:42,385
tienes...

2446
01:52:42,420 --> 01:52:45,228
No tengo ni idea de cuáles son los
modelos reales, pero asumo que tienen

2447
01:52:45,228 --> 01:52:48,313
como decenas de millones de parámetros o
algo así, lo cual no es la restricción

2448
01:52:48,313 --> 01:52:51,477
necesaria para el trabajo de conocimiento
con los LLMs. O quizás sí lo sea con el

2449
01:52:51,477 --> 01:52:52,822
uso de computadoras y otras cosas.

2450
01:52:52,820 --> 01:52:56,589
Pero bueno, de todas formas, el otro
punto importante, y quizás este sea aún

2451
01:52:56,589 --> 01:53:00,610
más relevante, en lo que respecta a esta
cuestión de la inversión de capital, sí,

2452
01:53:00,610 --> 01:53:04,329
por supuesto, hay un costo adicional al
desplegar una copia adicional de un

2453
01:53:04,329 --> 01:53:08,049
modelo, pero el gasto operativo que
conlleva una sesión, el tipo de OpEx de

2454
01:53:08,049 --> 01:53:08,702
una sesión...

2455
01:53:08,740 --> 01:53:08,879
Mm-hm

2456
01:53:09,380 --> 01:53:13,143
es bastante bajo. Y puedes amortizar el
costo de la IA en la propia ejecución del

2457
01:53:13,143 --> 01:53:16,623
entrenamiento, dependiendo de cómo vaya
la escalabilidad de la inferencia y

2458
01:53:16,623 --> 01:53:20,339
demás. Pero ciertamente no es tanto como
construir un coche completamente nuevo-

2459
01:53:20,660 --> 01:53:20,752
Mm-hm

2460
01:53:20,840 --> 01:53:24,082
para servir a otra instancia de un
modelo. Así que, simplemente, los

2461
01:53:24,082 --> 01:53:26,598
aspectos económicos de desplegarlo más
ampliamente...

2462
01:53:26,640 --> 01:53:26,825
Mmm

2463
01:53:27,180 --> 01:53:28,573
resultan mucho más favorables.

2464
01:53:28,580 --> 01:53:31,290
Creo que sí. [pause] Creo que si te
quedas en el mundo de los bits, [pause]

2465
01:53:31,290 --> 01:53:34,001
los bits son un millón de veces más
fáciles que cualquier cosa que toque el

2466
01:53:34,001 --> 01:53:34,477
mundo físico.

2467
01:53:35,000 --> 01:53:35,464
Así es.

2468
01:53:35,700 --> 01:53:39,683
Eh, definitivamente lo concedo. Eh, los
bits son completamente cambiables,

2469
01:53:39,683 --> 01:53:43,556
arbitrariamente reordenables a una
velocidad muy rápida. Eh, así que uno

2470
01:53:43,556 --> 01:53:47,812
esperaría mucha más... una adaptación más
rápida, eh, también en la industria y

2471
01:53:47,812 --> 01:53:50,049
demás. Y luego, eh, ¿cuál era la primera?

2472
01:53:44,160 --> 01:53:44,299
Sí

2473
01:53:50,420 --> 01:53:53,392
Los requisitos de latencia y- ... ¿qué
implica para el tamaño del modelo?

2474
01:53:51,520 --> 01:53:51,984
Oh, ahí

2475
01:53:53,420 --> 01:53:56,286
Creo que eso es más o menos correcto.
Quiero decir, también creo que, si

2476
01:53:56,286 --> 01:53:59,435
hablamos de trabajo intelectual a gran
escala, habrá ciertos, eh, requisitos de

2477
01:53:59,435 --> 01:54:02,504
latencia... en la práctica, porque, eh,
sabes, tendremos que generar una gran

2478
01:54:02,504 --> 01:54:03,311
cantidad de cómputo-

2479
01:54:03,340 --> 01:54:03,479
Sí

2480
01:54:03,640 --> 01:54:07,390
Y que sirva para eso. [pause] Y luego,
creo que el último aspecto del que quiero

2481
01:54:07,390 --> 01:54:10,048
hablar muy brevemente también es,
digamos, todo lo demás.

2482
01:54:10,200 --> 01:54:11,175
Ajá.

2483
01:54:11,140 --> 01:54:14,959
El... Todo lo demás. Entonces, eh, ¿qué
piensa la sociedad al respecto? ¿Cuáles

2484
01:54:14,959 --> 01:54:18,877
son las ramificaciones... o cómo funciona
legalmente? ¿Cómo funciona en cuanto al

2485
01:54:18,877 --> 01:54:19,220
seguro?

2486
01:54:19,300 --> 01:54:22,547
¿Quién es en realidad...? O sea, ¿cuál es
la cuestión...? ¿dónde, dónde están esas

2487
01:54:22,547 --> 01:54:25,394
capas de la situación, eh, y sus
aspectos? ¿Qué pasa con...? ¿Cuál es el

2488
01:54:25,394 --> 01:54:27,519
equivalente de que la gente ponga un cono
en un Waymo?

2489
01:54:27,527 --> 01:54:27,944
Así es.

2490
01:54:27,947 --> 01:54:31,148
¿Sabes? Uh, va a haber equivalentes de
todo eso. Y por eso creo que... casi

2491
01:54:31,148 --> 01:54:34,524
siento que la conducción autónoma es una
analogía muy buena de la que se pueden

2492
01:54:34,524 --> 01:54:37,769
tomar cosas. Sí, ¿cuál es el equivalente
de un cono en un coche? ¿Cuál es el

2493
01:54:37,769 --> 01:54:40,625
equivalente de un trabajador teleoperado
que está, como, escondido?

2494
01:54:40,727 --> 01:54:40,912
Mmm.

2495
01:54:41,287 --> 01:54:44,677
Y, este, casi como todos y cada uno de
los aspectos que lo conforman.

2496
01:54:44,707 --> 01:54:48,433
Sí. ¿Tienes alguna opinión sobre si esto
implica que el actual despliegue de

2497
01:54:48,433 --> 01:54:51,662
inteligencia artificial, que
multiplicaría por diez la cantidad de

2498
01:54:51,662 --> 01:54:55,339
capacidad de cómputo disponible en el
mundo en uno o dos años, y... quizás,

2499
01:54:55,339 --> 01:54:58,221
como cien, más de cien veces para el
final de... la década?

2500
01:54:58,447 --> 01:55:01,532
Si el uso de la IA será menor- ... de lo
que algunas personas ingenuamente

2501
01:55:01,532 --> 01:55:04,703
predicen- ... ¿significa eso que estamos
construyendo demasiada capacidad de

2502
01:55:04,703 --> 01:55:05,041
cómputo?

2503
01:55:05,108 --> 01:55:06,965
¿O es que... ¿es una cuestión diferente?

2504
01:55:06,967 --> 01:55:09,242
Sí. Como lo que pasó con los
ferrocarriles y demás.

2505
01:55:09,247 --> 01:55:10,036
¿Con qué? Lo siento.

2506
01:55:10,048 --> 01:55:11,719
¿Fueron los ferrocarriles? Lo siento.
Fue, eh-

2507
01:55:11,747 --> 01:55:12,536
Sí, así es.

2508
01:55:12,527 --> 01:55:15,342
Sí. Hay, como, un precedente histórico,
¿no? ¿O fue con la industria de las

2509
01:55:15,342 --> 01:55:18,081
telecomunicaciones... verdad? Como,
allanando el camino para internet que

2510
01:55:18,081 --> 01:55:20,935
llegó, como, una década después...
¿sabes? Y creando, como, toda una burbuja

2511
01:55:20,935 --> 01:55:22,000
en las telecomunicaciones...

2512
01:55:22,027 --> 01:55:22,677
De acuerdo

2513
01:55:22,727 --> 01:55:26,691
La industria a finales de los noventa,
algo por el estilo. Sí. Así que no lo sé.

2514
01:55:26,691 --> 01:55:30,203
Quiero decir, entiendo que estoy sonando
muy pesimista en este momento.

2515
01:55:30,187 --> 01:55:30,790
Mmm.

2516
01:55:30,848 --> 01:55:34,023
Solo estoy haciendo eso... En realidad,
soy optimista. Creo que esto funcionará.

2517
01:55:34,023 --> 01:55:36,756
Creo que es factible. Solo parezco
pesimista porque cuando entro a mi

2518
01:55:36,756 --> 01:55:39,810
timeline de Twitter- ... Veo todas estas
cosas que no tienen sentido para mí.

2519
01:55:40,308 --> 01:55:43,973
Y, eh, y creo que hay muchas razones por
las que eso existe, y creo que gran parte

2520
01:55:43,973 --> 01:55:47,276
de ello es, honestamente, solo una
cuestión de cómo se plantean las cosas.

2521
01:55:47,276 --> 01:55:50,353
Son simplemente estructuras de
incentivos. Mucho de ello puede ser la

2522
01:55:50,353 --> 01:55:53,973
recaudación de fondos. Gran parte es solo
la atención, eh, ya sabes, convertir la

2523
01:55:53,973 --> 01:55:56,190
atención en dinero en internet, ¿sabes?
Cosas así.

2524
01:55:57,168 --> 01:56:00,575
Pues, creo que hay, eh, mucho de eso
sucediendo, y creo que solo estoy

2525
01:56:00,575 --> 01:56:04,180
reaccionando a eso. Eh, pero sigo siendo,
en general, muy optimista con la

2526
01:56:04,180 --> 01:56:07,834
tecnología. Creo que vamos a superar
todas estas cosas, y creo que ahora ha

2527
01:56:07,834 --> 01:56:11,340
habido un progreso muy rápido. Eh, en
realidad no sé si hay un exceso de

2528
01:56:11,340 --> 01:56:11,982
construcción.

2529
01:56:12,068 --> 01:56:15,146
Creo que va a haber... vamos a poder
asimilar y aprovechar lo que, según mi

2530
01:56:15,146 --> 01:56:18,307
entendimiento, se está construyendo.
Porque, de hecho, creo que, por ejemplo,

2531
01:56:18,307 --> 01:56:21,677
Cloud Code o OpenAI Codex y cosas por el
estilo, ni siquiera existían hace un año,

2532
01:56:21,677 --> 01:56:22,842
¿no es cierto? ¿Es correcto?

2533
01:56:22,908 --> 01:56:26,385
Creo que es más o menos acertado. Eh,
esta es una tecnología milagrosa que

2534
01:56:26,385 --> 01:56:30,148
antes no existía. Yo creo que va a haber
una cantidad enorme de demanda, como ya

2535
01:56:30,148 --> 01:56:34,054
estamos viendo la demanda en ChatGPT y en
otras cosas. Así que, sí, no estoy seguro

2536
01:56:34,054 --> 01:56:35,864
de que haya un exceso de construcción.

2537
01:56:36,027 --> 01:56:39,390
Eh, pero supongo que solo estoy
reaccionando a, como, algunos de los

2538
01:56:39,390 --> 01:56:43,156
plazos muy rápidos que la gente sigue
diciendo incorrectamente. Y lo he oído

2539
01:56:43,156 --> 01:56:47,222
muchas, muchas veces a lo largo de mis 15
años en IA, donde personas muy reputadas

2540
01:56:47,222 --> 01:56:49,030
siguen equivocándose todo el tiempo.

2541
01:56:50,588 --> 01:56:53,279
Y creo que quiero que estemos
correctamente calibrados, y creo que

2542
01:56:53,279 --> 01:56:56,218
parte de esto también, tiene, por
ejemplo, ramificaciones geopolíticas y

2543
01:56:56,218 --> 01:56:57,461
cosas por el estilo, cuando...

2544
01:56:57,527 --> 01:57:01,244
Eh, como algunas de estas preguntas. Y
creo que, eh, no me gustaría que la gente

2545
01:57:01,244 --> 01:57:04,773
cometiese errores en esa, en esa esfera
de cosas. Así que, eh, sí quiero que

2546
01:57:04,773 --> 01:57:08,208
estemos anclados en la realidad de lo que
la tecnología es y lo que no es.

2547
01:57:08,608 --> 01:57:10,976
Hablemos de educación en Eureka y todo
eso.

2548
01:57:10,967 --> 01:57:11,384
Ajá.

2549
01:57:11,648 --> 01:57:14,576
Una cosa que podrías hacer es, eh,
establecer otro laboratorio de

2550
01:57:14,576 --> 01:57:18,145
inteligencia artificial... y, eh, tratar
de solucionar esos problemas. Mmm, sí,

2551
01:57:18,145 --> 01:57:21,623
tengo curiosidad por saber qué estás
haciendo ahora. Y luego, sí, ¿por qué no

2552
01:57:21,623 --> 01:57:24,140
la investigación en inteligencia
artificial en sí misma?

2553
01:57:21,168 --> 01:57:21,400
Sí.

2554
01:57:24,687 --> 01:57:28,291
Eh, supongo que, la forma en que lo
expresaría es que siento un cierto

2555
01:57:28,291 --> 01:57:32,210
determinismo en torno a las cosas que
están haciendo los laboratorios de IA.

2556
01:57:33,108 --> 01:57:33,200
Ajá.

2557
01:57:33,288 --> 01:57:37,119
Uhm, y siento que podría ser de ayuda
allí, pero no sé si yo, uh, de forma

2558
01:57:37,119 --> 01:57:40,846
única, um... no sé si yo, uh, lo
mejoraría de manera singular. Pero creo

2559
01:57:40,846 --> 01:57:44,993
que mi gran temor personal es que muchas
de estas cosas suceden en detrimento de

2560
01:57:44,993 --> 01:57:48,195
la humanidad, y que la humanidad se ve
desempoderada por ello.

2561
01:57:48,227 --> 01:57:48,459
Mmm.

2562
01:57:48,487 --> 01:57:53,045
Y a mí, me importa no solo las esferas de
Dyson que construiremos, y las que la IA

2563
01:57:53,045 --> 01:57:56,985
construirá de forma autónoma. Me importa
lo que les pase a los humanos.

2564
01:57:57,027 --> 01:57:57,444
Así es.

2565
01:57:57,747 --> 01:58:01,555
Y quiero que los seres humanos estén bien
en este futuro, y siento que es ahí donde

2566
01:58:01,555 --> 01:58:05,130
puedo aportar un valor mucho más único
que, eh, como una mejora incremental en

2567
01:58:05,130 --> 01:58:08,706
un laboratorio de vanguardia. Y así, eh,
supongo que a lo que más le temo es a

2568
01:58:08,706 --> 01:58:11,864
algo quizás como, eh, lo que se
representa en películas como WALL-E o

2569
01:58:11,864 --> 01:58:12,839
Idiocracy o algo así.

2570
01:58:12,888 --> 01:58:16,888
donde la humanidad está un poco al margen
de todo este asunto. Pues, y quiero que

2571
01:58:16,888 --> 01:58:20,389
los seres humanos sean mucho, mucho
mejores en este futuro. Y entonces,

2572
01:58:20,389 --> 01:58:24,140
supongo, para mí, esto es como a través
de la educación que realmente puedes

2573
01:58:24,140 --> 01:58:24,590
lograrlo.

2574
01:58:19,687 --> 01:58:19,872
Mmm.

2575
01:58:24,628 --> 01:58:26,949
Y, pues, eh, ¿en qué estás trabajando en
este momento?

2576
01:58:26,928 --> 01:58:30,197
Oh, sí. Eureka está intentando
construir... Creo que la forma más fácil

2577
01:58:30,197 --> 01:58:33,280
de describirlo es que estamos
construyendo una Academia de la Flota

2578
01:58:33,280 --> 01:58:34,869
Estelar. No sé si viste Star Trek.

2579
01:58:34,888 --> 01:58:35,863
No lo he, pero bueno.

2580
01:58:35,868 --> 01:58:39,266
Vale. Vale, la Academia de la Flota
Estelar es como esta institución de élite

2581
01:58:39,266 --> 01:58:42,619
para tecnología puntera... construir
naves espaciales y graduar cadetes para

2582
01:58:42,619 --> 01:58:45,481
que sean, sabes, los pilotos de las naves
espaciales y cosas así.

2583
01:58:41,047 --> 01:58:41,186
Mmm

2584
01:58:45,687 --> 01:58:50,494
Así que, me imagino, como, una
institución de élite para el conocimiento

2585
01:58:50,494 --> 01:58:55,302
técnico y, básicamente, un tipo de
escuela que está muy actualizada y de

2586
01:58:55,302 --> 01:58:56,182
primer nivel.

2587
01:58:56,148 --> 01:59:02,605
Mmm. Una de las preguntas que tengo para
ti es, uhm, cómo se enseña el contenido

2588
01:59:02,605 --> 01:59:04,321
técnico o científico.

2589
01:59:04,507 --> 01:59:04,924
A ver, mmm

2590
01:59:05,368 --> 01:59:08,741
Bueno, porque eres uno de los grandes
maestros mundiales en esto. Y me da

2591
01:59:08,741 --> 01:59:11,881
curiosidad cómo lo manejas para el
contenido que ya has publicado en

2592
01:59:11,881 --> 01:59:15,442
YouTube, [pause] pero también, si es que
es distinto, cómo lo aplicas para tu

2593
01:59:15,442 --> 01:59:15,770
Eureka.

2594
01:59:12,527 --> 01:59:12,666
Mmm. Sí.

2595
01:59:16,108 --> 01:59:19,879
Sí, sí. Con respecto a Eureka, creo que
algo que me resulta muy fascinante de la

2596
01:59:19,879 --> 01:59:23,603
educación es que la educación cambiará de
forma bastante fundamental con las IA

2597
01:59:23,603 --> 01:59:27,135
como apoyo. Y creo que tendrá que ser,
eh, reestructurada y modificada, eh,

2598
01:59:27,135 --> 01:59:28,043
hasta cierto punto.

2599
01:59:28,507 --> 01:59:31,648
Todavía pienso que estamos en una etapa
bastante temprana. Creo que habrá mucha

2600
01:59:31,648 --> 01:59:34,829
gente que intentará hacer las cosas más
obvias, que es como, ah, tener un LLM y,

2601
01:59:34,829 --> 01:59:38,091
pues, hacerle preguntas y obtener, sabes,
hacer todas las cosas básicas que harías

2602
01:59:38,091 --> 01:59:39,420
con el prompting en este momento.

2603
01:59:39,547 --> 01:59:43,328
Creo que es útil, pero aún me parece un
poco chapucero, como algo mal hecho. Me

2604
01:59:43,328 --> 01:59:47,109
gustaría hacerlo correctamente y creo que
no existe la capacidad para lo que yo

2605
01:59:47,109 --> 01:59:50,599
desearía. Lo que yo querría es, eh, como
una experiencia de tutoría real.

2606
01:59:50,628 --> 01:59:54,089
Eh, quizás un ejemplo destacado en mi
mente es, eh, recientemente estaba

2607
01:59:54,089 --> 01:59:57,891
aprendiendo coreano. Así que, aprendizaje
de idiomas. Y pasé por una fase donde

2608
01:59:57,891 --> 02:00:00,426
estaba aprendiendo coreano por mi cuenta
en internet.

2609
01:59:55,848 --> 01:59:55,987
Ajá.

2610
02:00:00,447 --> 02:00:03,985
Pasé por una etapa en la que de hecho fui
parte de una clase pequeña, uh, en Corea-

2611
02:00:03,985 --> 02:00:07,048
uh, aprendiendo coreano con un montón de
otras personas, lo cual fue muy

2612
02:00:07,048 --> 02:00:10,328
divertido. Pero teníamos un profesor y,
como, diez personas o así aprendiendo

2613
02:00:10,328 --> 02:00:13,650
coreano. Y luego cambié a un profesor
particular. Y, um, supongo que lo que me

2614
02:00:13,650 --> 02:00:16,283
resultó fascinante es que, creo que tuve
un muy buen profesor.

2615
02:00:16,444 --> 02:00:20,746
Pero, uhm, o sea, si me pongo a pensar en
lo que esta tutora hacía por mí, y lo

2616
02:00:20,746 --> 02:00:24,331
increíble que resultó ser esa
experiencia... y lo alto que está el

2617
02:00:24,331 --> 02:00:28,523
listón para lo que de verdad quiero
construir a la larga. Uhm, porque, o sea,

2618
02:00:28,523 --> 02:00:29,958
ella era extremadamente...

2619
02:00:27,284 --> 02:00:27,701
Ajá.

2620
02:00:29,964 --> 02:00:33,499
Así que, ella al instante, con una
conversación muy breve, comprendió mi

2621
02:00:33,499 --> 02:00:37,482
nivel como estudiante, lo que sé y lo que
no. Y pudo indagar exactamente qué tipo

2622
02:00:37,482 --> 02:00:40,320
de preguntas o cosas hacer para entender
mi modelo mental.

2623
02:00:40,364 --> 02:00:40,874
Ajá.

2624
02:00:41,144 --> 02:00:45,061
Ningún LLM lo hará por ti, ahora mismo al
cien. Ni de cerca, ¿verdad? Pero una

2625
02:00:45,061 --> 02:00:48,928
tutora sí lo hará si, si es buena. Uh,
una vez que ella entiende, um, ella de

2626
02:00:48,928 --> 02:00:52,338
hecho, realmente me dio todo lo que
necesitaba en mi actual nivel de

2627
02:00:52,338 --> 02:00:52,846
capacidad.

2628
02:00:52,964 --> 02:00:56,083
Necesito que siempre me desafíen de
manera apropiada. No puedo encarar algo

2629
02:00:56,083 --> 02:00:59,371
que sea demasiado difícil... o demasiado
trivial. Y un tutor es realmente bueno

2630
02:00:59,371 --> 02:01:02,575
dándote justo lo que necesitas. Y así,
básicamente, sentí que yo era la única

2631
02:01:02,575 --> 02:01:04,388
limitación para aprender. Como, mi
propia...

2632
02:01:04,404 --> 02:01:08,013
Yo era la única limitación, siempre se me
dio la información perfecta. Soy la única

2633
02:01:08,013 --> 02:01:11,534
limitación. Y me sentí bien porque soy el
único impedimento que existe. No es que

2634
02:01:11,534 --> 02:01:15,012
no pueda encontrar conocimiento o que no
esté bien explicado o etcétera. Es solo

2635
02:01:15,012 --> 02:01:18,137
mi capacidad para memorizar y así
sucesivamente. Y esto es lo que quiero

2636
02:01:18,137 --> 02:01:18,753
para la gente.

2637
02:01:07,304 --> 02:01:07,443
Mmm.

2638
02:01:18,744 --> 02:01:19,812
¿Cómo lo automatizas?

2639
02:01:20,344 --> 02:01:23,564
Así que, muy buena pregunta. Con la
capacidad actual, no lo haces. Pero sí

2640
02:01:23,564 --> 02:01:27,049
creo que con, eh, como, um... Y por eso
creo, um, que no es realmente el momento

2641
02:01:27,049 --> 02:01:30,535
adecuado, adecuado para construir este
tipo de tutor de IA. Sigo pensando que es

2642
02:01:30,535 --> 02:01:32,697
un producto útil, um, y mucha gente lo
construirá.

2643
02:01:33,024 --> 02:01:37,597
Pero sigo sintiendo que el listón está
tan alto, y la capacidad simplemente no

2644
02:01:37,597 --> 02:01:38,132
está ahí.

2645
02:01:38,584 --> 02:01:42,044
Eh, uh, pero quiero decir, incluso hoy
diría que ChatGPT es un producto

2646
02:01:42,044 --> 02:01:46,097
educativo [pausa] extremadamente valioso.
Pero creo que para mí, fue tan fascinante

2647
02:01:46,097 --> 02:01:49,952
ver lo alto que está el listón. Y cuando
estaba con ella, casi sentí que no hay

2648
02:01:49,952 --> 02:01:51,633
forma de que pueda construir esto.

2649
02:01:53,044 --> 02:01:54,158
Pero lo construyes, ¿no?

2650
02:01:54,144 --> 02:01:56,930
Cualquiera que haya tenido un muy buen
tutor, dice: "¿Cómo lo vas a construir?"

2651
02:01:58,184 --> 02:02:01,874
Eh, así que supongo que e- estoy
esperando esa capacidad. Yo, yo sí pienso

2652
02:02:01,874 --> 02:02:03,896
que de muchas maneras en la industria...

2653
02:02:04,024 --> 02:02:07,675
Por ejemplo, hice consultoría de IA para
visión artificial. Uhm, muchas veces, el

2654
02:02:07,675 --> 02:02:11,053
valor que aporté a la empresa fue
decirles que no usaran IA. No era como si

2655
02:02:11,053 --> 02:02:14,294
yo fuera el experto en IA, y me
describieron un problema, y yo dije: "No

2656
02:02:14,294 --> 02:02:14,705
usen IA".

2657
02:02:06,004 --> 02:02:06,282
Claro.

2658
02:02:15,724 --> 02:02:19,169
Esta fue mi aportación de valor. Y siento
que es lo mismo en la educación ahora

2659
02:02:19,169 --> 02:02:22,702
mismo, donde siento que para lo que tengo
en mente, aún no es el momento, pero el

2660
02:02:22,702 --> 02:02:26,103
momento llegará. Pero por ahora estoy
construyendo algo que funciona quizás de

2661
02:02:26,103 --> 02:02:29,548
forma un poco más convencional, que tiene
un componente físico y digital, y así

2662
02:02:29,548 --> 02:02:30,166
sucesivamente.

2663
02:02:23,804 --> 02:02:23,989
Mmm.

2664
02:02:30,304 --> 02:02:34,202
Pero creo que es obvio, es obvio... Es
obvio cómo debería verse esto en el

2665
02:02:34,202 --> 02:02:34,576
futuro.

2666
02:02:34,604 --> 02:02:38,267
Mm-hmm. En la medida en que estés
dispuesto a decirlo, ¿cuál es esa cosa

2667
02:02:38,267 --> 02:02:40,641
que esperas que se lance este año o el
próximo?

2668
02:02:40,884 --> 02:02:44,281
Bueno, pues estoy creando el primer
curso. Y quiero que sea un curso muy, muy

2669
02:02:44,281 --> 02:02:47,858
bueno. Un destino obvio y de vanguardia
al que vas para aprender IA en este caso.

2670
02:02:47,858 --> 02:02:51,122
Porque es justo con lo que estoy
familiarizado, así que creo que es un muy

2671
02:02:51,122 --> 02:02:54,430
buen primer producto para que llegue a
ser realmente bueno. Y eso es lo que

2672
02:02:54,430 --> 02:02:55,280
estoy construyendo.

2673
02:02:49,624 --> 02:02:49,856
Mmm.

2674
02:02:53,484 --> 02:02:53,623
Sí.

2675
02:02:55,324 --> 02:02:58,492
Y Nanochat, que mencionaste brevemente,
es el proyecto final- ... de, uh, LL.M.

2676
02:02:58,492 --> 02:03:01,335
101-N, que es una clase que estoy
construyendo. Así que, um, eso es una

2677
02:03:01,335 --> 02:03:04,463
parte realmente grande de ello, pero
ahora tengo que desarrollar muchos de los

2678
02:03:04,463 --> 02:03:07,753
intermedios, y luego tengo que, de hecho,
como, contratar un pequeño equipo de, ya

2679
02:03:07,753 --> 02:03:10,962
sabes, asistentes de cátedra y así- ...
y, de hecho, como, uh, construir todo el

2680
02:03:10,962 --> 02:03:11,206
curso.

2681
02:03:11,204 --> 02:03:14,928
Y quizás otra cosa que diría es que,
muchas veces cuando la gente piensa en la

2682
02:03:14,928 --> 02:03:18,702
educación, piensan en algo así como el
lado más... lo que yo diría es, como, un

2683
02:03:18,702 --> 02:03:21,652
componente más blando, como, de difundir
conocimiento o, eh...

2684
02:03:21,644 --> 02:03:24,865
Pero en realidad tengo algo sumamente
difícil y técnico en mente. Y en mi

2685
02:03:24,865 --> 02:03:27,996
opinión, la educación es, por así
decirlo, como ese proceso tan arduo y

2686
02:03:27,996 --> 02:03:30,188
técnico de construir rampas hacia el
conocimiento.

2687
02:03:30,264 --> 02:03:30,867
Ajá.

2688
02:03:31,084 --> 02:03:34,812
Así que, en mi opinión, Nanochat es una
rampa al conocimiento porque es muy

2689
02:03:34,812 --> 02:03:38,339
simple... Es, como, la pila completa
súper simplificada. Si le das este

2690
02:03:38,339 --> 02:03:41,765
artefacto a alguien y lo revisa, está
aprendiendo un montón de cosas.

2691
02:03:41,804 --> 02:03:42,407
Claro que sí.

2692
02:03:42,464 --> 02:03:46,034
Y así, eh, te está dando muchas de las
que yo llamo Eurekas por segundo. Es

2693
02:03:46,034 --> 02:03:49,894
decir, comprensión por segundo. Eso es lo
que quiero. Muchas Eurekas por segundo.

2694
02:03:46,064 --> 02:03:46,342
Claro.

2695
02:03:49,884 --> 02:03:50,348
Ajá.

2696
02:03:50,444 --> 02:03:53,624
Um, y para mí, este es un problema
técnico de cómo construimos estas rampas

2697
02:03:53,624 --> 02:03:56,891
hacia el conocimiento. Y, uh, siempre
pienso en Eureka como casi un... No es,

2698
02:03:56,891 --> 02:04:00,200
como, quizás tan diferente, tal vez a
través de algunos de los laboratorios de

2699
02:04:00,200 --> 02:04:02,564
frontera o parte del trabajo que se va a
estar haciendo.

2700
02:04:02,564 --> 02:04:05,875
Porque quiero averiguar cómo construir
estas rampas fronterizas, eh, estas

2701
02:04:05,875 --> 02:04:09,413
rampas de manera muy eficiente para que
la gente nunca se quede atascada, eh, y

2702
02:04:09,413 --> 02:04:12,996
que todo nunca sea demasiado difícil o
demasiado e- no demasiado trivial, y, eh,

2703
02:04:12,996 --> 02:04:16,217
no puedes... Tienes justo el material
adecuado para realmente progresar.

2704
02:04:16,384 --> 02:04:19,886
Sí. Entonces, te imaginas a corto plazo
que en lugar de que un tutor pueda,

2705
02:04:19,886 --> 02:04:23,293
digamos, sondear tu comprensión, si
tienes suficiente autoconciencia para

2706
02:04:23,293 --> 02:04:26,937
poder sondearte a ti mismo... no, nunca
te quedarás atascado. Puedes, digamos,

2707
02:04:26,937 --> 02:04:30,392
encontrar la respuesta correcta entre
hablar con el asistente de cátedra o

2708
02:04:30,392 --> 02:04:31,244
hablar con un LLM.

2709
02:04:25,563 --> 02:04:25,702
Mm-hm

2710
02:04:31,284 --> 02:04:32,862
y viendo la referencia de limitación.

2711
02:04:32,844 --> 02:04:33,540
Ajá.

2712
02:04:33,543 --> 02:04:38,141
Parece que, uh, la automatización o la
inteligencia artificial no es en realidad

2713
02:04:38,141 --> 02:04:42,798
tan significativa. Como que, hasta ahora,
en realidad, lo más importante, el gran

2714
02:04:42,798 --> 02:04:46,174
factor aquí es tu capacidad de explicar
la IA codificada...

2715
02:04:46,184 --> 02:04:46,323
Sí

2716
02:04:46,543 --> 02:04:50,393
En el material de origen de la clase,
¿verdad? Eso es fundamentalmente lo que

2717
02:04:50,393 --> 02:04:51,001
es el curso.

2718
02:04:51,024 --> 02:04:54,200
Quiero decir, creo que siempre tienes que
estar calibrado a la capacidad, qué

2719
02:04:54,200 --> 02:04:57,460
capacidad existe en la industria. Y creo
que mucha gente va a buscar, como, oh,

2720
02:04:57,460 --> 02:04:58,965
solo pregúntale a ChatGPT, etcétera.

2721
02:04:53,884 --> 02:04:53,976
Sí

2722
02:04:59,164 --> 02:05:02,907
Bueno, pero yo creo que, por ejemplo,
ahora mismo, si vas a ChatGPT y le dices,

2723
02:05:02,907 --> 02:05:06,362
"Oh, enséñame IA", no hay manera. Es
como, quiero decir, te va a dar pura

2724
02:05:06,362 --> 02:05:10,153
basura, ¿no? La IA nunca va a escribir
Nanochat ahora mismo. Pero Nanochat es un

2725
02:05:10,153 --> 02:05:12,120
punto realmente útil, creo yo,
intermedio.

2726
02:05:05,624 --> 02:05:06,041
Correcto.

2727
02:05:12,264 --> 02:05:15,917
Así que, sigo c- estoy colaborando con la
IA para crear todo este material. Sí,

2728
02:05:15,917 --> 02:05:17,743
sigue siendo fundamentalmente muy útil.

2729
02:05:18,144 --> 02:05:21,618
Eh, al principio, monté, eh, CS231 en
Stanford, que fue una de las primeras...

2730
02:05:21,618 --> 02:05:25,318
De hecho, perdón. Creo que fue la primera
clase de deep learning en Stanford que se

2731
02:05:25,318 --> 02:05:26,085
hizo muy popular.

2732
02:05:26,424 --> 02:05:32,043
Y la diferencia al desarrollar 231N y
LL.M. 101-N ahora es bastante marcada.

2733
02:05:32,484 --> 02:05:36,294
Porque me siento muy empoderado por los
LLM como existen ahora, pero estoy muy

2734
02:05:36,294 --> 02:05:39,264
involucrado. Así que, me ayudan a crear
todos los materiales.

2735
02:05:37,424 --> 02:05:37,656
Mmm.

2736
02:05:39,364 --> 02:05:43,062
Voy mucho más rápido. Uh, ellos hacen la
parte aburrida, etcétera. Uh, así que

2737
02:05:43,062 --> 02:05:46,952
siento que desarrollo el curso mucho más
rápido y tiene LLM integrado, pero aún no

2738
02:05:46,952 --> 02:05:50,458
está en un punto donde pueda crear el
contenido de forma creativa. Todavía

2739
02:05:50,458 --> 02:05:54,300
estoy ahí para hacerlo. Así que, creo que
lo complicado es siempre ajustarse a lo

2740
02:05:54,300 --> 02:05:54,828
que existe.

2741
02:05:55,024 --> 02:05:59,516
Y así, cuando uno imagina lo que estará
disponible a través de Eureka en un par

2742
02:05:59,516 --> 02:06:03,894
de años, parece que el gran cuello de
botella va a ser encontrar Karpathys en

2743
02:06:03,894 --> 02:06:07,753
un campo tras otro que puedan...
convertir su entendimiento en estas

2744
02:06:07,753 --> 02:06:08,445
rampas, ¿no?

2745
02:06:05,304 --> 02:06:05,489
Mmm

2746
02:06:08,464 --> 02:06:12,190
Sí. Así que, creo que cambiará con el
tiempo. Y ahora mismo, creo que sería,

2747
02:06:12,190 --> 02:06:13,433
eh, contratar profesores-

2748
02:06:13,424 --> 02:06:13,749
Ajá

2749
02:06:14,084 --> 02:06:18,512
para trabajar mano a mano con la IA y...
un equipo de personas, probablemente,

2750
02:06:18,512 --> 02:06:20,353
para crear cursos de vanguardia.

2751
02:06:20,360 --> 02:06:20,870
Así es.

2752
02:06:20,880 --> 02:06:24,319
Y luego, creo que con el tiempo, algunos
TAs podrían convertirse en IAs. Porque

2753
02:06:24,319 --> 02:06:27,714
algunos TAs, como, bueno, solo tomas
todos los materiales del curso y entonces

2754
02:06:27,714 --> 02:06:30,493
creo que podrías servir como un muy buen,
como, TA automatizado.

2755
02:06:30,500 --> 02:06:30,592
Sí

2756
02:06:31,100 --> 02:06:34,099
para los estudiantes cuando tienen
preguntas más básicas o algo así,

2757
02:06:34,099 --> 02:06:37,368
¿verdad? Pero creo que necesitarás
profesores para la arquitectura general

2758
02:06:37,368 --> 02:06:40,502
de un curso. Y asegurarte de que todo
encaje. Y así, veo una especie de

2759
02:06:40,502 --> 02:06:42,338
progresión de cómo todo esto
evolucionará.

2760
02:06:38,700 --> 02:06:38,839
Mm.

2761
02:06:42,360 --> 02:06:45,947
Y quizás en algún punto futuro, sabes, yo
ni siquiera sea tan útil y la IA haga la

2762
02:06:45,947 --> 02:06:49,446
mayor parte del diseño mucho mejor que
yo. Pero sigo pensando que eso tardará en

2763
02:06:49,446 --> 02:06:50,022
manifestarse.

2764
02:06:47,420 --> 02:06:47,745
Ajá.

2765
02:06:50,040 --> 02:06:53,762
Pero, ¿te estás imaginando que gente con
experiencia en otros campos está

2766
02:06:53,762 --> 02:06:55,055
contribuyendo con cursos?

2767
02:06:56,780 --> 02:07:01,670
¿O sientes que es realmente esencial para
la visión que tú, dada tu comprensión de

2768
02:07:01,670 --> 02:07:06,258
cómo quieres enseñar, seas quien diseñe
el contenido? Por ejemplo, no sé, Sal

2769
02:07:06,258 --> 02:07:10,967
Khan está narrando todos los videos en
Khan Academy. ¿Estás imaginando algo así

2770
02:07:10,967 --> 02:07:11,269
o...?

2771
02:07:09,140 --> 02:07:09,557
Claro, sí.

2772
02:07:11,300 --> 02:07:15,057
Oh no, contrataré profesores, creo,
porque hay áreas en las que no soy un

2773
02:07:15,057 --> 02:07:19,231
experto. Y creo que esa es la única forma
de ofrecer la experiencia de vanguardia

2774
02:07:19,231 --> 02:07:22,989
para... el estudiante, en última
instancia, así que, sí, espero contratar

2775
02:07:22,989 --> 02:07:26,067
profesores. Pero probablemente me quede
en IA por un tiempo.

2776
02:07:26,100 --> 02:07:26,378
Claro.

2777
02:07:26,420 --> 02:07:30,219
Eh, pero en... sí tengo algo que creo más
convencional en mente para la capacidad

2778
02:07:30,219 --> 02:07:32,689
actual, de lo que la gente probablemente
anticiparía.

2779
02:07:33,000 --> 02:07:35,925
Eh, y cuando estoy construyendo la
Academia de la Flota Estelar,

2780
02:07:35,925 --> 02:07:39,640
probablemente me imagino una institución
física, eh, y quizás un nivel por debajo

2781
02:07:39,640 --> 02:07:43,448
de eso, una oferta digital que, eh, no es
la ex- no es la experiencia de vanguardia

2782
02:07:43,448 --> 02:07:45,631
que obtendrías cuando alguien viene
físicamente.

2783
02:07:45,680 --> 02:07:45,819
Sí

2784
02:07:45,900 --> 02:07:48,574
a tiempo completo y trabajamos el
material de principio a fin y nos

2785
02:07:48,574 --> 02:07:51,572
aseguramos de que lo entiendas. Eh, esa
es la oferta física. Mmm, la oferta

2786
02:07:51,572 --> 02:07:54,692
digital es, sí, un montón de cosas en
internet y quizás algún asistente LLM, y

2787
02:07:54,692 --> 02:07:57,771
es un poco más llamativa y un nivel
inferior, pero, eh, al menos es accesible

2788
02:07:57,771 --> 02:07:59,878
para, como, ocho mil millones de
personas, así que...

2789
02:07:51,380 --> 02:07:51,612
Sí.

2790
02:07:59,860 --> 02:08:03,309
Mm. Sí, yo, yo, yo creo que lo que estás
haciendo es, en el fondo, inventar la

2791
02:08:03,309 --> 02:08:06,222
institución universitaria partiendo de
los principios más básicos.

2792
02:08:06,380 --> 02:08:10,195
para las herramientas que están
disponibles hoy y luego, simplemente,

2793
02:08:10,195 --> 02:08:14,178
seleccionar a aquellas personas que
tienen la motivación y el interés de

2794
02:08:14,178 --> 02:08:17,432
realmente, de verdad, involucrarse a
fondo con el material.

2795
02:08:17,440 --> 02:08:20,848
Sí. Y pienso que va a ser necesaria mucha
no solamente educación, sino también

2796
02:08:20,848 --> 02:08:23,105
reeducación, y me encantaría, eh, poder
ayudar allí.

2797
02:08:23,100 --> 02:08:23,425
Ajá.

2798
02:08:23,620 --> 02:08:26,718
Eh, porque creo que los trabajos
probablemente cambiarán bastante. Em, y,

2799
02:08:26,718 --> 02:08:29,946
por ejemplo, hoy en día, mucha gente está
intentando mejorar sus habilidades

2800
02:08:29,946 --> 02:08:33,174
específicamente en IA. así que creo que
es un muy buen curso para enseñar en

2801
02:08:33,174 --> 02:08:36,445
este, en este sentido. Em, y sí, creo
que, eh, en cuanto a la motivación, eh,

2802
02:08:36,445 --> 02:08:37,134
antes de la IAG,

2803
02:08:37,840 --> 02:08:41,245
eh, la motivación es muy simple en sí
misma porque, eh, la gente quiere ganar

2804
02:08:41,245 --> 02:08:44,696
dinero y así es como se gana dinero en la
industria hoy. Creo que post-AGI, es

2805
02:08:44,696 --> 02:08:47,743
mucho más interesante, um, pro-
posiblemente porque, sí, si todo está

2806
02:08:47,743 --> 02:08:51,104
automatizado y no hay nada que hacer para
nadie, ¿por qué iría alguien a una

2807
02:08:51,104 --> 02:08:51,911
escuela, etcétera?

2808
02:08:53,520 --> 02:08:57,502
Bueno, pues, yo creo que, uhm, supongo
que, a menudo digo que la educación

2809
02:08:57,502 --> 02:09:00,393
pre-IAG es útil, y la educación post-IAG
es divertida.

2810
02:09:01,400 --> 02:09:04,902
Y, eh, de manera similar a como la
gente... Por ejemplo, eh, la gente va al

2811
02:09:04,902 --> 02:09:07,931
gimnasio hoy día... eh, pero no
necesitamos su fuerza física para

2812
02:09:07,931 --> 02:09:11,245
manipular... eh, objetos pesados porque
tenemos máquinas que hacen eso.

2813
02:09:05,480 --> 02:09:05,805
Ajá

2814
02:09:11,420 --> 02:09:11,605
Sí.

2815
02:09:11,660 --> 02:09:14,776
Entonces todavía van al gimnasio. ¿Por
qué van al gimnasio? Bueno, porque es

2816
02:09:14,776 --> 02:09:17,975
divertido, es sano, es, uh, y es... y te
ves genial cuando tienes abdominales.

2817
02:09:18,000 --> 02:09:21,699
No lo sé. Supongo, como, eh, y entonces,
supongo que lo que estoy diciendo es, eh,

2818
02:09:21,699 --> 02:09:23,572
que es atractivo para la gente hacer eso.

2819
02:09:18,740 --> 02:09:18,879
Sí.

2820
02:09:24,060 --> 02:09:27,823
en, en un cierto, como, muy profundo
sentido psicológico, evolutivo para la

2821
02:09:27,823 --> 02:09:28,332
humanidad.

2822
02:09:28,360 --> 02:09:28,917
Claro que sí.

2823
02:09:28,940 --> 02:09:32,714
Y así, de alguna manera, creo que la
educación se desarrollará de la misma

2824
02:09:32,714 --> 02:09:36,436
forma. Irás a la escuela como vas al
gimnasio. Y creo que ahora mismo, no

2825
02:09:36,436 --> 02:09:40,262
mucha gente aprende, porque aprender es
difícil. Te rindes ante el material

2826
02:09:40,262 --> 02:09:44,450
porque... y algunas personas superan esa
barrera, pero para la mayoría es difícil.

2827
02:09:44,460 --> 02:09:44,692
Sí.

2828
02:09:44,720 --> 02:09:47,526
Pero yo sí creo que es un problema
técnico que debemos resolver. Es un

2829
02:09:47,526 --> 02:09:50,700
problema técnico el hacer lo que mi tutor
hizo por mí cuando estaba aprendiendo

2830
02:09:50,700 --> 02:09:53,873
coreano. Creo que es algo abordable y que
se puede construir, y alguien debería

2831
02:09:53,873 --> 02:09:56,883
hacerlo, y creo que esto hará que el
aprendizaje de cualquier cosa sea algo

2832
02:09:56,883 --> 02:09:59,812
trivial y deseable, y la gente lo hará
por pura diversión. Porque es algo

2833
02:09:59,812 --> 02:10:00,138
trivial.

2834
02:10:00,600 --> 02:10:03,629
Si tuviera un tutor así para cualquier
tipo de conocimiento, creo que sería

2835
02:10:03,629 --> 02:10:06,821
mucho más fácil aprender cualquier cosa.
Y la gente lo hará. Y lo harán por las

2836
02:10:06,821 --> 02:10:08,541
mismas razones por las que van al
gimnasio.

2837
02:10:08,620 --> 02:10:11,174
Lo que quiero decir es que eso suena un
poco diferente a

2838
02:10:12,840 --> 02:10:14,047
haciendo empleo de esto...

2839
02:10:14,080 --> 02:10:17,623
Entonces, post-IA General, estás usando
esto, eh, básicamente como

2840
02:10:17,623 --> 02:10:21,711
entretenimiento o, eh, como, m- mejora
personal. Pero sonaba como si también

2841
02:10:21,711 --> 02:10:26,181
tuvieras una visión de que esta educación
es relevante para mantener a la humanidad

2842
02:10:26,181 --> 02:10:29,451
en control de la IA. Y suenan diferentes,
y tengo curiosidad.

2843
02:10:27,320 --> 02:10:27,737
Ya veo.

2844
02:10:29,460 --> 02:10:32,357
¿Es entretenimiento para unos, y
empoderamiento para otros? ¿Tú qué

2845
02:10:32,357 --> 02:10:32,664
opinas?

2846
02:10:32,700 --> 02:10:35,762
Yo creo que esto, este... Así que,
definitivamente, siento que la gente va a

2847
02:10:35,762 --> 02:10:39,070
estar, este... Y creo que, a la larga, es
un poco una batalla perdida... si es que

2848
02:10:39,070 --> 02:10:39,805
eso tiene sentido.

2849
02:10:40,700 --> 02:10:42,557
Yo sí que creo que es a largo plazo-

2850
02:10:43,060 --> 02:10:46,191
a largo plazo, que creo que es más largo
de lo que la mayoría de la gente en la

2851
02:10:46,191 --> 02:10:49,442
historia piensa, es un juego perdido. Yo,
yo sí creo que la gente puede llegar muy

2852
02:10:49,442 --> 02:10:52,533
lejos y que apenas arañamos la superficie
de lo lejos que una persona puede...

2853
02:10:52,533 --> 02:10:55,343
puede llegar. Y eso es solo porque la
gente se topa con material que es

2854
02:10:55,343 --> 02:10:57,270
demasiado fácil... o demasiado difícil y
ellos...

2855
02:10:57,320 --> 02:11:01,671
Y, y yo, yo siento que la gente podrá ir
mucho más lejos. Como, cualquiera habla

2856
02:11:01,671 --> 02:11:04,425
cinco idiomas, ¿por qué no?, porque es
tan trivial.

2857
02:11:04,720 --> 02:11:08,158
Pues, cualquiera, eh, sabe, ya sabes,
todo el currículo básico de los estudios

2858
02:11:08,158 --> 02:11:09,363
universitarios... etcétera.

2859
02:11:09,580 --> 02:11:13,405
No, ahora que ya estoy entendiendo la
visión, e- eso es muy interesante. Como,

2860
02:11:13,405 --> 02:11:13,852
creo que-

2861
02:11:14,180 --> 02:11:16,973
De hecho, tiene un análogo perfecto en lo
que es la cultura del gimnasio. No creo

2862
02:11:16,973 --> 02:11:19,453
que hace cien años atrás, alguien
estuviera, por ejemplo, tan definido o

2863
02:11:19,453 --> 02:11:22,003
musculoso. Nadie tendría, sabes... la
habilidad de simplemente levantar de

2864
02:11:22,003 --> 02:11:24,692
forma espontánea dos o tres discos en
press de banca, o algo por el estilo. Y,

2865
02:11:24,692 --> 02:11:26,997
de hecho, es algo que se ha vuelto muy,
muy común en la actualidad.

2866
02:11:27,020 --> 02:11:30,961
Y es que la idea de entrenar
sistemáticamente, ya sea levantando pesas

2867
02:11:30,961 --> 02:11:34,616
o para correr un maratón, es una
capacidad que espontáneamente no

2868
02:11:34,616 --> 02:11:37,701
tendrías... o que la mayoría de los
humanos no tendría.

2869
02:11:36,520 --> 02:11:36,705
Sí

2870
02:11:37,780 --> 02:11:38,337
Claro que sí.

2871
02:11:38,340 --> 02:11:42,502
Y estás imaginando cosas similares para
aprender en muchos dominios diferentes

2872
02:11:42,502 --> 02:11:45,259
mucho más intensamente, profundamente, y
más rápido.

2873
02:11:45,320 --> 02:11:49,343
Sí. Sí, exactamente. Y en cierto modo
siento que estoy apostando un poco

2874
02:11:49,343 --> 02:11:53,479
implícitamente a la atemporalidad de la
naturaleza humana. Y creo que será

2875
02:11:53,479 --> 02:11:54,329
deseable ser...

2876
02:11:49,860 --> 02:11:50,277
Así es.

2877
02:11:54,820 --> 02:11:59,442
Para, para hacer todas estas cosas. Eh, y
creo que la gente lo va a ver como un

2878
02:11:59,442 --> 02:12:02,761
ejemplo, co- como lo han hecho por, por
milenios, porque-

2879
02:11:56,300 --> 02:11:56,532
Sí.

2880
02:12:02,860 --> 02:12:06,162
Y creo que esto seguirá siendo cierto. Y
de hecho, también, quizás haya pruebas

2881
02:12:06,162 --> 02:12:09,422
históricas de ello porque si miras, por
ejemplo, a los aristócratas o si miras

2882
02:12:09,422 --> 02:12:12,640
quizás la antigua Grecia o algo así,
cuando tenías pequeños entornos aislados

2883
02:12:12,640 --> 02:12:15,984
que eran post-AGI en cierto sentido...
siento que la gente ha pasado mucho de su

2884
02:12:15,984 --> 02:12:17,720
tiempo, uh, floreciendo de cierta manera.

2885
02:12:14,300 --> 02:12:14,439
Sí

2886
02:12:18,180 --> 02:12:21,473
eh, ya sea física o, o, eh,
cognitivamente. Y entonces creo que, um,

2887
02:12:21,473 --> 02:12:25,159
me siento bastante optimista con las
perspectivas de eso. Y creo que si esto

2888
02:12:25,159 --> 02:12:28,846
resulta ser falso y me equivoco, y
terminamos en, como, sabes, eh, un futuro

2889
02:12:28,846 --> 02:12:32,631
tipo WALL-E o Idiocracy, entonces creo
que es muy... ni siquiera me importaría

2890
02:12:32,631 --> 02:12:34,155
si hay, como, esferas de Dyson.

2891
02:12:34,260 --> 02:12:35,513
Es un resultado terrible.

2892
02:12:35,580 --> 02:12:36,926
Sí, sí, claro que sí, totalmente.

2893
02:12:36,940 --> 02:12:41,030
Es que, de verdad, me importa mucho la
humanidad, como... todos tienen que ser,

2894
02:12:41,030 --> 02:12:43,023
uh, como superhumanos, en cierto modo.

2895
02:12:43,324 --> 02:12:47,162
Yo- yo- yo supongo que, eh, sigue siendo
un mundo en el que eso no nos está

2896
02:12:47,162 --> 02:12:51,155
permitiendo realmente... Es- es como el
mundo de la cultura, ¿no? Donde, en el

2897
02:12:51,155 --> 02:12:54,423
fondo, no vas a poder, digamos,
transformar la trayectoria de...

2898
02:12:55,224 --> 02:12:59,566
eh, la tecnología o... influir en las
decisiones solo con tu propio esfuerzo o

2899
02:12:59,566 --> 02:13:03,287
cognición. Quizás puedas influir en
decisiones porque la IA pide tu

2900
02:13:03,287 --> 02:13:03,908
aprobación.

2901
02:13:04,584 --> 02:13:08,136
Pero no es que uno esté, como... No es
porque yo haya, como, inventado algo o

2902
02:13:08,136 --> 02:13:11,875
haya, como, ideado un diseño nuevo, que
esté, de verdad, influyendo en el futuro.

2903
02:13:11,844 --> 02:13:15,334
Mm. Eh, sí, quizás. En realidad no creo
que, eh... Yo, yo creo que habrá un

2904
02:13:15,334 --> 02:13:18,919
período de transición donde podremos
estar al tanto y, sabes... hacer avanzar

2905
02:13:18,919 --> 02:13:20,806
las cosas si realmente entendemos mucho.

2906
02:13:21,104 --> 02:13:24,504
Mmm, yo sí creo que a largo plazo, eso
probablemente desaparezca, ¿verdad? Pero,

2907
02:13:24,504 --> 02:13:27,690
eh, quizás incluso se convierta en un
deporte. Por ejemplo, ahora mismo hay

2908
02:13:27,690 --> 02:13:31,091
levantadores de pesas que van al extremo
en esta dirección. Entonces, ¿qué es el

2909
02:13:31,091 --> 02:13:32,899
levantamiento de pesas en la era
cognitiva?

2910
02:13:23,464 --> 02:13:23,789
Claro.

2911
02:13:32,924 --> 02:13:33,156
Sí.

2912
02:13:33,204 --> 02:13:36,640
Eh, quizás es gente que de verdad intenta
convertir en Olimpiadas el saber.

2913
02:13:36,684 --> 02:13:37,566
Sí, así es, claro.

2914
02:13:37,684 --> 02:13:41,773
Eh, como... Eh, y, y si tienes un tutor
de IA perfecto, eh, quizás puedas llegar

2915
02:13:41,773 --> 02:13:45,501
extremadamente lejos. Casi siento que
apenas estamos... Los, los, eh, los

2916
02:13:45,501 --> 02:13:49,383
genios de hoy apenas están rascando la
superficie de lo que una mente humana

2917
02:13:49,383 --> 02:13:50,315
puede hacer, creo.

2918
02:13:43,424 --> 02:13:43,656
Sí.

2919
02:13:50,364 --> 02:13:54,803
Sí. Me encanta, de verdad, esta visión. Y
también, eh... Es como que siento que la

2920
02:13:54,803 --> 02:13:58,805
persona con la que, uh, tu producto
encaja mejor en el mercado, soy... yo,

2921
02:13:58,805 --> 02:14:03,080
porque, verdad, mi trabajo implica tener
que estar aprendiendo temas diferentes

2922
02:14:03,080 --> 02:14:03,738
cada semana.

2923
02:14:03,864 --> 02:14:08,136
Y yo, la verdad, estoy... estoy como muy,
muy, muy emocionado si puedes...

2924
02:14:08,164 --> 02:14:10,950
Yo soy igual en ese sentido. Quiero
decir, yo... Sabes, mucha gente, por

2925
02:14:10,950 --> 02:14:14,012
ejemplo, odia la escuela y quiere salir.
Yo, de hecho, a mí me gustaba mucho la

2926
02:14:14,012 --> 02:14:17,034
escuela. Me encantaba aprender cosas,
etcétera. Quería quedarme en la escuela.

2927
02:14:17,034 --> 02:14:19,938
Me quedé hasta el doctorado, y luego no
me dejaron seguir, así que fui a la

2928
02:14:19,938 --> 02:14:20,331
industria.

2929
02:14:12,544 --> 02:14:12,776
Sí.

2930
02:14:21,224 --> 02:14:24,167
Pero quiero decir, yo... Básicamente, es,
a grandes rasgos, me encanta, eh, me

2931
02:14:24,167 --> 02:14:27,071
encanta aprender, eh, incluso por el
placer de aprender. Pero también, eh, me

2932
02:14:27,071 --> 02:14:30,091
encanta aprender porque es- ... una forma
de empoderamiento y de ser útil- ... y

2933
02:14:30,091 --> 02:14:30,511
productivo.

2934
02:14:29,744 --> 02:14:29,836
Sí

2935
02:14:30,544 --> 02:14:34,316
Yo, yo creo que también mencionaste un
punto, que, eh, también comenzamos, solo

2936
02:14:34,316 --> 02:14:38,234
para dejarlo bien claro. Creo que lo que
ha sucedido hasta ahora con los cursos en

2937
02:14:38,234 --> 02:14:41,813
línea es que, ¿por qué no nos han
permitido ya... e- e- permitir a cada ser

2938
02:14:41,813 --> 02:14:43,361
humano saber absolutamente todo?

2939
02:14:43,384 --> 02:14:44,266
Ajá.

2940
02:14:44,244 --> 02:14:48,881
Y creo que están tan cargados de
motivación porque no hay entradas obvias.

2941
02:14:48,881 --> 02:14:50,977
Y es tan fácil quedarse atascado.

2942
02:14:51,604 --> 02:14:55,814
Eh, y si tuvieras, en su lugar, esta co-
esta cosa... Básicamente, como, un tutor

2943
02:14:55,814 --> 02:14:59,761
humano realmente, realmente bueno...
sería, sería simplemente una liberación

2944
02:14:59,761 --> 02:15:02,656
tan, tan grande desde una perspectiva de
motivación. Sí.

2945
02:14:58,624 --> 02:14:58,949
Ajá

2946
02:15:01,924 --> 02:15:05,818
Mm-hmm. Sí, creo que sí. Porque se siente
mal dejar un material. Se siente mal.

2947
02:15:05,818 --> 02:15:09,413
Obtienes una mala recompensa de... uh,
dedicar mucho tiempo a algo que no

2948
02:15:09,413 --> 02:15:13,158
funciona, o, como, estar completamente
aburrido porque lo que recibes es muy

2949
02:15:13,158 --> 02:15:15,205
fácil o muy difícil. Así que, creo que
sí.

2950
02:15:15,224 --> 02:15:18,648
Creo que se siente... Cuando realmente lo
haces bien, aprender se siente bien. Y

2951
02:15:18,648 --> 02:15:21,985
creo que es un problema técnico el llegar
a ese punto. Y creo que, uhm, por un

2952
02:15:21,985 --> 02:15:25,366
tiempo será una colaboración entre IA y
humanos, y en algún momento, quizás sea

2953
02:15:25,366 --> 02:15:26,276
solo la IA. No lo sé.

2954
02:15:18,384 --> 02:15:18,801
Claro, sí.

2955
02:15:26,304 --> 02:15:27,139
Ya lo veremos.

2956
02:15:27,164 --> 02:15:30,946
Sí. ¿Puedo hacerte algunas preguntas
sobre cómo enseñar bien? Si tuvieras que,

2957
02:15:30,946 --> 02:15:34,286
por así decirlo, dar un consejo a otro
educador, en otro campo que te

2958
02:15:34,286 --> 02:15:34,826
interese...

2959
02:15:28,964 --> 02:15:29,381
Mmm.

2960
02:15:32,564 --> 02:15:32,703
Mmm

2961
02:15:34,844 --> 02:15:35,029
Mmm

2962
02:15:36,024 --> 02:15:39,444
eh, para hacer el tipo de tutoriales de
YouTube que has hecho... eh, quizás,

2963
02:15:39,444 --> 02:15:42,637
quizás sea especialmente interesante
hablar de dominios donde no puedes

2964
02:15:42,637 --> 02:15:46,377
simplemente, como... No puedes evaluar la
comprensión técnica de alguien pidiéndole

2965
02:15:46,377 --> 02:15:48,794
que programe algo o algo así. ¿Qué
consejo les darías?

2966
02:15:49,284 --> 02:15:52,087
Uh, creo que ese es un tema bastante
amplio. Realmente siento que hay,

2967
02:15:52,087 --> 02:15:55,013
básicamente... Casi me parece que hay
diez o veinte trucos y consejos que

2968
02:15:55,013 --> 02:15:56,760
probablemente hago de forma
semi-consciente.

2969
02:15:57,964 --> 02:15:58,056
Ajá.

2970
02:15:59,124 --> 02:16:02,100
Pero, eh, supongo que a un nivel más
general, siempre intento... Creo que

2971
02:16:02,100 --> 02:16:05,119
mucho de esto viene de mi formación en
física. Realmente disfruté mucho mi

2972
02:16:05,119 --> 02:16:07,889
formación en física. Tengo todo un
discurso cuando pienso cómo todos

2973
02:16:07,889 --> 02:16:08,922
deberían aprender física-

2974
02:16:08,984 --> 02:16:09,169
Mm

2975
02:16:09,324 --> 02:16:12,321
eh, en la, en la educación escolar
temprana, porque creo que la educación

2976
02:16:12,321 --> 02:16:15,318
escolar inicial no se trata de, eh,
acumular conocimientos o memoria para

2977
02:16:15,318 --> 02:16:18,565
tareas futuras en el ámbito industrial.
Se trata de poner en marcha el cerebro.

2978
02:16:18,644 --> 02:16:22,533
Y creo que la física es la que mejor
activa el cerebro, porque lo que te hace

2979
02:16:22,533 --> 02:16:26,167
hacer en tu cerebro durante la física es
extremadamente valioso después.

2980
02:16:26,184 --> 02:16:26,555
Ajá.

2981
02:16:26,584 --> 02:16:29,785
La idea de construir modelos y de crear
abstracciones y de comprender que existe

2982
02:16:29,785 --> 02:16:33,067
un primer orden de aproximación que es el
que describe la mayor parte del sistema,

2983
02:16:33,067 --> 02:16:36,309
pero luego existen términos de segundo,
de tercer y de cuarto orden que podrían o

2984
02:16:36,309 --> 02:16:37,079
no estar presentes.

2985
02:16:37,304 --> 02:16:40,834
Y la idea de que estás observando, como,
un sistema muy ruidoso, pero en realidad

2986
02:16:40,834 --> 02:16:44,143
hay como estas frecuencias fundamentales
que puedes abstraer. Como cuando un

2987
02:16:44,143 --> 02:16:47,585
físico entra a la, a la clase y dice,
"Oh, asuman que hay una vaca esférica," y

2988
02:16:47,585 --> 02:16:50,585
bla, bla, bla. Y todos se ríen de eso,
pero en realidad es brillante.

2989
02:16:42,824 --> 02:16:43,195
Ajá.

2990
02:16:50,644 --> 02:16:53,830
Es un pensamiento brillante, muy
generalizable en toda la industria,

2991
02:16:53,830 --> 02:16:57,588
porque, claro, una vaca es... Se puede
aproximar como una esfera, supongo yo, de

2992
02:16:57,588 --> 02:17:00,442
muchas maneras. Hay un libro muy bueno,
por ejemplo, 'Scale'.

2993
02:17:00,924 --> 02:17:05,124
Uh, es de un físico que habla de
biología, y este es un libro que quizás

2994
02:17:05,124 --> 02:17:05,893
recomendaría.

2995
02:17:05,884 --> 02:17:08,814
Pero, de hecho, puedes obtener una gran
cantidad de aproximaciones muy

2996
02:17:08,814 --> 02:17:11,787
interesantes y establecer leyes de
escalamiento para los animales. Y si

2997
02:17:11,787 --> 02:17:14,547
observas sus latidos cardíacos y
elementos similares, y, de hecho,

2998
02:17:14,547 --> 02:17:17,308
coinciden con el tamaño del animal y
otros aspectos por el estilo.

2999
02:17:17,344 --> 02:17:20,711
Se puede hablar de un animal como un
volumen, y de hecho se pueden derivar

3000
02:17:20,711 --> 02:17:24,447
muchas cosas, em... Se puede hablar de la
disipación de calor, eh, de este, porque

3001
02:17:24,447 --> 02:17:27,953
la disipación de calor crece con la
superficie, que lo hace al cuadrado, pero

3002
02:17:27,953 --> 02:17:30,997
tu creación o generación de calor, em,
está, eh, creciendo al cubo.

3003
02:17:31,044 --> 02:17:31,322
Ajá.

3004
02:17:32,084 --> 02:17:35,392
Y siento que los físicos tienen las
herramientas cognitivas adecuadas para

3005
02:17:35,392 --> 02:17:38,972
resolver problemas en el mundo. Por esa
formación, siempre busco los términos de

3006
02:17:38,972 --> 02:17:39,607
primer orden-"

3007
02:17:39,624 --> 02:17:39,809
Mmm

3008
02:17:39,824 --> 02:17:43,284
o los términos de segundo orden de todo.
Cuando observo un sistema o, o una cosa,

3009
02:17:43,284 --> 02:17:46,486
tengo una maraña de ideas o conocimientos
en mi mundo, en mi mente, y estoy

3010
02:17:46,486 --> 02:17:48,043
tratando de encontrar qué es lo que,

3011
02:17:48,124 --> 02:17:51,042
¿Qué es lo que realmente importa? ¿Cuál
es el componente de primer orden? ¿Cómo

3012
02:17:51,042 --> 02:17:53,961
puedo simplificarlo? ¿Cómo puedo tener lo
más simple que realmente muestre eso,

3013
02:17:53,961 --> 02:17:55,833
verdad? Que lo muestre, que lo demuestre
en acción.

3014
02:17:54,244 --> 02:17:54,383
Sí.

3015
02:17:55,884 --> 02:17:57,184
Y luego puedo añadir los demás.

3016
02:17:57,204 --> 02:17:57,343
Sí.

3017
02:17:57,744 --> 02:18:00,962
Quizás, uh, qui- quizás un ejemplo de mi,
uh, de uno de mis repos que creo que lo

3018
02:18:00,962 --> 02:18:04,059
ilustra bien es, uh, llamado MicroGrad.
No sé si estás familiarizado con esto.

3019
02:18:04,064 --> 02:18:04,760
Ajá.

3020
02:18:04,884 --> 02:18:08,377
Pero... MicroGrad son cien líneas de
código que muestran la retropropagación.

3021
02:18:08,377 --> 02:18:11,365
Puedes crear redes neuronales con
operaciones simples, como suma y

3022
02:18:11,365 --> 02:18:13,986
multiplicación, etc., bloques de Lego de
redes neuronales.

3023
02:18:14,304 --> 02:18:17,406
Y construyes un grafo computacional, y
haces una pasada hacia adelante y una

3024
02:18:17,406 --> 02:18:20,633
hacia atrás para obtener los gradientes.
Um, esto está en el corazón de todo el

3025
02:18:20,633 --> 02:18:23,776
aprendizaje de redes neuronales. Así que,
MicroGrad son cien líneas de código

3026
02:18:23,776 --> 02:18:27,003
Python bastante interpretable, y puede
hacer redes neuronales arbitrarias hacia

3027
02:18:27,003 --> 02:18:29,071
adelante y hacia atrás, pero no de forma
eficiente.

3028
02:18:29,404 --> 02:18:32,685
Así que MicroGrad, estas cien líneas de
Python, son todo lo que necesitas para

3029
02:18:32,685 --> 02:18:35,668
entender cómo entrenan las redes
neuronales. Todo lo demás es, eh, solo

3030
02:18:35,668 --> 02:18:36,137
eficiencia.

3031
02:18:36,144 --> 02:18:36,561
Así es.

3032
02:18:36,664 --> 02:18:39,554
Lo demás es eficiencia. Y hay mucho
trabajo para la eficiencia. Sabes,

3033
02:18:39,554 --> 02:18:40,518
necesitas tus tensores.

3034
02:18:40,544 --> 02:18:43,507
Los despliegas y los recorres. Te
aseguras de que tus kernels orquesten el

3035
02:18:43,507 --> 02:18:46,430
movimiento de memoria correctamente,
etcétera. Todo es solo eficiencia, a

3036
02:18:46,430 --> 02:18:49,231
grandes rasgos. Pero el aspecto
intelectual clave del entrenamiento de

3037
02:18:49,231 --> 02:18:52,153
redes neuronales es MicroGrads, 100
líneas. Puedes entenderlo fácilmente.

3038
02:18:46,864 --> 02:18:47,189
Claro.

3039
02:18:52,364 --> 02:18:54,984
Estás encadenando. Es aplicar
recursivamente la regla de la cadena para

3040
02:18:54,984 --> 02:18:57,791
derivar el gradiente, lo que permite
optimizar cualquier función diferencial

3041
02:18:57,791 --> 02:18:58,540
arbitraria. Así que-

3042
02:18:59,514 --> 02:19:02,882
Es que... me encanta encontrar estos,
como, ya sabes, los términos más

3043
02:19:02,882 --> 02:19:03,322
pequeños.

3044
02:19:03,334 --> 02:19:03,473
Sí

3045
02:19:03,834 --> 02:19:07,489
y sirviéndolos en bandeja de plata, y
descubriéndolos. Y siento que la

3046
02:19:07,489 --> 02:19:11,674
educación es, como, lo más interesante
intelectualmente, porque tienes un enredo

3047
02:19:11,674 --> 02:19:15,542
de conocimientos y estás tratando de
exponerlos de una manera que cree una

3048
02:19:15,542 --> 02:19:18,509
rampa, donde cada cosa solo depende de la
que la precede.

3049
02:19:18,594 --> 02:19:18,919
Ajá.

3050
02:19:18,954 --> 02:19:22,695
Y encuentro que este, sabes, desentrañar
el conocimiento es tan intelectualmente

3051
02:19:22,695 --> 02:19:26,531
interesante, como una tarea cognitiva. Y
por eso me encanta hacerlo personalmente,

3052
02:19:26,531 --> 02:19:29,988
pero simplemente me fascina intentar
organizar las cosas de cierta manera,

3053
02:19:29,988 --> 02:19:30,935
quizás eso me ayude.

3054
02:19:24,634 --> 02:19:24,959
Claro.

3055
02:19:31,034 --> 02:19:35,820
También, también hace que la experiencia
de aprendizaje sea mucho más motivadora.

3056
02:19:35,820 --> 02:19:40,307
Tu, tu tutorial sobre el transformer
empieza con bigramas, literalmente como

3057
02:19:40,307 --> 02:19:43,897
una tabla de consulta de... aquí está la
palabra ahora mismo.

3058
02:19:43,914 --> 02:19:44,424
Así es

3059
02:19:44,434 --> 02:19:47,731
o la palabra anterior... la siguiente. Y
es solo una tabla de consulta.

3060
02:19:47,714 --> 02:19:49,060
Sí. Es la esencia de ello, sí.

3061
02:19:49,074 --> 02:19:52,148
Es una forma brillante, como, vale,
empieza con una tabla de búsqueda y luego

3062
02:19:52,148 --> 02:19:55,464
pasa a un transformador y cada parte está
motivada, "¿Por qué añadirías eso?" "¿Por

3063
02:19:55,464 --> 02:19:56,597
qué añadirías lo siguiente?"

3064
02:19:56,594 --> 02:19:56,965
Ajá.

3065
02:19:57,154 --> 02:20:00,159
Podrías memorizar esta especie de fórmula
de atención, [pause] lo cual es como

3066
02:20:00,159 --> 02:20:03,164
tener una comprensión de por qué cada
pieza es relevante, [pause] qué problema

3067
02:20:03,164 --> 02:20:03,516
resuelve.

3068
02:20:03,554 --> 02:20:07,083
Sí, sí. Sí, estás presentando el dolor
antes de la solución. ¡Y qué astuto es

3069
02:20:07,083 --> 02:20:07,269
eso!

3070
02:20:06,234 --> 02:20:06,373
Sí.

3071
02:20:07,314 --> 02:20:10,453
Y uno quiere guiar al estudiante a través
de esa progresión, así que, eh, hay

3072
02:20:10,453 --> 02:20:13,551
muchas otras pequeñas cosas de ese tipo
que creo que lo hacen muy agradable,

3073
02:20:13,551 --> 02:20:16,732
atractivo e interesante. Y- y, ya sabes,
siempre impulsando al estudiante. Hay

3074
02:20:16,732 --> 02:20:19,954
muchas pequeñas cosas de ese tipo que
creo que son, ya sabes, importantes y que

3075
02:20:19,954 --> 02:20:21,524
muchos buenos educadores suelen hacer.

3076
02:20:21,514 --> 02:20:21,978
Ajá.

3077
02:20:22,054 --> 02:20:25,911
Eh, como, "¿Tú cómo resolverías esto?" O
sea, no voy a presentar una solución

3078
02:20:25,911 --> 02:20:29,920
antes de que tú intentes adivinar. Eso
sería una pérdida de tiempo. Eso sería...

3079
02:20:29,920 --> 02:20:31,341
Eso es- eso es un poco de...

3080
02:20:31,394 --> 02:20:35,412
No quiero decir palabrotas, pero es una
jugada muy sucia hacia ti, presentarte la

3081
02:20:35,412 --> 02:20:39,381
solución antes de darte la oportunidad de
que intentes encontrarla por ti mismo.

3082
02:20:37,874 --> 02:20:38,059
Bien

3083
02:20:39,374 --> 02:20:43,306
Sí, sí. Y es que, si lo intentas idear
por ti mismo, vas a... supongo que

3084
02:20:43,306 --> 02:20:47,129
obtendrás una mejor comprensión de, como,
cuál es el espacio de acción-

3085
02:20:47,854 --> 02:20:51,104
Y entonces, ¿cuál es el objetivo,
digamos? Entonces, ¿por qué solo esta

3086
02:20:51,104 --> 02:20:52,730
acción cumple ese objetivo, verdad?

3087
02:20:52,734 --> 02:20:56,189
Sí. Bueno, tienes la oportunidad de
ponerte a prueba, y te das cuenta, lo

3088
02:20:56,189 --> 02:20:59,933
aprecias cuando te doy la solución. Y,
uh, maximiza la cantidad de conocimiento

3089
02:20:59,933 --> 02:21:01,325
por cada nuevo hecho añadido.

3090
02:21:01,354 --> 02:21:07,610
Eso es, sí. Sí, sí. ¿Por qué crees que,
por defecto, los verdaderos expertos en

3091
02:21:07,610 --> 02:21:12,824
su campo suelen ser malos explicando a
alguien que está empezando?

3092
02:21:14,134 --> 02:21:17,264
Sí. Bueno, es la maldición del
conocimiento y la experiencia. Este es un

3093
02:21:17,264 --> 02:21:20,748
fenómeno real, y yo mismo lo he sufrido
por mucho que intente no... no sufrirlo.

3094
02:21:20,748 --> 02:21:24,099
Pero uno da ciertas cosas por sentadas, y
no puede ponerse en el lugar de los

3095
02:21:24,099 --> 02:21:27,627
nuevos... De la gente que recién empieza.
Y, uh, esto es generalizado y me pasa a

3096
02:21:27,627 --> 02:21:28,112
mí también.

3097
02:21:16,034 --> 02:21:16,405
Mmm. Sí.

3098
02:21:28,134 --> 02:21:28,459
Ajá.

3099
02:21:28,734 --> 02:21:31,756
Una cosa que, de hecho, me parece
extremadamente útil... Por ejemplo,

3100
02:21:31,756 --> 02:21:35,224
alguien me estaba mostrando un artículo
de biología hace poco, y al instante me

3101
02:21:35,224 --> 02:21:37,046
surgieron muchísimas preguntas terribles.

3102
02:21:37,354 --> 02:21:37,818
Así es.

3103
02:21:38,034 --> 02:21:41,481
Eh, pues lo que hice fue usar ChatGPT
para hacer las preguntas con el artículo

3104
02:21:41,481 --> 02:21:45,018
en la ventana de contexto, y luego, eh,
resolvió algunas de las cosas simples. Y

3105
02:21:45,018 --> 02:21:48,420
luego, de hecho, compartí el hilo con la
persona que lo compartió, eh, que en

3106
02:21:48,420 --> 02:21:51,823
realidad, como, escribió ese artículo o,
como, trabajó en ese trabajo. Y casi

3107
02:21:51,823 --> 02:21:52,987
siento que fue como, eh...

3108
02:21:52,994 --> 02:21:57,185
Si pueden ver las preguntas estúpidas que
hice, podría ayudarles a explicarlo mejor

3109
02:21:57,185 --> 02:21:59,588
en el futuro o algo por el estilo.
Porque, eh...

3110
02:21:59,594 --> 02:22:02,937
Así que, por ejemplo, para mi material,
me encantaría que la gente compartiera

3111
02:22:02,937 --> 02:22:06,281
sus conversaciones tontas con ChatGPT
sobre las cosas que he creado, porque me

3112
02:22:06,281 --> 02:22:09,625
ayuda mucho a ponerme, de nuevo, en los
zapatos de alguien que está empezando.

3113
02:22:09,654 --> 02:22:14,158
Otro truco como ese, que yo... funciona
de una manera verdaderamente asombrosa.

3114
02:22:14,354 --> 02:22:15,468
Ajá.

3115
02:22:15,554 --> 02:22:20,576
Uh, si alguien escribe un artículo, una
publicación de blog o un anuncio, es, en

3116
02:22:20,576 --> 02:22:24,708
el cien por ciento de los casos,
completamente cierto, que solo la

3117
02:22:24,708 --> 02:22:29,285
narración o la transcripción... de cómo
te lo explicarían a ti durante un

3118
02:22:29,285 --> 02:22:29,857
almuerzo—

3119
02:22:26,994 --> 02:22:27,086
Mm-hm

3120
02:22:30,234 --> 02:22:30,419
Sí

3121
02:22:30,434 --> 02:22:34,567
es mucho más, este, no solo que se pueda
entender, sino que...

3122
02:22:35,194 --> 02:22:39,516
pero en realidad, también es más preciso
y científico... en el sentido de que las

3123
02:22:39,516 --> 02:22:43,677
personas tienen la tendencia a explicar
las cosas de la manera más abstracta y

3124
02:22:43,677 --> 02:22:48,053
llena de jerga posible, y a aclararse la
garganta durante cuatro párrafos antes de

3125
02:22:48,053 --> 02:22:49,404
explicar la idea central.

3126
02:22:49,394 --> 02:22:49,672
Claro.

3127
02:22:50,494 --> 02:22:53,280
Pero hay algo en hablar cara a cara con
una persona-

3128
02:22:53,934 --> 02:22:57,045
que te obliga a simplemente decir lo que
es evidente.

3129
02:22:57,634 --> 02:22:58,284
Solo dilo.

3130
02:22:58,314 --> 02:22:58,917
Claro que sí.

3131
02:22:59,054 --> 02:23:02,177
De hecho, vi ese tuit. Me pareció muy
bueno. Lo compartí con mucha gente, la

3132
02:23:02,177 --> 02:23:03,094
verdad. Sí, muy bueno.

3133
02:23:03,154 --> 02:23:06,841
Y esto lo he notado muchísimas veces.
Mmm, quizás el ejemplo más destacado es,

3134
02:23:06,841 --> 02:23:09,954
recuerdo, uh, en mis tiempos de
doctorado, haciendo investigación,

3135
02:23:09,954 --> 02:23:12,349
etcétera, uh, lees el artículo de
alguien, ¿verdad?

3136
02:23:12,414 --> 02:23:15,388
Y trabajas. Intentas entender qué está
haciendo, etcétera. Y luego los

3137
02:23:15,388 --> 02:23:18,448
encuentras, tomando cervezas en la
conferencia después, y les preguntas:

3138
02:23:18,448 --> 02:23:21,767
"Entonces, este artículo, ¿qué estaban
haciendo? ¿De qué trata el artículo?" Y

3139
02:23:21,767 --> 02:23:25,043
ellos te dirán estas tres frases que
capturan perfectamente la esencia de ese

3140
02:23:25,043 --> 02:23:26,810
artículo... y te dan la idea por
completo.

3141
02:23:26,834 --> 02:23:30,160
Y no tuviste que leer el artículo de
nuevo. Y, es que, solo cuando estás en la

3142
02:23:30,160 --> 02:23:33,357
mesa con una cerveza o algo así, y te
dicen: "Ah, sí, el artículo es así...

3143
02:23:33,357 --> 02:23:36,899
Tomas esta idea, tomas aquella, y pruebas
este experimento. Y, eh, y pruebas esto."

3144
02:23:36,899 --> 02:23:39,664
Y tienen una forma de decirlo
conversacionalmente... y tan, como,

3145
02:23:39,664 --> 02:23:41,694
perfecto, como: "¿Por qué no es ese el
resumen?"

3146
02:23:28,234 --> 02:23:28,651
Sí. Sí,sí.

3147
02:23:39,374 --> 02:23:39,513
Bien

3148
02:23:41,654 --> 02:23:42,350
Exactamente.

3149
02:23:45,594 --> 02:23:49,532
Um, esto viene desde la perspectiva de
cómo una persona que intenta explicar una

3150
02:23:49,532 --> 02:23:51,027
idea debería formularla mejor.

3151
02:23:51,234 --> 02:23:55,416
¿Cuál es tu consejo, como estudiante,
para otros estudiantes en situaciones

3152
02:23:55,416 --> 02:23:59,598
donde, si no tienes a un Karpathy que te
esté exponiendo una idea, si estás

3153
02:23:59,598 --> 02:24:01,915
leyendo un artículo de alguien o un
libro-

3154
02:24:02,934 --> 02:24:06,474
¿Qué tipo de estrategias utilizas tú...
para aprender sobre material que te

3155
02:24:06,474 --> 02:24:09,249
interesa, en aquellos campos en los que
no eres un experto?

3156
02:24:05,414 --> 02:24:05,506
Mm-hm

3157
02:24:09,554 --> 02:24:13,787
Eh, la verdad, no sé si realmente tengo,
eh, como, uh, trucos o consejos únicos,

3158
02:24:13,787 --> 02:24:17,913
para serte honesto. Eh, básicamente, es
un... es un proceso bastante doloroso.

3159
02:24:17,974 --> 02:24:22,652
Pero, sabes, como, la primera versión.
Creo que algo que siempre me ha ayudado

3160
02:24:22,652 --> 02:24:27,331
bastante, la verdad, es... De hecho, hice
un pequeño tuit sobre esto. Así que,

3161
02:24:27,331 --> 02:24:31,766
aprender cosas bajo demanda es bastante
bueno, aprendiendo en profundidad.

3162
02:24:31,874 --> 02:24:34,296
Siento que necesitas alternar la
profundidad del aprendizaje según la

3163
02:24:34,296 --> 02:24:36,612
demanda. Estás intentando lograr un
proyecto del que obtendrás una

3164
02:24:36,612 --> 02:24:37,075
recompensa...

3165
02:24:37,054 --> 02:24:37,425
Ajá

3166
02:24:37,734 --> 02:24:40,780
y aprendiendo de forma amplia, que es
simplemente, "Hagamos un curso básico, y

3167
02:24:40,780 --> 02:24:43,827
aquí están todas las cosas que podrías
necesitar." Mucha escuela implica mucho

3168
02:24:43,827 --> 02:24:47,071
aprendizaje general. Como, "Confía en mí,
lo necesitarás después." ¿Sabes? Ese tipo

3169
02:24:47,071 --> 02:24:48,693
de cosas. Como, "Está bien, confío en
ti."

3170
02:24:46,114 --> 02:24:46,253
Sí.

3171
02:24:48,734 --> 02:24:51,906
Lo aprenderé, porque supongo que lo
necesito. Pero me encanta el tipo de

3172
02:24:51,906 --> 02:24:55,481
aprendizaje donde realmente obtienes una
recompensa... al hacer algo y aprendes a

3173
02:24:55,481 --> 02:24:55,839
demanda.

3174
02:24:50,574 --> 02:24:50,991
Así es.

3175
02:24:55,914 --> 02:24:59,197
Otra cosa que he descubierto y que me ha
resultado sumamente útil es, em... Y

3176
02:24:59,197 --> 02:25:02,134
quizás este sea, eh, un aspecto en el que
la educación es un poco más

3177
02:25:02,134 --> 02:25:05,504
desinteresada, porque, eh, explicarle
cosas a la gente es una forma preciosa de

3178
02:25:05,504 --> 02:25:07,059
aprender algo con mayor profundidad.

3179
02:25:07,154 --> 02:25:10,722
Eh, esto, eh, me pasa todo el tiempo.
Creo que a otras personas también les

3180
02:25:10,722 --> 02:25:14,629
debe ocurrir, porque me doy cuenta de que
si no comprendo algo de verdad, no puedo

3181
02:25:14,629 --> 02:25:18,438
explicarlo, ¿sabes? Y- y, eh, lo intento
y me digo: "En realidad, en realidad no

3182
02:25:18,438 --> 02:25:20,946
lo entiendo". Y es tan molesto tener que
aceptar eso.

3183
02:25:20,954 --> 02:25:23,965
Y entonces puedes regresar y verificar
que lo hayas comprendido. Y de esa

3184
02:25:23,965 --> 02:25:26,935
manera, llena esos vacíos en tu
entendimiento. Te fuerza a confrontarlos

3185
02:25:26,935 --> 02:25:30,281
y, uh, a encontrarles una solución. A mí
me encanta volver a explicar y cosas por

3186
02:25:30,281 --> 02:25:33,167
el estilo, y creo que la gente debería
hacer eso más a menudo también.

3187
02:25:33,174 --> 02:25:36,086
Creo que eso te obliga a manejar el
conocimiento y a asegurarte de que sabes

3188
02:25:36,086 --> 02:25:37,678
de qué estás hablando cuando lo explicas.

3189
02:25:37,874 --> 02:25:41,124
Oh, sí. Creo que es una magnífica nota
para cerrar. Andrei, estuvo excelente.

3190
02:25:39,614 --> 02:25:39,985
Ajá.

3191
02:25:41,134 --> 02:25:41,877
Sí, muchas gracias.

3192
02:25:42,234 --> 02:25:42,651
Gracias.

3193
02:25:42,674 --> 02:25:43,324
Cuando quieras.

3194
02:25:43,714 --> 02:25:47,307
Hola a todos. Espero que hayan disfrutado
de este episodio. Si les gustó, lo más

3195
02:25:47,307 --> 02:25:50,763
útil que pueden hacer es simplemente
compartirlo con otras personas a quienes

3196
02:25:50,763 --> 02:25:54,493
crean que podría gustarles. También es de
gran ayuda si dejan una calificación o un

3197
02:25:54,493 --> 02:25:56,949
comentario en la plataforma en la que
estén escuchando.

3198
02:25:57,374 --> 02:26:01,717
Si te interesa patrocinar el podcast,
puedes contactarnos en

3199
02:26:01,717 --> 02:26:06,429
dwarkesh.com/advertise. De lo contrario,
nos vemos en el próximo.

