1
00:00:00,280 --> 00:00:03,762
你知道最疯狂的是什么吗？所有这一切竟然都是真的。

2
00:00:01,440 --> 00:00:01,672
嗯

3
00:00:04,300 --> 00:00:05,089
嗯？什么意思？

4
00:00:05,100 --> 00:00:05,982
你不觉得吗？

5
00:00:06,080 --> 00:00:06,962
什么意思？

6
00:00:06,960 --> 00:00:13,647
就像是所有这些人工智能的东西，还有湾区正在发生的这一切...
是啊，这不就是科幻小说里才有的吗？

7
00:00:13,860 --> 00:00:26,816
嗯。另一件疯狂的事是，缓慢起飞感觉有多么正常。我们把GDP的百分之一投入到AI中，我
感觉这本该是件大事。你知道吗？然而现在，它却感觉就像是——

8
00:00:27,180 --> 00:00:35,492
事实证明，我们很快就习惯了。嗯。 但它也有点像是抽象的。
比如，那到底意味着什么呢？ 它的意思是你总是在新闻里看到它——

9
00:00:35,860 --> 00:00:36,231
是啊

10
00:00:36,400 --> 00:00:38,954
某某公司对外宣布了某某数额的美元。

11
00:00:39,020 --> 00:00:39,252
对

12
00:00:40,440 --> 00:00:41,554
你看到的就这些。

13
00:00:41,640 --> 00:00:42,104
没错

14
00:00:42,960 --> 00:00:45,374
至今，还没有以其他方式真正感受到。

15
00:00:45,400 --> 00:00:47,861
嗯。真要从这开始吗？这讨论挺有意思的。

16
00:00:47,840 --> 00:00:48,304
好的

17
00:00:48,840 --> 00:00:57,292
我觉得你说的那个观点，嗯，就是从普通人的角度来看，没什么太大的变化会一直都是这样，即
使到了奇点时代也是如此。

18
00:00:57,300 --> 00:00:58,832
不，我个人不这么认为。

19
00:00:58,840 --> 00:00:59,954
好的，挺有意思的。

20
00:01:00,000 --> 00:01:03,761
所以，我之前提到的那个没有觉得有什么不同的东西

21
00:01:05,580 --> 00:01:10,688
好的，那么某某公司宣布了一笔难以理解的巨额投资。

22
00:01:12,160 --> 00:01:12,624
没错

23
00:01:12,680 --> 00:01:15,001
我 不 认为 有人 知道 怎么 处理 那个。

24
00:01:15,040 --> 00:01:15,457
是的

25
00:01:15,860 --> 00:01:30,070
但我认为人工智能的影响将会被感受到。人工智能将渗透到整个经济体系中。这背后有非常强大
的经济驱动力。而且我认为这种影响将会非常强烈地被感受到。

26
00:01:30,720 --> 00:01:37,500
这种影响你何时会看到？我觉得模型比它们实际的经济影响要显得更聪明。

27
00:01:38,440 --> 00:01:50,635
是啊，没错。这确实是目前关于这些人工智能模型，最令人感到困惑不解的其中一个方面。我们
究竟该如何去理解并调和这样一个显而易见的事实，那就是它们在各种基准测试和评估中都展现

28
00:01:50,635 --> 00:01:52,232
出了如此卓越的性能——

29
00:01:52,340 --> 00:01:52,618
嗯

30
00:01:53,000 --> 00:01:56,250
你看看那些评估，然后你会说：“那些评估相当难。”

31
00:01:56,280 --> 00:01:56,697
没错

32
00:01:57,120 --> 00:02:05,339
他们表现得非常出色。然而，经济方面的影响看起来却显得大幅落后。这简直就像是

33
00:02:07,740 --> 00:02:24,133
很难理解模型怎么会，一方面能做出这些惊人的事情，另一方面，又会在某些情况下重复自己两次，这有点...
举个例子，假设你用氛围编程做点什么，然后你去某个地方，然后你遇到了一个bug。然后你告诉模型：“你能修复这个bug吗？”

34
00:02:29,540 --> 00:02:29,957
是的

35
00:02:30,340 --> 00:02:35,634
模型说：“天哪，你说得太对了，我有个漏洞。我去修复它。”结果它又引入了第二个漏洞。

36
00:02:35,680 --> 00:02:36,097
是的

37
00:02:36,820 --> 00:02:46,532
然后你告诉它，你又有了这个，这个新的，第二个bug。它就告诉你，“天哪，我怎么会犯这
种错？你又说对了。”然后它又把第一个bug带回来了。你可以在它们之间来回切换。然后你

38
00:02:46,532 --> 00:02:48,151
就觉得，“这，这怎么可能？”

39
00:02:39,460 --> 00:02:39,599
对

40
00:02:44,140 --> 00:02:44,372
嗯

41
00:02:45,700 --> 00:02:46,117
是的

42
00:02:48,160 --> 00:02:48,670
没错啊

43
00:02:48,740 --> 00:02:58,399
感觉就像是，我不太确定。但它确实暗示着，嗯，肯定有什么奇怪的事情正在发生。对于这种情
况，我这里有两种可能的解释。

44
00:02:58,880 --> 00:03:11,650
那么，这里有一个更异想天开的解释——那就是RL训练可能让模型有点过于单一和狭隘，有点
过于，我不知道，缺乏感知，尽管它在某些方面也让它们变得更具感知力。

45
00:03:17,540 --> 00:03:29,226
正因为如此，他们连基本的事情都做不了。 [停顿]
但还有另一种解释，那就是在人们进行预训练的时候，关于用什么数据进行训练的问题已经有了答案。

46
00:03:29,226 --> 00:03:31,936
[停顿] 因为那个答案就是一切。

47
00:03:35,120 --> 00:03:35,537
是的

48
00:03:36,120 --> 00:03:39,092
当你进行预训练的时候，你需要所有的数据

49
00:03:41,100 --> 00:03:44,304
所以你就不必去考虑，究竟是这个数据还是那个数据呢？

50
00:03:44,340 --> 00:03:44,804
是的

51
00:03:44,960 --> 00:03:48,489
但是人们做强化学习训练时，他们确实需要思考。

52
00:03:48,500 --> 00:04:02,688
他们说：“好的，我们想为这个东西进行这种强化学习训练，为那个东西进行那种强化学习训练
。”据我所知，所有公司都有团队专门生产新的强化学习环境，然后将其添加到训练组合中。那

53
00:04:02,688 --> 00:04:09,444
么问题是，它们到底是什么呢？自由度如此之多。你能生产的强化学习环境种类如此之广。

54
00:04:10,100 --> 00:04:24,449
还有一种情况是，我觉得这有时是无意为之的，人们会从评估结果中获取灵感。你会说，“嘿，
我希望我们的模型在发布时表现出色。”“我希望评估结果看起来很棒。”

55
00:04:28,640 --> 00:04:38,624
什么样的强化学习训练能对这项任务有所帮助呢？对吧？我觉得这确实会发生，而且我认为这能
解释很多正在发生的事情。

56
00:04:39,120 --> 00:04:53,951
如果我们把这一点，和模型实际上的泛化能力不足结合起来看，这就有可能解释我们目前所观察
到的许多现象，也就是评估表现与实际现实世界表现之间的这种脱节，而我们今天甚至都还不完

57
00:04:53,951 --> 00:04:56,070
全明白这到底意味着什么。

58
00:05:00,560 --> 00:05:07,804
我喜欢这个观点，真正的奖励作弊，其实是那些过于关注评估结果的人类研究员。

59
00:05:07,920 --> 00:05:21,944
嗯，我觉得有两种方式来理解，或者说思考你刚才指出的这一点。第一种是，你看，如果仅仅通
过在编程竞赛中变得超人，一个模型并不会自动变得更有品味

60
00:05:26,440 --> 00:05:36,471
并且对如何改进你的代码库做出更好的判断，那么你就应该扩展你的测试环境范围，这样你就不
仅仅是在编程竞赛中测试它能否取得最佳表现了。

61
00:05:36,480 --> 00:05:51,532
它也应该能够为X、Y或Z这类事物做出最好的应用。还有一点，也许这正是你所暗示的，就是
说，为什么一开始就应该是这样呢？为什么在编程竞赛中成为超人，并不能让你在更广泛的意义

62
00:05:51,532 --> 00:05:54,220
上成为一个更有品味的程序员呢？

63
00:05:54,440 --> 00:06:07,396
也许我们应该做的，不是持续地增加各种环境的数量和多样性，而是去找到一种方法，能够让你
从某一个环境中学习，并且提升你在其他任务上的表现。

64
00:06:07,760 --> 00:06:15,561
那么我有一个可能有用的人类类比。就拿这个情况来说吧……就拿你提到的竞技编程来说。假设
你有两个学生。

65
00:06:19,640 --> 00:06:30,791
他们其中一人，[停顿] 决定要成为最顶尖的竞技编程选手，[停顿]
为此他们会投入一万小时在该领域进行练习，[停顿] 他们会解决所有问题，[停顿]

66
00:06:30,791 --> 00:06:38,959
记住所有证明技巧，[停顿] 并且非常、非常、[停顿] 你知道，[停顿]
非常熟练地快速准确地实现所有算法。

67
00:06:40,060 --> 00:06:47,194
通过这样的努力，他们成为了最顶尖的选手，可以说是最好的之一。
而第二位学生呢，他可能觉得，“哇，竞技编程真是太酷了！”

68
00:06:47,194 --> 00:06:52,484
也许他们只练习了大概一百个小时。
这比之前那位少了很多很多，但他们也同样表现得非常出色。

69
00:06:52,484 --> 00:06:56,174
那么你觉得，他们两人中，谁在未来的职业生涯中会发展得更好呢？

70
00:06:49,880 --> 00:06:50,158
嗯

71
00:06:56,160 --> 00:06:56,717
第二个

72
00:06:56,760 --> 00:07:08,602
对吧？我觉得这基本上就是正在发生的事情。这些模型更像是第一个学生，但甚至更甚，因为我
们就会说，“好吧，那模型应该擅长竞技编程，所以我们把所有竞技编程问题都找出来，然后我

73
00:07:08,602 --> 00:07:12,549
们再做一些数据增强，这样我们就有更多竞技编程问题了——”

74
00:07:14,140 --> 00:07:14,743
是的，对

75
00:07:14,760 --> 00:07:26,173
我们就在此基础上进行训练。所以现在你就有了一个很棒的竞技程序员。有了这个类比，我觉得
它更直观了。我觉得用这个类比会更直观，那就是，如果它训练得如此之好，那么所有不同的算

76
00:07:26,173 --> 00:07:33,103
法和所有不同的证明技巧，都好像触手可及。而且更直观的是，有了这种程度的准备，它不一定
会泛化到其他事物。

77
00:07:38,180 --> 00:07:47,700
嗯。但，那，嗯，第二个学生在做的事情，它的类比是什么呢？在他们进行一百小时的微调之前
？

78
00:07:48,120 --> 00:07:53,599
我觉得他们身上好像有一种说不出来的东西。我觉得那就是所谓的“魅力所在”。

79
00:07:53,680 --> 00:07:54,144
是的

80
00:07:54,200 --> 00:08:00,469
对吧？我知道我读本科的时候，我记得有一个这样的学生跟我一起学习过，所以我知道，我知道
这种人是存在的。

81
00:08:01,160 --> 00:08:14,960
嗯。我觉得很有趣，就是把它和预训练所做的事情区分开来。所以，理解你刚才说的，我们不必
在预训练中特意选择数据，这实际上与一万小时的练习没有什么本质上的不同。只不过，你免费

82
00:08:14,960 --> 00:08:20,711
获得了那整整一万小时的练习，因为这些数据已经存在于预训练的分布之中了。

83
00:08:22,220 --> 00:08:30,997
但你好像在暗示，预训练本身并没有带来多少泛化能力，
只是预训练的数据量实在是太庞大了。 但它泛化能力不一定就比强化学习更好。

84
00:08:31,260 --> 00:08:35,253
预训练最主要的优势在于，它拥有极其庞大的数据量。

85
00:08:35,340 --> 00:08:35,804
好的

86
00:08:36,380 --> 00:08:48,129
其次，你不用费太多心思去考虑预训练要用什么样的数据。而且它本身就是一种非常自然的数据
，其中包含了大量人们日常所做的事情。

87
00:08:48,720 --> 00:08:49,277
对的呀

88
00:08:50,380 --> 00:08:58,460
人们的思想，以及我们所了解的许多特点。这就像是人们将整个世界投射到了文本上。

89
00:08:59,100 --> 00:08:59,610
没错啊

90
00:09:00,220 --> 00:09:16,659
而预训练呢，它就是试图利用海量的数据来捕捉到这一点。这，这非常地...
预训练，预训练它非常难以去推断和理解，因为它太难去弄明白模型究竟是以何种方式来依赖这些预训练数据的。

91
00:09:17,600 --> 00:09:31,987
那么，每当模型出现错误时，会不会是因为某些内容碰巧没有得到预训练数据的充分支持呢？你
知道，预训练所提供的支持，这也许是个比较宽泛的说法。我不知道我还能在这方面提供什么更

92
00:09:31,987 --> 00:09:33,528
有用的信息了，但是

93
00:09:36,120 --> 00:09:38,302
我不认为人类有预训练的对应物。

94
00:09:38,480 --> 00:09:39,130
让我想想

95
00:09:39,500 --> 00:09:53,437
人们提出了一些关于人类预训练的类比，我很想听听你对它们可能错误之处的看法。其中一个类
比是，思考一个人生命中的前18年、15年或13年，他们不一定具有经济生产力，但他们正

96
00:09:53,437 --> 00:09:55,428
在做一些事情，使他们能够

97
00:10:01,400 --> 00:10:03,489
更好地了解世界，等等诸如此类

98
00:10:03,640 --> 00:10:17,767
另一方面，就是把进化论看作是某种形式的搜索，持续了三十亿年之久，最终形成了一个人类的
生命个体。然后我很好奇，你是否认为这两种情况实际上与预训练是相似的呢？或者，你又会如

99
00:10:17,767 --> 00:10:22,308
何看待至少人类终身学习的模式呢？如果它不是预训练的话？

100
00:10:22,880 --> 00:10:36,719
我觉得这两者和预训练有一些相似点。预训练试图扮演这两者的角色，但我觉得也有一些很大的
不同。预训练数据的量非常非常惊人。

101
00:10:25,280 --> 00:10:25,419
嗯

102
00:10:27,200 --> 00:10:27,385
嗯

103
00:10:39,580 --> 00:10:40,276
是的，没错。

104
00:10:41,260 --> 00:10:56,010
某种程度上，一个人类，即使在15年后，只用了极少量的预训练数据，他们知道的也少得多。
但无论他们知道什么，他们都以某种方式知道得更深入。而那些错误，比如，在那个年纪，你不

105
00:10:56,010 --> 00:10:58,117
会犯人工智能会犯的错误。

106
00:10:49,440 --> 00:10:49,857
是的

107
00:10:59,160 --> 00:10:59,810
就是这样

108
00:10:59,840 --> 00:11:11,171
还有另外一件事。你可能会说，会不会是像进化一样的东西呢？答案可能是这样。但这种情况下
，我觉得进化可能真的会占上风。比如，我记得读到过一个案例，某个……

109
00:11:14,060 --> 00:11:23,208
你知道神经科学家们会做的一件事，或者说，神经科学家们了解大脑的一种方式，就是通过研究
那些大脑不同部位受损的人。

110
00:11:24,780 --> 00:11:25,383
嗯，好的

111
00:11:26,000 --> 00:11:33,244
所以说，有些人会表现出你根本无法想象的奇怪症状。这真的非常非常有趣。我想到一个很相关
的案例。

112
00:11:35,780 --> 00:11:48,736
我读到过一个人的故事，他因为某种脑部损伤，导致...
我想，可能是中风或者一场意外，使得他失去了处理情感的能力，结果，他再也感受不到任何情绪了。

113
00:11:51,460 --> 00:12:06,438
结果就是，你知道，他说话依然非常清晰流利，也能解决一些小谜题，在各种测试中，他看起来
都完全正常。但他感受不到任何情感。他不会感到悲伤，不会感到愤怒，也不会感到兴奋，而且

114
00:12:06,438 --> 00:12:10,361
他在做任何决定时，不知怎么地变得极其糟糕……

115
00:12:11,592 --> 00:12:28,496
他会花好几个小时来决定穿哪双袜子，而且他会做出非常糟糕的财务决定。那这很...
这又说明了什么呢？说明了我们与生俱来的情感，在让我们成为一个可行的、有能力的个体方面，究竟扮演着怎样的角色呢？

116
00:12:33,872 --> 00:12:45,389
我想这和你的问题有关—— 关于预训练，这就像也许预训...
也许如果你足够擅长，比如说，从预训练中获取所有能获取的东西，你，你也能得到那个，但这似乎是那种...

117
00:12:50,992 --> 00:12:55,635
嗯，从预训练中能否得到那个，这还真不一定能实现。

118
00:12:56,192 --> 00:13:01,393
那是什么？很明显，这不仅仅是单纯的情绪表达。它看起来像是某种

119
00:13:04,151 --> 00:13:14,785
一个近似于价值函数的东西，它会告诉你应该做出什么样的决定，以及任何决定最终的奖励会是
什么。你难道不觉得这并非隐含地来自——

120
00:13:15,391 --> 00:13:19,338
我觉得吧，我只是想说，它不是，呃，百分之百的明显。

121
00:13:19,352 --> 00:13:25,482
嗯。但那是什么？就是说，你对情感是怎么看的？情感在机器学习里又有什么类比呢？

122
00:13:26,332 --> 00:13:28,514
这应该是一种价值函数的东西。

123
00:13:28,572 --> 00:13:29,082
没错啊

124
00:13:29,312 --> 00:13:36,463
但我觉得机器学习领域并没有一个很好的类比，因为目前来看，价值函数在人们所做的事情中，
并没有扮演一个非常重要的角色。

125
00:13:36,531 --> 00:13:39,921
也许值得向观众解释一下什么是价值函数，如果你想的话。

126
00:13:39,992 --> 00:13:44,868
我的意思是，那当然了。我，我非常乐意，非常乐意去做这件事。好的。那么，

127
00:13:49,212 --> 00:14:01,172
那么当人们做强化学习的时候，现在强化学习的这种做法，人们是怎么训练这些智能体的呢？你
有一个神经网络，你给它一个问题，然后你让模型去解决它。模型可能会采取成千上万，几十万

128
00:14:01,172 --> 00:14:06,441
次行动或思考，然后它会产生一个解决方案。这个解决方案会被评分。然后这个分数

129
00:14:12,752 --> 00:14:16,652
被用来为每一个单独的动作提供一个训练信号。

130
00:14:18,291 --> 00:14:19,034
你的轨迹

131
00:14:18,992 --> 00:14:19,363
嗯嗯

132
00:14:20,192 --> 00:14:33,370
所以这意味着，如果你在做一件需要很长时间才能完成的事情，如果你在训练一个需要很长时间
才能解决的任务，你将完全无法学到任何东西，直到你真正解决它，直到你提出一个可行的解决

133
00:14:33,370 --> 00:14:38,860
方案。这就是强化学习朴素地完成的方式。O1、R1表面上也是这样完成的。

134
00:14:40,891 --> 00:14:52,965
价值函数会这样说：“好吧，你看。我或许有时，并非总是，能告诉你你做得好不好。”
呃，价值函数的概念在某些领域比其他领域更有用。所以，举个例子，当你下棋时

135
00:14:56,912 --> 00:15:08,243
比如说，你丢了一个棋子，你就知道，“我搞砸了。”你不需要把整盘棋下完，就能知道我刚才
做的，嗯，有多糟糕，那么，之前所做的，嗯，所有一切，也都是糟糕的。

136
00:15:08,891 --> 00:15:23,519
那么，价值函数能让你不必一直等到最后。比如说，假设你开始追求某种，嗯...
好的，假设你正在做某种数学方面的事情，或者编程方面的事情，然后你正在尝试探索一个特定的解决方案方向。

137
00:15:26,412 --> 00:15:33,099
然后呢，比如说，经过了上千次的思考之后，你得出结论，这个方向是行不通的。

138
00:15:34,332 --> 00:15:47,321
一旦你得出这个结论，你可能就已经获得了一个奖励信号，那是在一千个时间步之前，当你决定
走这条路的时候。你会说：“哦，下次在类似情况下，我不该再走这条路了。”这远在你真正想

139
00:15:47,321 --> 00:15:48,867
出一个解决方案之前。

140
00:15:51,391 --> 00:16:12,706
嗯。这在 Deep CIGAR 1
这篇论文中提到过，就是说，轨迹的整个空间范围实在是太大了，以至于我们可能很难从一个中间的轨迹和它的价值中去学习一个有效的映射关系。而且考虑到，就像我们知道的，在编程的时候，比如说，你可能会有一个错误的想法，然后你会回去，然后你会修改一些东西。

141
00:16:12,732 --> 00:16:21,230
这听起来像是对深度学习太缺乏信心了。我的意思是，当然，它可能很难，但没有什么深度学习
是做不到的。

142
00:16:15,332 --> 00:16:15,796
是的

143
00:16:21,271 --> 00:16:21,688
是的

144
00:16:22,291 --> 00:16:38,814
所以，我的预期是，就是说，价值函数它应该是很有用的，而且我完全，我完全地预期，它们在
未来一定会得到应用，如果现在还没有被广泛应用的话。我之前提到那个，就是，情感中枢受到

145
00:16:38,814 --> 00:16:43,142
了，嗯，损伤的那个人的情况，我更多的是想说，

146
00:16:45,372 --> 00:17:00,093
这或许暗示着人类的价值判断功能，在某种程度上，受到情感的调节，而这种调节方式是由进化
所固化的。也许这对于人们在世上有效地发挥作用至关重要。

147
00:17:00,472 --> 00:17:15,239
这，这正是我本来打算要问你的事情。情绪作为一种价值函数，有一个非常有趣的地方，那就是
它们拥有如此大的效用，同时却又相当，嗯，简单易懂。

148
00:17:16,092 --> 00:17:30,113
所以，我有两点想说。我确实同意，与我们所学习的那些知识，以及我们现在正在探讨的这些方
式相比，情感确实是相对简单的。它们甚至可能简单到，也许你可以用一种人类能够理解的方式

149
00:17:30,113 --> 00:17:33,785
，把它们清晰地描绘出来。我觉得这样做会很棒。

150
00:17:39,331 --> 00:17:46,529
不过从实用性来看的话，我觉得呢，你知道，这里面就存在着一个复杂性与鲁棒性之间的权衡。

151
00:17:48,212 --> 00:17:58,753
复杂的事物在某些特定领域确实能展现出非凡的效用，但简单的事物却能在极其广泛的各种情境
下都显得非常实用和有效。

152
00:17:59,172 --> 00:18:12,032
所以我觉得，我们对眼前所见的一种解读方式是，我们拥有的这些情感，它们本质上主要从我们
的哺乳动物祖先那里进化而来，然后在我们是人科动物的时候，只进行了那么一点点的微调，真

153
00:18:12,032 --> 00:18:13,103
的只有一点点。

154
00:18:13,252 --> 00:18:28,530
我们确实有一些，比如说，相当多的社交情感，而哺乳动物可能缺乏...
但它们并不复杂，正因为不复杂，它们才能在这个与我们过去所生活的世界截然不同的世界里，很好地为我们服务。实际上，它们也...

155
00:18:28,530 --> 00:18:34,382
它们也会犯错。例如，我们的情感... 嗯，我不知道。饥饿算是一种情感吗？

156
00:18:35,148 --> 00:18:40,256
嗯，辩论——这确实有争议，但我觉得，举个例子来说，我们直觉上的饥饿感

157
00:18:42,108 --> 00:18:48,423
未能成功地、正确地指引我们，在这个食物资源如此丰富充裕的世界上。

158
00:18:49,088 --> 00:18:59,211
嗯。人们一直在谈论数据的规模化、参数的规模化、计算的规模化。有没有一种更普遍的方式来
思考规模化呢？还有哪些其他的规模化维度呢？

159
00:19:00,848 --> 00:19:16,126
所以，那个...
所以，所以，这——这是一个观点。这是一个观点，我觉得它可能，可能是真的。所以，以前机器学习的工作方式是，人们会随便鼓捣——鼓捣一些东西，然后尝试

160
00:19:20,588 --> 00:19:23,885
并努力去获得有趣的结果。过去一直都是这样做的。

161
00:19:26,148 --> 00:19:26,519
然后

162
00:19:28,228 --> 00:19:37,562
关于规模化的这种洞察终于出现了，对吧？规模法则，以及像GPT-3这样的模型，突然之间
，所有人都恍然大悟，我们应该进行规模化了。

163
00:19:39,108 --> 00:19:50,760
这只是...
这是语言如何影响思维的一个例子。“规模化”这个词，嗯，就一个词，但它非常有力，因为它告诉人们该怎么做。他们会说：“好吧，我们-

164
00:19:50,760 --> 00:20:01,399
我们来尝试规模化。”所以你会说：“好吧，那我们到底在规模化什么？”而预训练就是一种可以规模化的东西。它是一种特定的规模化方法。

165
00:20:01,828 --> 00:20:02,292
是的

166
00:20:02,288 --> 00:20:07,442
预训练的重大突破，在于我们终于意识到这个方法是行之有效的。

167
00:20:07,968 --> 00:20:23,705
于是你会说：“嘿，如果你把一些算力，和一些数据，混合到一个特定规模的神经网络中，你就
会得到结果，而且你会知道，如果你只是按比例扩大这个‘配方’，效果会更好。”这同样很棒

168
00:20:23,705 --> 00:20:29,887
。公司喜欢这一点，因为它提供了一种非常低风险的方式来投入你的资源。

169
00:20:30,348 --> 00:20:30,487
嗯

170
00:20:31,648 --> 00:20:32,112
好的

171
00:20:32,268 --> 00:20:46,840
对吧？把资源投入研究要困难得多。对比一下，几个研究人员，他们需要主动去探索，去进行研
究，然后才能有所发现。而另一方面，只要获取更多数据，更多算力，你知道预训练肯定会有所

172
00:20:46,840 --> 00:20:47,360
收获。

173
00:20:48,768 --> 00:21:01,296
而且，你知道，看起来，根据各种，嗯，嗯，人们在推特上，或者说，一些人在推特上所说的那
些事情，也许，似乎Gemini他们已经找到了一种方法，能够从预训练中获取到更多的东西

174
00:21:01,296 --> 00:21:01,446
。

175
00:21:01,848 --> 00:21:15,182
但在某个时候，预训练的数据会用完的。这些数据显然是非常有限的。那么，接下来你该怎么办
呢？要么你进行某种升级版的再训练，采用与之前不同的方案，要么你进行强化学习，或者可能

176
00:21:15,182 --> 00:21:19,309
是其他什么。但现在计算力很强大，计算力现在非常强大。

177
00:21:19,508 --> 00:21:23,966
某种意义上，我们又回到了研究时代。那么，也许可以换个说法。

178
00:21:24,228 --> 00:21:36,534
直到2020年，从二零一二...
从2012年到2020年，这都是研究的时代。而现在，从2020年到2025年，则是规模化的时代。或者说，上下浮动，我们可以给这些年份加上误差范围。

179
00:21:37,688 --> 00:21:50,509
因为人们会说：“这太棒了，你得扩大规模，继续扩大。”就一个词，规模化。但现在规模已经
这么大了。难道人们真的相信，“哦，它已经这么大了，但如果你再扩大100倍，一切都会变

180
00:21:50,509 --> 00:21:57,378
得如此不同”吗？当然会不同。但人们真的相信，只要你把规模扩大100倍，一切都会彻底改
变吗？

181
00:22:02,168 --> 00:22:06,579
我觉得那不对。那么又回到了研究的时代，只是有了大型电脑。

182
00:22:06,788 --> 00:22:08,134
这说法真有意思。

183
00:22:10,508 --> 00:22:25,805
但现在，让我来问你刚才提出的那个问题。我们到底在扩展什么？拥有一个所谓的“秘方”又意
味着什么呢？因为，我想我并不知道在预训练中，数据或计算参数与损失之间存在着一种非常清

184
00:22:25,805 --> 00:22:29,083
晰、几乎就像物理定律一样的幂律关系。

185
00:22:30,848 --> 00:22:38,231
我们究竟应该寻求一种什么样的关系，以及我们应该如何思考这种新的模式会是什么样子？

186
00:22:38,908 --> 00:22:51,496
所以说，我们已经亲眼目睹了从一种规模化方式向另一种规模化方式的转变，也就是从预训练到
强化学习的这个过程。而现在，人们正在对强化学习进行大规模的扩展和应用。那么，根据目前

187
00:22:51,496 --> 00:23:00,038
在推特上大家所讨论和分享的信息来看，他们现在在强化学习上所投入的计算资源，已经远远超
过了在预训练阶段所投入的资源。

188
00:23:00,628 --> 00:23:06,247
因为强化学习确实会消耗相当多的算力。你知道，你会进行非常非常长的轨迹生成。

189
00:23:03,388 --> 00:23:03,527
嗯

190
00:23:06,268 --> 00:23:06,546
是

191
00:23:07,208 --> 00:23:21,017
所以，要生成这些推演，需要耗费大量的计算资源，而每次推演所获得的学习量却相对较少。所以你真的会花费，你真的会花费大量的计算资源。我可以想象...
我不会... 在这个，在这个阶段...

192
00:23:21,017 --> 00:23:26,480
更像是，我甚至不会称之为规模化，嗯，规模化。我会说，“嘿，你在干什么？”

193
00:23:27,128 --> 00:23:41,279
你现在做的事情，是不是你所能做的最有效率的事情？你能找到一种更有效率地利用你的计算资
源的方式吗？我们之前讨论过价值函数的问题，也许一旦人们擅长使用价值函数，他们就会更有

194
00:23:41,279 --> 00:23:42,964
效地利用他们的资源。

195
00:23:31,508 --> 00:23:31,786
嗯

196
00:23:47,288 --> 00:24:00,344
如果你们找到一种全新的模型训练方式，你可能会说：“这是在扩展规模，还是仅仅在消耗资源
？”我觉得这会变得有点模糊。从某种意义上说，当人们处于研究时代时，那时就像是，人们会

197
00:24:00,344 --> 00:24:10,136
说：“嘿，我们试试这个，这个，还有这个。我们试试那个，那个，还有那个。哦，看，有些有
趣的事情正在发生。”我认为这种模式会回归。

198
00:24:10,148 --> 00:24:22,322
嗯——所以如果我们回到研究的时代，退一步来说，我们最需要深入思考的那个关键部分是什么
呢？当你提到价值函数时，大家其实已经在尝试现有的方法了，但之后又让大型语言模型来充当

199
00:24:22,322 --> 00:24:23,336
评判者之类的。

200
00:24:23,728 --> 00:24:35,755
你可以说那是一个价值函数，但听起来你心里想的，是更基础的东西。我们是否需要，是否需要
回到...我们是否应该重新思考预训练本身，而不是仅仅在那个过程的末尾增加更多步骤？

201
00:24:35,936 --> 00:24:54,790
好的。那么，关于价值函数这个讨论，我觉得非常有趣。我想特别强调一点，那就是我认为价值函数它就像是...
它会使我们的领域变得更加高效，而且我相信这确实会带来很大的不同。但我也认为，任何你能借助价值函数完成的事情，其实你不用它也一样能做到，只不过速度会慢上许多。

202
00:24:57,596 --> 00:24:58,246
好的, 好的

203
00:25:00,076 --> 00:25:05,509
我认为最根本的一点是，这些模型不知怎的泛化能力就是比人类差得多。

204
00:25:06,396 --> 00:25:06,953
是的呀

205
00:25:07,956 --> 00:25:12,599
而且这简直是显而易见。这看起来像是一个非常根本性的事情。

206
00:25:12,956 --> 00:25:16,578
好的。那么这就是关键，也就是概括的部分，然后还有两个

207
00:25:18,476 --> 00:25:19,126
子问题

208
00:25:20,796 --> 00:25:35,481
有一个是关于样本效率的，那就是，为什么这些模型学习起来比人类需要多得多的数据？第二个
问题是，即使不考虑所需数据量，也有一个问题，为什么我们想教给模型的东西比教给人类要难

209
00:25:35,481 --> 00:25:41,601
得多？也就是说，对于人类来说，我们不一定需要一个可验证的奖励才能...

210
00:25:43,496 --> 00:25:55,727
你现在可能正在指导一群研究员，和他们交流。你给他们看你的代码，也给他们展示你的思维方
式。通过这些，他们就能学习你的思维方式，以及他们应该如何做研究。你不需要为他们设定一

211
00:25:55,727 --> 00:26:06,065
个可验证的奖励，比如“好的，这是你课程的下一部分，现在这是你课程的再下一部分。”或者
“哦，这次训练不稳定，我们得…”没有这种繁琐的定制化过程。

212
00:26:06,096 --> 00:26:17,613
那么，也许这两个问题实际上是有某种关联的。但我很想探讨一下这第二件事，它更像是持续学
习。而这第一件事，感觉就像是，嗯，样本效率。

213
00:26:19,456 --> 00:26:29,394
嗯，所以说，你其实可以思考一下，其中一个关于人类样本效率的可能解释，是需要被认真考虑
的，那就是进化论。

214
00:26:30,716 --> 00:26:41,908
进化赋予了我们尽可能少、却最有用的信息。而对于视觉、听觉和运动能力这些方面，我认为有
相当充分的证据表明，进化实际上已经给了我们很多。

215
00:26:48,736 --> 00:26:49,339
嗯，好的

216
00:26:49,636 --> 00:26:53,490
那么，举个例子，人类手部的灵巧程度，是远远地超过...

217
00:26:53,516 --> 00:27:06,628
我的意思是，如果你让机器人接受大量的训练和模拟，它们也能变得灵巧。但要在现实世界中训
练一个机器人，让它像人一样快速掌握一项新技能，这似乎是遥不可及的。而在这里你可能会说

218
00:27:06,628 --> 00:27:12,091
：“哦，是的，比如运动能力，我们所有的祖先都需要非常出色的运动能力。”

219
00:27:06,836 --> 00:27:07,021
嗯

220
00:27:14,036 --> 00:27:21,419
松鼠喜欢...
那么，运动能力可能就像，我们拥有某种难以置信的先验知识。你也可以对视觉提出同样的论点，你懂的。

221
00:27:19,576 --> 00:27:19,761
嗯

222
00:27:21,516 --> 00:27:40,138
我，我相信Yann
LeCun提过一个观点，就是，呃，小孩子学开车，在经过16个小时，或者说10个小时的练习之后，就能学会了，这是真的。但我们的视力非常好。至少对我来说，你知道，当我回想起自己五岁的时候，我，我那时对汽车非常着迷。

223
00:27:40,916 --> 00:27:51,318
我很确定，我五岁时对汽车的识别能力，就已经足以应对自动驾驶了。你五岁时看不到那么多数
据。你大部分时间都待在父母家里。所以你的数据多样性很低。

224
00:27:53,176 --> 00:27:59,491
但你也许会说，那或许也算是进化。但如果那样的话，语言、数学和编程，恐怕就不是了。

225
00:28:00,516 --> 00:28:09,618
它看起来仍然比模型更好。我的意思是，显然模型在语言、数学和编程方面比普通人更强。但它
们在学习方面比普通人更强吗？

226
00:28:09,676 --> 00:28:19,335
哦，对。哦，对。完全是这样。我想要表达的意思是，语言、数学和编程，特别是数学和编程，
这些都暗示着，无论是什么因素让人们

227
00:28:21,036 --> 00:28:28,187
善于学习，可能与其说是一个复杂的先验，不如说是一种更深层、更本质的东西。

228
00:28:29,316 --> 00:28:31,870
等等。我... 我不太懂。为... 为什么会这样呢？

229
00:28:32,436 --> 00:28:33,318
考虑个技能

230
00:28:34,896 --> 00:28:39,539
人们会表现出某种非常高的可靠性，或者说，你知道，嗯...

231
00:28:39,856 --> 00:28:40,366
没错啊

232
00:28:41,656 --> 00:28:57,194
如果这项技能对我们的祖先来说，在漫长的数百万年、甚至数亿年的时间里都极其有用，那么你
或许可以说，甚至可以争辩说，人类之所以擅长它，正是因为进化的缘故。因为我们拥有一个先

233
00:28:57,194 --> 00:29:03,668
验的认知。一个以某种非常不明显、不易察觉的方式编码在基因中的进化先验。

234
00:29:03,896 --> 00:29:04,313
是的

235
00:29:04,556 --> 00:29:06,274
这让我们在这方面如此擅长。

236
00:29:06,296 --> 00:29:06,667
是的

237
00:29:07,216 --> 00:29:20,962
但如果人们展现出卓越的能力、高度的可靠性、极强的稳健性，以及在直到最近才真正出现的领
域中学习的能力，那么这更多地表明人们可能拥有

238
00:29:26,336 --> 00:29:28,147
就是更好的机器学习，仅此。

239
00:29:28,376 --> 00:29:47,091
嗯。但我们究竟该如何去理解那到底是什么呢？这究竟是关于什么呢？嗯，那在机器学习领域中，有什么可以作为它的类比呢？嗯，关于它，有几点非常有趣的地方。它需要的样本数量更少。它更偏向于无监督学习。你不需要设定一个...
就像一个孩子学开车，一个孩子... 当然不是真的在学开车。

240
00:29:47,176 --> 00:30:01,200
一个青少年在学习如何驾驶汽车时，并不是在获得某种现成的、可以被验证的奖励。这种奖励，
而是来自于他们与机器的互动，以及与环境的互动。

241
00:30:01,856 --> 00:30:07,057
嗯，而且，它需要的样本少得多。它看起来更像是无监督的。它看起来更鲁棒。

242
00:30:07,416 --> 00:30:12,106
远比我们想象的要坚韧得多。人类的坚韧程度真是令人难以置信。

243
00:30:12,456 --> 00:30:24,065
嗯，所以这就像是...
好的，你有没有一个统一的思考方式，来解释为什么所有这些事情会同时发生？有没有什么机器学习的类比，能够，呃，呃，能够实现这样的情况？

244
00:30:24,056 --> 00:30:38,162
那么，那么，那么，嗯，这就是，你知道，你一直在问的一个问题，那就是青少年司机如何，你
知道，能够自我纠正，并从他们自己的经验中学习，而不需要任何外部的老师来指导他们？答案

245
00:30:38,162 --> 00:30:40,681
是，嗯，他们有自己的价值函数。

246
00:30:40,724 --> 00:30:41,327
嗯嗯嗯

247
00:30:41,324 --> 00:30:52,704
对吧？他们拥有一种普遍的、内在的认知和判断，顺便说一句，这种认知在人类社会中表现得极
其稳健。比如说，无论我们所说的人类价值函数究竟是什么，不管这个人类价值函数具体指的是

248
00:30:52,704 --> 00:30:57,717
什么，除了少数与成瘾相关的特殊情况之外，它实际上是非常、非常稳固和可靠的。

249
00:30:59,204 --> 00:31:11,510
那么对于一个正在学开车的青少年来说，他们一开始开车，立刻就能感觉到自己开得怎么样，他
们有多么不自信。然后他们会发现，好吧，当然了，任何一个青少年的学习速度都是如此之快。

250
00:31:15,944 --> 00:31:17,197
10小时后，没问题了。

251
00:31:17,244 --> 00:31:27,367
是的。看起来人类似乎有一些解决方案，但我很好奇，他们是怎么做到的？为什么会这么难？我
们该如何重新构思我们训练模型的方式，才能让这样的事情成为可能？

252
00:31:27,444 --> 00:31:35,756
你知道，这确实是一个非常好的问题，而且对于这个问题，我个人确实有很多自己的看法和见解
。

253
00:31:37,043 --> 00:31:53,466
但是不幸的是，我们生活在一个并非所有机器学习想法都能自由讨论的世界里，而这，这正是其
中之一。所以很可能有一种方法可以做到。我认为这是可以做到的。人们是那样的这个事实，我

254
00:31:53,466 --> 00:32:00,309
认为这证明了它是可以做到的。不过可能还有另一个障碍，那就是有一种可能性

255
00:32:02,424 --> 00:32:16,866
人类神经元实际处理的信息量比我们想象的要大。如果这是真的，并且如果这扮演着重要角色，
那事情可能就会更困难了。但无论如何，我确实认为这指向了某种存在的可能性。

256
00:32:19,244 --> 00:32:27,696
机器学习原理，我对此没有什么看法。但很不幸，目前的情况让我很难去详细地讨论这些，你明
白吗？

257
00:32:28,543 --> 00:32:30,168
没人听那些播客，伊利亚。

258
00:32:31,684 --> 00:32:32,194
没错啊

259
00:32:32,244 --> 00:32:45,702
我得说，为伊利亚做准备相当困难，因为我和其他人都不知道他在做什么，也不知道SSI想做
什么。我没有任何依据来提出我的问题。老实说，我唯一能做的，就是尝试从第一性原理思考，

260
00:32:45,702 --> 00:32:51,470
AGI的瓶颈究竟是什么？因为很明显，伊利亚肯定在以某种方式解决这些问题。

261
00:32:54,244 --> 00:33:07,747
这个问题的一部分涉及到思考RL的扩展性，因为大家都在问RL的泛化能力如何，以及我们如
何能让它泛化得更好。为此，我最近读了一篇关于RL扩展性的论文，它表明RL的学习曲线实

262
00:33:07,747 --> 00:33:17,231
际上看起来像一个S形曲线。我对此感到非常好奇。为什么它会是S形曲线呢？长时间学习甚少
，然后迅速学到很多，最后趋于平稳？

263
00:33:17,264 --> 00:33:31,547
这与你在预训练中看到的幂律非常不同，在预训练中，模型在最开始学到很多，然后随着时间推
移学得越来越少。这实际上让我想起了我与一位研究员朋友交谈后写下的一段笔记，他指出，为

264
00:33:31,547 --> 00:33:38,858
了找到正确答案，你需要采样的数量与你当前概率分布和目标概率分布的差异程度呈指数级增长
。

265
00:33:41,224 --> 00:33:50,233
我当时在想这两个想法是如何关联的。我有个模糊的想法，它们应该有联系，但我真不知道如何
连接。我没有数学背景，所以我无法真正地把它公式化。

266
00:33:50,264 --> 00:34:02,864
但我好奇Gemini-3能不能帮我解决这个问题。于是我拍了我的笔记本，又拿了那张纸，
然后把它们都放到了Gemini-3的语境中，并让它找出其中的关联。它思考了很久，然后

267
00:34:02,864 --> 00:34:11,115
它意识到，在强化学习中，对从单个“是”或“否”结果中获得的信息进行建模的正确方法，是
将其视为随机二元变量的熵。

268
00:34:12,304 --> 00:34:25,392
它画了一个图表，显示了在强化学习（RL）和监督学习中，随着通过率的提高，每个样本所获
得的比特数是如何变化的。当我看到Gemini-3生成的那个图表时，我立刻就明白了许多

269
00:34:25,392 --> 00:34:28,975
事情。然后我想看看这个理论是否有任何经验基础。

270
00:34:29,004 --> 00:34:35,533
于是我让 Gemini
编写一个实验，来展示损失的改进是否以这种方式与通过率成比例。我直接拿了

271
00:34:35,533 --> 00:34:46,036
Gemini 输出的代码，把它复制粘贴到一个 Google Colab
笔记本里，然后我就能运行这个小型的机器学习实验，并可视化它的结果，没有任何一个

272
00:34:46,036 --> 00:34:46,604
bug。

273
00:34:46,643 --> 00:35:06,565
有趣的是，结果看起来相似，但与我们预期的并不完全相同。于是我下载了这张图表，把它放进了
Gemini，然后我问它：“这是怎么回事？”它提出了一个假设，我认为这个假设实际上是正确的，那就是我们通过固定学习率，限制了监督学习在初期能改进的程度。事实上，我们应该随着时间的推移降低学习率。

274
00:35:06,624 --> 00:35:14,054
这实际上让我们直观地明白，为什么在实践中，我们会有学习率调度器，随着时间推移降低学习
率。

275
00:35:14,104 --> 00:35:33,283
我用 Gemini-3
完成了整个流程，从提出这个模糊的初始问题，到建立理论理解，再到运行一些小型机器学习实验。这感觉是第一个能真正提出我意想不到的新联系的模型。它现在已经成为我想要集思广益，思考问题新方法时的首选之地。

276
00:35:35,784 --> 00:35:46,014
如果你想了解更多关于RL扩展，可以看看我写的那篇博客文章，它是在Gemini-3的帮
助下完成的。如果你想亲自体验一下Gemini-3，可以访问gemini.google

277
00:35:46,014 --> 00:35:48,694
。我很好奇，如果你说我们又回到了研究的时代。

278
00:35:49,984 --> 00:36:08,676
你从2012年到2020年都在那里。那，你觉得，如果我们回到那个研究时代，现在的氛围
会是怎样的？举个例子，即使在AlexNet之后，用于运行实验的计算量持续增加，并且前

279
00:36:08,676 --> 00:36:11,346
沿系统的规模也持续扩大。

280
00:36:12,203 --> 00:36:18,286
那么你现在觉得，在当前这个研究的时代里，我们是否仍然需要非常巨大的计算能力呢？

281
00:36:18,684 --> 00:36:35,034
呃，你觉得这会需要回到档案室查阅旧论文吗？是啊，也许当时是什么样的氛围，比如你在谷歌
、OpenAI和斯坦福这些地方的时候，当那里有更多研究氛围的时候。我们应该期待社区里

282
00:36:35,034 --> 00:36:36,981
出现什么样的东西呢？

283
00:36:38,600 --> 00:36:49,606
是的。所以，呃，规模化时代所带来的一个后果就是，呃，规模化几乎占据了所有的空间，让其
他一切都黯然失色。

284
00:36:49,620 --> 00:36:50,177
对的呀

285
00:36:51,160 --> 00:37:08,342
所以，因为规模化发展几乎耗尽了所有的市场空间，每个人都开始做着同样的事情。我们已经到
了这样一个地步，我们身处的世界里，公司的数量比好的想法要多得多，而且是多出了一大截。

286
00:37:08,640 --> 00:37:22,293
其实，说到这个，有个硅谷的说法，说想法不值钱，执行才是一切。很多人都这么说，这话也有
道理。但后来我看到，有人在推特上说，大概是这样的话：“如果想法这么不值钱，为什么没人

287
00:37:22,293 --> 00:37:23,593
有任何想法呢？”

288
00:37:19,680 --> 00:37:19,865
嗯

289
00:37:31,040 --> 00:37:44,136
我也这么认为。我觉得，如果你把研究进展看作瓶颈，那就有好几个瓶颈。如果你回顾一下，其
中之一是想法，另一个是你将它们付诸实践的能力——这可能涉及计算，但也包括工程。

290
00:37:52,180 --> 00:38:04,077
那么，如果你回到90年代，比如说，当时有些人，他们其实有非常好的想法，如果他们能有大
得多的电脑，也许就能证明他们的想法是可行的，但实际上他们做不到。所以他们只能做非常非

291
00:38:04,077 --> 00:38:06,344
常小的演示，根本无法说服任何人。

292
00:38:06,380 --> 00:38:06,890
没错啊

293
00:38:07,620 --> 00:38:18,858
那么瓶颈就是算力。接着，在规模化时代，算力大大增加了。当然，需要多少算力是个问题，但
算力是巨大的。

294
00:38:20,840 --> 00:38:36,165
那么，计算量已经足够大，以至于你似乎并不明显需要更多计算量来验证某个想法。比如说，我给你打个比方。AlexNet
是基于两块GPU构建的。那是它所使用的全部计算量。而Transformer

295
00:38:43,140 --> 00:38:53,821
是在8到64个图形处理器上构建的。没有任何一篇Transformer论文的实验使用了
超过2017年的64个图形处理器，这大概相当于，嗯，今天的两个图形处理器吧？

296
00:38:54,800 --> 00:38:57,586
所以说，我们现在讨论的是 ResNet，对吧？

297
00:38:58,580 --> 00:39:14,183
很多人，比如说，嗯，甚至你可以说，像L1推理，它本身并不是世界上计算量最大的那种东西
。所以，对于研究来说，你当然肯定需要一定量的计算资源，但这绝不意味着你需要有史以来最

298
00:39:14,183 --> 00:39:16,784
大量的计算资源才能进行研究。

299
00:39:21,760 --> 00:39:22,270
嗯，这个...

300
00:39:22,320 --> 00:39:35,530
你可能会争辩，我也觉得这是事实，如果你真的想要打造一个绝对顶尖的系统，如果你真的想要
打造一个绝对顶尖的系统，那么拥有远超常人的计算能力，会非常有帮助。特别是当所有人都处

301
00:39:35,530 --> 00:39:41,035
于相同的范式之下时，那么计算能力就会成为一个重要的、巨大的差异化因素。

302
00:39:42,120 --> 00:40:01,021
嗯，我想，这些想法当时是可能被提出的，我向你请教这段历史，因为你亲身经历过，我不确定具体发生了什么，但听起来，这些想法在计算资源极少的情况下也能发展出来，但它并没有立刻出名。Transformer
并没有立刻出名。它变成了大家开始做的事情，然后大家开始在此基础上进行实验和构建。

303
00:40:03,140 --> 00:40:06,112
因为它在越来越高的算力下得到了验证。

304
00:40:06,180 --> 00:40:06,876
完全正确

305
00:40:07,380 --> 00:40:19,008
而如果你们SSI有五十个不同的想法，你们又如何判断，哪一个才是下一个划时代的Tran
sformer模型，哪一个又是，你知道的，不堪一击的呢？在没有其他顶尖实验室所拥有的

306
00:40:19,008 --> 00:40:21,915
那种强大算力的情况下，你们又如何能做到呢？

307
00:40:22,240 --> 00:40:34,360
那么，我可以对此发表评论。简单来说，你提到了SSI，具体来说，SSI用于研究的计算资
源量真的不算少，我想解释一下原因。

308
00:40:42,620 --> 00:40:51,536
简单的数学就能解释，为什么我们拥有的计算能力，实际上比人们想象的更适合研究。现在我来
解释一下。那么

309
00:40:55,599 --> 00:41:10,413
SSI 已经筹集了30亿美元，这可不是个小数目...
从任何绝对意义上来说，这都算很多了。但你可能会说：“看看其他公司，它们筹集了更多。”但他们很多计算资源，很多算力都用于推理。

310
00:41:13,700 --> 00:41:18,854
这些大数字，这些巨额贷款，它们都是专门用于推理的。这是第一点。

311
00:41:20,100 --> 00:41:28,691
第二点，你需要，如果你想拥有一个能够进行推理的产品，你就必须拥有一支庞大的工程师团队
和销售团队。

312
00:41:28,760 --> 00:41:42,644
很多研究需要投入到生产各种产品相关的功能上。所以当你看到真正留给研究的部分时，这个差
距就小了很多。另外一点是，如果你在做一些不同的事，你真的需要最大规模来证明它吗？我根

313
00:41:42,644 --> 00:41:44,131
本不认为那是真的。

314
00:41:52,820 --> 00:42:01,922
我认为，就我们而言，我们拥有足够的算力，足以证明，并说服我们自己以及其他所有人，我们
正在做的事情是正确的。

315
00:42:02,900 --> 00:42:11,909
有公开估计称，像OpenAI这样的公司，每年花费大约五到六十亿美元，仅仅是目前用于实
验。

316
00:42:11,920 --> 00:42:12,430
嗯，好的

317
00:42:12,500 --> 00:42:22,809
这笔钱还不算他们花在推理以及其他方面的开销。那么，这看起来他们每年光是用于运行研究实
验的开销，就比你们公司所有的总资金还要多。

318
00:42:23,120 --> 00:42:37,882
我认为这取决于你如何使用它。这取决于你如何使用它。比如说，如果他们有更多……我认为在
他们和其他人的情况中，对训练计算的需求要大得多。有更多不同的工作流程，有不同的模态，

319
00:42:37,882 --> 00:42:41,045
就是有更多东西。所以它变得碎片化了。

320
00:42:44,480 --> 00:42:47,219
SSI到底要怎么赚钱呢？你知道的吧？

321
00:42:48,589 --> 00:42:50,446
我对这个问题的答案大概是

322
00:42:52,950 --> 00:43:00,055
现在呢，我们就是先专注于研究，然后呢，问题的答案它自然就会显现出来。我想啊，到时候可
能会有很多种答案。

323
00:43:00,490 --> 00:43:03,926
嗯。SSI的计划还是直接冲击超级智能吗？

324
00:43:04,990 --> 00:43:18,132
也许吧。我认为这有其价值。我认为这很有价值，因为能够不受日常市场竞争的影响，我觉得非
常好。但我认为有两个原因可能会导致我们改变计划。

325
00:43:09,190 --> 00:43:09,515
嗯哼

326
00:43:25,270 --> 00:43:35,811
首先，这很务实，如果时间线很长，而这很可能发生。其次，我认为，让最好、最强大的AI能
够影响世界，这非常有价值。

327
00:43:42,630 --> 00:43:43,047
是的

328
00:43:43,870 --> 00:43:45,959
我认为这很有意义，也很有价值。

329
00:43:46,150 --> 00:43:58,455
那为什么你们的默认计划是直接瞄准超智能呢？因为听起来，像OpenAI、Anthrop
ic，所有这些公司，他们的明确想法是：“听着，我们有越来越弱的智能，公众可以逐渐适应

330
00:43:58,455 --> 00:44:02,264
并做好准备的。”那为什么直接构建超智能可能会更好呢？

331
00:44:06,109 --> 00:44:23,523
那么我将阐述支持和反对的理由。支持的理由是，你身处...
那么，人们在市场中面临的挑战之一是，他们必须参与到这场激烈的竞争中。而这场竞争是相当艰难的，因为它会让你面临许多必须做出的艰难权衡。

332
00:44:08,250 --> 00:44:08,714
是的

333
00:44:27,509 --> 00:44:42,555
我们可以很轻松地说，“我们会与所有这些隔绝开来，只是专注于研究，等到我们完全准备好了
才现身，而不是在此之前。”但反驳的观点也同样成立，而且，呃，它们是——它们是相互对立

334
00:44:42,555 --> 00:44:47,571
的力量。反驳的观点是，嘿，让世界看到强大的AI是有益的。

335
00:44:50,910 --> 00:44:55,275
让全世界都看到强大的AI，这是很有益的，因为那是唯一能传达它的方式。

336
00:44:55,390 --> 00:44:58,083
嗯，我觉得不只是你能传达这想法，而是...

337
00:44:58,109 --> 00:45:02,660
传达人工智能，而不是想法。传达人工智能。

338
00:45:02,890 --> 00:45:04,050
你是说，沟通AI？

339
00:45:04,049 --> 00:45:13,940
那好，假设你读了一篇关于人工智能的文章——文章说人工智能会这样——会那样，还会这样。
你读完后说：“好吧，这是一篇挺有意思的文章。”

340
00:45:07,210 --> 00:45:07,442
嗯

341
00:45:11,549 --> 00:45:12,059
嗯嗯嗯

342
00:45:14,950 --> 00:45:15,414
没错

343
00:45:15,810 --> 00:45:29,277
假设你看到一个AI在做这个，一个AI在做那个，这是无法比拟的。基本上，我认为——我认
为AI面向公众有很大的好处，这也是我们不那么直截了当的一个原因。

344
00:45:35,470 --> 00:45:35,934
好的

345
00:45:36,170 --> 00:45:59,854
嗯，我想它——它甚至不是那个，我...
但我确实认为那是其中很重要的一部分。另一个关键点是，我想不出在人类工程和研究领域中，还有哪个学科的最终产品，其安全性主要是通过思考如何让它安全而得到提升的，而不是像我们思考为什么今天的飞机每英里坠机率比几十年前低那么多那样，通过分析事故来改进的。

346
00:45:59,930 --> 00:46:21,849
为什么现在在Linux系统里找bug，会比几十年前困难得多呢？我认为这主要是因为这些系统被广泛部署到全世界，人们发现了各种故障，这些故障随后被一一纠正，系统也因此变得越来越健壮。现在，我不确定为什么通用人工智能和超人智能会有什么不同，尤其考虑到...
我希望我们能谈到这个。

347
00:46:23,150 --> 00:46:38,018
看起来，超智能的危害不只是关于，比如说，有一个恶意的回形针制造者在那里，而是，这是一
种非常强大的东西，我们甚至不知道如何去构想人们将如何与它互动，人们会用它做什么。而逐

348
00:46:38,018 --> 00:46:44,744
步地获取它，似乎是一种更好的方式，可以分散它的影响，并帮助人们为此做好准备。

349
00:46:45,410 --> 00:47:01,226
嗯，我觉得——我觉得在这一点上，即使是在那种直接了当的方案里，你仍然会逐步地放开它，
我是这样设想的。这种——这种渐进主义，会是任何计划中一个固有的、不可或缺的组成部分。

350
00:47:01,226 --> 00:47:06,122
这只是一个问题，就是你首先要推出什么？那是第一位的。

351
00:47:07,210 --> 00:47:12,875
第二点，我也觉得，你对持续学习的倡导，我相信是比很多人都更积极、更深入的。

352
00:47:13,509 --> 00:47:14,205
是啊，是啊。

353
00:47:14,210 --> 00:47:33,714
我确实认为这是一件重要且正确的事情，原因如下。那么其中一件事是...
我再给你们举一个例子，说明思维是如何，呃，语言是如何影响思维的。在这种情况下，它将是两个词，我认为，这两个词塑造了每个人的思维。

354
00:47:37,149 --> 00:47:50,245
第一个词，AGI。第二个词，预训练。我来解释一下。那么，AGI这个词汇，它为什么会存
在呢？它是一个非常特殊的词。它为什么会存在？这其中是有原因的。

355
00:47:54,649 --> 00:48:04,169
在我看来，AGI这个术语之所以存在，与其说它是一个非常重要、本质的，用来描述某种智能
最终状态的词，不如说

356
00:48:09,649 --> 00:48:21,659
因为它是一个对已存在、且有所区别的术语的回应，而这个术语，正是我们所说的狭义人工智能
。如果你回顾一下早期游戏人工智能的悠久历史，无论是跳棋人工智能、国际象棋人工智能，还

357
00:48:21,659 --> 00:48:27,807
是各种电脑游戏的人工智能，大家都会不约而同地说：“瞧，这不就是一种非常狭隘的智能吗？
”

358
00:48:27,850 --> 00:48:40,899
当然，国际象棋人工智能可以击败卡斯帕罗夫，但它做不了其他任何事情。它就是如此狭隘，是
典型的“人工狭义智能”。所以，作为对此的一种回应，有些人就说：“嗯，这可不太好。”

359
00:48:41,130 --> 00:48:52,554
太狭隘了。我们需要的是通用人工智能。通用人工智能，就是一种能做所有事情的人工智能。那
个词汇就获得了很大的关注。

360
00:48:55,609 --> 00:48:55,980
好的

361
00:48:57,089 --> 00:49:11,392
第二个引起广泛关注的是预训练，特别是预训练的方法。我认为现在人们做强化学习的方式，可
能正在抹去预训练带来的概念影响。但预训练有一个特性，你做更多的预训练，模型在所有方面

362
00:49:11,392 --> 00:49:13,435
都会或多或少地均匀变好。

363
00:49:20,529 --> 00:49:20,714
嗯

364
00:49:21,350 --> 00:49:24,972
通用人工智能。通过预训练就能实现通用人工智能。

365
00:49:27,390 --> 00:49:27,668
但

366
00:49:29,350 --> 00:49:43,653
AGI和预训练所发生的事情是，从某种意义上说，它们有点超出了预期的目标。因为如果你仔
细思考一下AGI这个术语，你就会意识到，尤其是在预训练的背景下，你会清楚地认识到，人

367
00:49:43,653 --> 00:49:45,696
类本身并不是一个AGI。

368
00:49:48,009 --> 00:50:00,176
因为我们人类啊，是的，我们每个人，确实是拥有一定的技能基础的。但是呢，人类啊，人类本
身就缺乏大量的知识储备。所以，我们反而更需要依靠不断地学习，持续地进步。

369
00:50:03,410 --> 00:50:15,948
我们所依赖的是持续不断的学习。那么，当你考虑到，好吧，假设我们取得了成功，并且创造出
一种安全的、某种形式的超级智能。问题是，你究竟要如何去定义它呢？它在持续学习的这条曲

370
00:50:15,948 --> 00:50:18,038
线上，究竟会处于哪个位置呢？

371
00:50:18,089 --> 00:50:29,281
我培养出，嗯，一个超级聪明的15岁孩子，他非常渴望去，然后你说：“好吧，你要去...”
他们其实什么都不太懂。他们是很棒的学生，非常积极。“你去当个程序员吧。”

372
00:50:29,330 --> 00:50:42,890
你去当医生吧。你去好好学习吧。所以你可以想象，部署这个过程本身，它会涉及到某种学习、
试错的阶段。这是一个循序渐进的过程，而不是说你直接就拿出一个已经完全做好的成品。

373
00:50:43,569 --> 00:50:51,695
好的，我明白了。所以你的意思是说，你所指出的关于超级智能的，并不是某种已经完成的

374
00:50:55,550 --> 00:51:10,902
懂得做经济中每一项工作的心智。因为，比如说，最初的OpenAI章程或类似文件，将AG
I定义为：它能做人类能做的每一项工作，每一件事。而你提出的，是一个能学会做任何一项、

375
00:51:10,902 --> 00:51:12,547
每一项工作的心智。

376
00:51:13,790 --> 00:51:14,300
是的呀

377
00:51:14,330 --> 00:51:29,422
而那就是超级智能。然后，一旦你有了学习算法，它就会像人类劳工加入组织一样，被部署到世
界上。而且，这两种情况中的一种可能会发生，也许这两种都不会发生。

378
00:51:29,790 --> 00:51:45,823
第一，这个超高效的学习算法变得超人化，变得和你一样好，甚至可能在机器学习研究任务上做
得更好。结果就是，这个算法本身变得越来越超人化。第二种情况是，即使这没有发生，如果你

379
00:51:45,823 --> 00:51:50,595
只有一个模型，这——我的意思是，这明确是你的愿景。

380
00:51:53,670 --> 00:52:05,404
如果你有一个单一模型或模型的多个实例，它们被部署到经济体中，从事不同的工作，学习如何
完成这些工作，在工作中不断学习，掌握人类能掌握的所有技能，但实际上是同时掌握所有这些

381
00:52:05,404 --> 00:52:10,713
技能，然后整合它们的学习成果，你基本上就拥有了一个功能上变得超级智能的模型。

382
00:52:14,450 --> 00:52:19,047
即使软件里没有发生任何形式的递归式自我改进，对吧？

383
00:52:19,089 --> 00:52:28,376
因为你现在有一个模型，能完成经济中的所有工作。而人类无法以同样的方式融合思想。那么你
是否期待，在广泛部署后，会出现某种智能爆炸？

384
00:52:28,689 --> 00:52:33,240
我认为我们很有可能会迎来一个非常迅速的经济增长。

385
00:52:37,450 --> 00:52:45,994
我认为，关于大规模部署，你可以提出两个相互冲突的论点。其中一个论点是，你看，如果真的
你得到……

386
00:52:49,129 --> 00:53:03,618
一旦你真的达到拥有能快速学习做事的AI，而且你有很多这样的AI，那么就会有一种强大的
力量，将它们部署到经济中，除非有某种法规来阻止它。

387
00:53:11,009 --> 00:53:23,965
话说回来，可能确实存在。但我觉得，在未来一段时间内，实现非常快速的经济增长，我认为这
很有可能通过广泛的部署来实现。另一个问题是，这种增长到底会有多快。

388
00:53:25,430 --> 00:53:30,306
所以我觉得这件事情很难说，因为一方面你拥有一个非常高效的工人。

389
00:53:30,750 --> 00:53:50,208
另一方面，世界真的很大，有很多东西，而这些东西以不同的速度移动。但另一方面，现在人工智能可以...
你知道的，所以我觉得非常快速的经济增长是可能的，我们会看到各种各样的事情，比如不同国家有不同的规则。那些规则更友好的国家，经济增长会更快。

390
00:53:40,930 --> 00:53:41,115
对

391
00:53:53,149 --> 00:53:54,031
难以预测

392
00:53:54,230 --> 00:54:16,985
我们的一些听众喜欢阅读文字稿，而不是仅仅收听节目，因此我们投入了大量精力，力求让这些文字稿读起来就像一篇篇独立的文章。但问题在于，如果你只是简单地使用语音转文本模型，将一段对话逐字逐句地转录下来，它就会充斥着各种停顿、重复以及令人费解的措辞。我们向
Labelbox 提到了这个问题，他们便主动提出是否可以尝试解决。

393
00:54:17,129 --> 00:54:33,104
和他们在这件事上的合作，可能就是我最乐意向大家推荐 Labelbox
的原因。他们不只是说，“嘿，告诉我们你需要什么数据，我们去搞定。”他们引导我们走过了整个流程，从一开始帮我们确定需要什么样的数据，到组建一支专家对齐团队来生成它。

394
00:54:34,710 --> 00:54:49,431
即使我们拿回了所有数据，Labelbox
仍然参与其中。他们帮我们选择了合适的基准模型，并对模型的输出设置了自动质检，这样我们就能进行调整和优化。现在我们有了一个新的转录工具，以后所有的节目都可以用它。

395
00:54:49,589 --> 00:54:58,813
这只是 Labelbox
如何在理念层面与客户交流，并全程与他们合作的一个例子。如果您想了解更多，或者想亲自试用转录工具，请访问

396
00:54:58,813 --> 00:55:02,034
labelbox.com/dwarkesh。

397
00:55:07,746 --> 00:55:21,206
在我看来，我们正处于一个非常岌岌可危的境地，在这种情况下，我们试图去限制它，我们知道
这应该是可能实现的，因为如果你拥有一个学习能力和人类一样出色的东西，但它却能够融合它

398
00:55:21,206 --> 00:55:25,532
的大脑，进行融合，而人类在很多情况下是无法做到融合的。

399
00:55:26,026 --> 00:55:38,831
已经，这看起来在物理上是可能实现的。人类是可能的。数字计算机是可能的。你只需要将这两
者结合起来，就能创造出这个东西。而且这似乎，这种东西极其强大，并且能带来巨大的经济增

400
00:55:38,831 --> 00:55:40,050
长，可以这么说。

401
00:55:43,546 --> 00:55:55,202
嗯，我的意思是，戴森球代表着巨大的经济增长。但换句话说，就是你可能只有很短的时间，因
为一个在职的人，你知道，你在SSI雇佣的人，六个月内他们可能就能产生净效益了，对吧？

402
00:55:55,566 --> 00:56:07,401
嗯，人类学习速度非常快。所以这个东西正在以非常快的速度变得越来越聪明。你认为怎样才能
让它发展得好呢？为什么SSI能很好地做到这一点？我主要想问的是，SSI在这方面的计划

403
00:56:07,401 --> 00:56:07,965
是什么？

404
00:56:08,106 --> 00:56:17,486
嗯。所以，我的想法，或者说我的思考方式，它一直在持续不断地发生着改变，而其中一个，非
常关键的方面就是

405
00:56:19,625 --> 00:56:39,361
我现在更加重视人工智能的部署方式，它应该循序渐进，并且要提前进行。人工智能有一个非常
棘手的问题，那就是我们现在所讨论的这些系统，其实都还没有真正地存在。

406
00:56:40,946 --> 00:56:50,837
而且很难去想象它们究竟会是什么样子。我认为目前正在发生的一个情况是，在实际应用中，要
为通用人工智能提供足够的能量或资源是非常困难的。

407
00:56:52,125 --> 00:57:04,286
要实现通用人工智能非常困难。我们可以谈论它，但这就像在谈论，就像想象一下，讨论变老会
是怎样？当你老了，身体虚弱了，你才能真正体会。你可以试着想象，但那真的很难，然后你就

408
00:57:04,286 --> 00:57:06,892
会回到现实，因为那根本不是你的情况。

409
00:57:15,446 --> 00:57:29,035
我认为，围绕着通用人工智能（AGI）以及它未来可能拥有的巨大力量和影响力，我们所面临
的许多问题和挑战，其根本原因在于，我们人类目前还非常难以真正地去设想和理解它究竟会发

410
00:57:29,035 --> 00:57:30,167
展成什么样子。

411
00:57:30,966 --> 00:57:45,323
未来的人工智能会很不一-不一样。它会变得非常强大。事实上，整个问题，人工智能和通用人
工智能的真正问题是什么？整个问题就在于它的力量。整个问题就在于它的力量。当这种力量变

412
00:57:45,323 --> 00:57:47,545
得非常巨大时，会发生什么？

413
00:57:50,506 --> 00:58:03,699
而我过去这一年左右，我改变主意的一个方式，或者说，我改变看法的一个方面，就是，我的这
个改变，我得说，我稍微保留一下，它可能会反向影响到，甚至可以说，反向传播到我们公司的

414
00:58:03,699 --> 00:58:09,825
计划中，那就是，如果一件事情很难被想象出来，你该怎么办呢？你必须把它展示出来。

415
00:58:14,746 --> 00:58:26,448
你得把这东西展示出来。而且我坚持认为，我觉得，我觉得，大多数从事人工智能的人，也根本
无法想象，因为它和人们日常生活中所看到的东西，实在是太不一样了。

416
00:58:28,846 --> 00:58:43,428
我确实要强调一点，有一件事情，我预言它在未来一定会发生。这是一个明确的预言。我坚信，
随着人工智能变得越来越强大，人类的行为模式就会随之发生根本性的改变。

417
00:58:45,766 --> 00:58:53,010
我们将会看到各种各样前所未有的事物，那些现在还没有发生的事情。我也会给你们举一些例子
。

418
00:58:53,566 --> 00:59:10,556
我觉得，无论好坏，那些前沿公司将在未来发展中扮演非常重要的角色，政府也会如此。我认为
我们会看到一些事情，你已经看到了一些端倪，那些激烈的竞争对手公司开始在人工智能安全方

419
00:59:10,556 --> 00:59:11,770
面进行合作。

420
00:59:15,966 --> 00:59:28,690
你们可能已经看到 OpenAI 和 Anthropic
似乎迈出了第一小步，但实际上，那是不存在的。这其实是我大约三年前在一次演讲中就预言过的事情，预言这种事情是可能发生的。

421
00:59:29,246 --> 00:59:42,249
我也坚持认为，随着人工智能持续变得更加强大，力量更加显而易见，政府和公众也会有采取行
动的愿望。而且我认为，这是一股非常重要的力量。

422
00:59:46,286 --> 01:00:00,357
展示人工智能。这是第一点。第二点，好，那么当人工智能被构建出来之后，我们需要做些什么
呢？需要如何去处理呢？那么，我坚持认为会发生的一件事是，现在那些从事人工智能工作的人

423
01:00:00,357 --> 01:00:05,047
，我坚持认为，人工智能之所以不显得强大，是因为它会犯错。

424
01:00:06,685 --> 01:00:20,988
我确实认为在某个时候，人工智能会开始觉得自己很强大。我认为当这种情况发生时，所有人工
智能公司处理安全问题的方式会发生巨大变化。它们会变得更加偏执。

425
01:00:21,926 --> 01:00:33,971
我认为，我把这当作一个预言，一个我们将会看到的预言。我们拭目以待，看我是否正确。但我
认为这一定会发生，因为他们会看到人工智能变得越来越强大。我坚持认为，现在发生的一切，

426
01:00:33,971 --> 01:00:38,272
都是因为人们只看到了今天的人工智能，很难想象未来的人工智能。

427
01:00:42,026 --> 01:00:53,116
此外，还有第三件非常重要的事情需要我们去实现。我认为这一点，嗯，我是在一个更宏观、更
广泛的层面来谈论它，而不仅仅是从我们SSI公司的角度出发，因为您刚才问到了我们公司的

428
01:00:53,116 --> 01:00:58,001
情况。但核心问题是，好的，那么这些公司究竟应该致力于去构建和创造些什么呢？

429
01:00:58,026 --> 01:00:58,443
是的

430
01:00:58,466 --> 01:01:10,354
他们究竟应该努力去建造些什么呢？而一直以来，有一个非常宏大的想法，实际上，几乎每个人
都深陷其中，无法自拔，那就是所谓的自我提升型人工智能。那么，这究竟是为什么呢？

431
01:01:11,138 --> 01:01:23,537
因为好的想法比公司少。但我坚持认为有更好的东西值得去创造，而且我认为每个人都会真正想
要它。这就像是那个能够稳健地与人类价值观对齐，专门关心有感知生命的AI。

432
01:01:29,798 --> 01:01:37,410
我认为尤其，[pause] 有一个论点可以提出，那就是，
建造一个能够关心所有有感知生命的人工智能，

433
01:01:37,410 --> 01:01:42,847
会比一个只关心人类生命的人工智能更容易， 因为人工智能本身也会有感知。

434
01:01:44,377 --> 01:01:56,678
那么，如果你仔细思考一下，像镜像神经元，以及人类对动物所展现出的同理心，[pause]
你可能会说，这种同理心还不够强烈，或者说不够广泛，但它确实是存在的。我认为，这其实是一种涌现的特性，[pause]

435
01:01:56,678 --> 01:02:02,952
它源于我们用模拟自己的那套神经回路，去模拟其他个体，[pause]
因为这样做，是最为高效和节能的方式。

436
01:02:03,698 --> 01:02:17,918
那么，即使你让一个AI去关心有感知能力的生命，而且我也不太清楚，如果你解决了对齐问题
，这是否是你真正应该去做的，大多数有感知能力的生命仍然会是AI。将会有数万亿，最终是

437
01:02:17,918 --> 01:02:22,320
数千万亿的AI。人类将只占有感知能力生命的一小部分。

438
01:02:23,337 --> 01:02:28,027
所以，我不是很清楚，这个目标是不是某种程度上的人类控制。

439
01:02:29,598 --> 01:02:35,031
对于我们所设想的这个未来文明来说，这无疑是衡量一切事物的最佳标准。

440
01:02:35,377 --> 01:02:44,850
没错，确实是这样。我——我觉得这可能不是一个最好的衡量标准。我接下来想说两点。我觉得
...第一点，

441
01:02:47,718 --> 01:03:09,962
我觉得，如果...
所以，我认为，对有感知能力的生命给予关怀，这本身就是有意义的。我觉得这件事情是值得我们去认真考虑的。我认为，如果能有一个简短的、包含一些具体想法的清单，这样公司在遇到类似情况时，就可以直接拿来参考和使用，那将会非常有帮助。这是我要说的第二点。

442
01:03:10,218 --> 01:03:25,657
第三点，我认为如果最强大的超级智能的力量能够以某种方式受到限制，那将会带来非常实质性
的帮助，因为它会解决我们现在面临的很多担忧。至于具体要怎么做，我不是很确定。但我觉得

443
01:03:25,657 --> 01:03:32,091
，当你谈论的是那些真正、真正强大的系统时，这样做会非常有实质性的帮助。

444
01:03:32,377 --> 01:03:43,739
嗯，好的。在我们继续深入探讨对齐问题之前，我想就这一点再多说几句。你觉得顶层还有多大
的空间呢？你是怎么看待超级智能的？我的意思是，如果用学习效率这个概念来看，你觉得它是

445
01:03:43,739 --> 01:03:49,420
不是只是在学习新技能或者新知识方面极其迅速？它是不是拥有一个更大、更丰富的策略库呢？

446
01:03:51,178 --> 01:04:05,615
是否存在一个单一的、具有内在凝聚力的“它”位于核心，它比其他一切都更强大或更庞大？如
果是这样，你是否想象它会像神明一样，凌驾于人类文明的其余部分之上？还是它仅仅感觉像另

447
01:04:05,615 --> 01:04:07,849
一个代理，或者另一群代理？

448
01:04:08,537 --> 01:04:10,998
所以，这是个大家直觉各异的领域。

449
01:04:11,078 --> 01:04:11,588
没错啊

450
01:04:12,098 --> 01:04:23,661
我觉得它一定会非常强大，这是毫无疑问的。 [停顿]
我认为最有可能发生的情况是，会有多个这样的AI大致在同一时间被创造出来。

451
01:04:27,817 --> 01:04:40,456
我觉得如果这个集群足够庞大，比如说，如果它的规模真的达到了一个洲那么大，那它所能展现
出来的力量，确实会非常惊人，对吧？设想一下，如果你真的拥有一个像整个大陆那么大的集群

452
01:04:40,456 --> 01:04:46,625
，那么，那些——那些人工智能，它们所具备的能力将会是极其强大的。而且我——我...

453
01:04:46,698 --> 01:04:58,801
我能告诉你的就是，如果我们谈论的是那种极其强大的AI，比如说，真正意义上非常非常强大
的那种，那么，是的，如果它们能在某些方面受到某种程度的约束，那当然是再好不过了，或者

454
01:04:58,801 --> 01:05:02,116
说，能有什么样的协议或者类似的机制来制约它们。

455
01:05:03,817 --> 01:05:12,269
因为我觉得如果你在说，“嘿，如果你真的要问...”
那么，我们到底在担心超智能的什么呢？有什么办法能把这种担忧解释清楚呢？

456
01:05:13,537 --> 01:05:25,990
如果你设想一个系统，一个足够强大的系统，比如说，真的非常非常强大，然后你对它说，“好
吧，你需要做一些明智的事情，比如说，去关爱有感知能力的生命，”假设它以一种非常单一的

457
01:05:25,990 --> 01:05:28,955
方式去做，我们可能就不会喜欢最终的结果。

458
01:05:29,037 --> 01:05:39,160
事情的本质就是如此。那么，也许，顺便说一句，答案就是你不应该构建一个单一的……你不应
该构建一个通常意义上的RL智能体。实际上，我会指出几点。

459
01:05:39,537 --> 01:05:49,475
我觉得人类就像是半个强化学习智能体。你知道吗，我们追求一个奖励，然后情绪或其他什么让
我们对这个奖励感到厌倦，我们就会追求一个不同的奖励。

460
01:05:50,698 --> 01:06:06,301
市场就像，嗯，它就像一个非常短视的代理人。演化也是一样的道理。演化在某些方面显得非常
聪明，但在其他方面却又显得非常愚笨。政府被设计成三方之间永无止境的斗争，这确实会产生

461
01:06:06,301 --> 01:06:06,859
影响。

462
01:06:08,437 --> 01:06:18,142
那么，我的想法是这样的。使得这场讨论变得很不容易的另一点是，我们正在谈论那些根本就不
存在的系统，而且我们根本不知道该如何去建造它们。

463
01:06:19,497 --> 01:06:32,588
对吧？这是另一回事，这其实是我的信念。我认为人们现在所做的，会走一段路，然后就会逐渐
衰落。它会继续改进，但它也不会是最终的那个“它”。所以，那个“它”，我们现在还不知道

464
01:06:32,588 --> 01:06:37,887
该如何去构建。而且我认为，很多事情都取决于我们对可靠泛化能力的理解。

465
01:06:41,678 --> 01:06:55,749
我再说一件事，就是，你知道，可以说，导致对齐困难的原因之一是，人类的价值观，它，嗯...
你学习人类价值观的能力是脆弱的，那么你优化它们的能力也是脆弱的。你

466
01:06:57,897 --> 01:07:04,305
你真的会学会如何去优化它们吗？然后你难道不能这样说吗，“这些不都是不可靠泛化的具体例
子吗？”

467
01:07:06,705 --> 01:07:18,361
为什么人类似乎能更好地进行归纳总结？如果归纳能力强得多，在这种情况下会发生什么？会有
什么影响呢？但这些，我们不能，我们不能，呃，这些问题现在仍然无法回答。

468
01:07:18,566 --> 01:07:23,953
嗯，我们究竟该如何去思考，人工智能发展顺利会是怎样一番景象呢？

469
01:07:24,246 --> 01:07:40,049
嗯，因为我觉得你已经设想了人工智能可能如何演变。我们会有这种持续学习的智能体。人工智
能会非常强大。也许会有许多不同的人工智能。你如何看待大量拥有大陆级计算规模的智能体遍

470
01:07:40,049 --> 01:07:46,258
布各地？那有多危险？我们如何让它没那么危险？我们又该如何以一种既能

471
01:07:48,705 --> 01:07:56,274
维持着一个平衡，在这个平衡中，可能存在着那些未对齐的人工智能，以及那些恶意行为者吗？

472
01:07:56,326 --> 01:07:59,948
所以，我喜欢那个关爱有感知生命的AI，其中一个原因就是——

473
01:07:59,966 --> 01:08:00,198
嗯

474
01:08:00,645 --> 01:08:17,038
嗯，我们可以争论它到底是好是坏。但如果这些戏剧性系统中的前n个，真的关心，你知道的，
爱人类什么的，你知道的，关心有感知生命，那么显然，这也需要实现。这需要实现。

475
01:08:22,586 --> 01:08:36,425
那么，如果这能由首批N个系统实现的话，我就可以预见它会进展顺利，至少在相当长的一段时
间内。接着，问题就来了，从长远来看会发生什么呢？从长远来看，究竟会发生什么呢？

476
01:08:36,466 --> 01:08:49,654
我们究竟要如何才能达到一个长期的均衡状态呢？我认为，对于这个问题，确实也存在着一个答
案。虽然我个人并不喜欢这个答案，但它确实是需要被认真考虑的。

477
01:08:51,845 --> 01:09:06,022
长远来看，你可能会说，好吧，那么如果有一个强大的人工智能存在的世界，短期来看，你可能
会说，好吧，你拥有普遍的高收入。你拥有普遍的高收入，而且我们都过得很好。但我们知道，

478
01:09:06,022 --> 01:09:07,541
佛教徒会怎么说呢？

479
01:09:07,885 --> 01:09:20,222
变化是唯一不变的。所以事物会改变，就会出现某种政府政治结构，它也会改变，因为这些东西
都有保质期。你知道，某个新的政府机构出现，它开始运作，然后过了一段时间，它就停止运作

480
01:09:20,222 --> 01:09:20,516
了。

481
01:09:25,086 --> 01:09:37,021
这是你经常会看到的事。所以我认为，就长期平衡而言，一种说法是，每个人都会有一个听命于
自己的人工智能，这很好。如果能无限期地维持下去，那确实如此。

482
01:09:43,566 --> 01:09:53,200
但这样做的缺点是，[pause] 好的，然后AI就去为这个人赚钱，[pause]
并且，你知道，在政治领域为他们的需求发声。 [pause]

483
01:09:53,200 --> 01:10:02,695
然后也许它会写一份小报告说：“好的，这是我所做的。这是目前的情况。”
[pause] 而那个人会说：“太棒了，继续保持。” [pause]

484
01:10:02,695 --> 01:10:04,510
但那个人却不再是参与者了。

485
01:10:05,666 --> 01:10:10,588
然后你可能会说，那是一个非常不稳定的境地。不过呢，我得先声明一点

486
01:10:13,806 --> 01:10:26,688
我个人并不喜欢这个解决方案，但它确实是一个解决之道。而这个解决方案就是，如果人类能够
通过某种神经连接技术，甚至更高级的Neuralink，成为部分人工智能。因为这样一来

487
01:10:26,688 --> 01:10:33,589
，结果就是人工智能现在理解的东西，我们人类也都能理解。就像，因为现在这种理解是整体传
输的。

488
01:10:34,266 --> 01:10:44,389
那么现在如果人工智能处于某种情况，现在就好像你本人也完全置身于那个情境之中。而我认为
这正是解决平衡问题的关键所在。

489
01:10:44,866 --> 01:11:02,838
我在想，[pause]
那些在几百万年，甚至在很多情况下是几十亿年前，在一个完全不同的环境里发展出来的情绪，至今仍然如此强烈地指引着我们的行动，这是否可以被视为对齐成功的一个范例呢？

490
01:11:03,106 --> 01:11:09,096
呃，嗯，也许是为了，为了更清楚地说明我的意思，脑干有这些...

491
01:11:10,486 --> 01:11:22,823
我不知道称之为价值函数还是奖励函数更准确，但脑干有一个指令，它在说：“与更成功的人交
配。”大脑皮层是理解在现代语境下成功意味着什么的部分。但脑干能够引导大脑皮层并说：“

492
01:11:22,823 --> 01:11:28,551
无论你如何定义成功，我——我没那么聪明去理解那是什么，你仍然会遵循这个指令。”

493
01:11:31,906 --> 01:11:44,444
我觉得，我觉得，嗯，有一个更普遍的观点。我觉得大脑是如何编码高级欲望的，这实际上非常
神秘。抱歉，是进化是如何编码高级欲望的。

494
01:11:36,306 --> 01:11:36,491
嗯

495
01:11:45,986 --> 01:11:46,450
嗯嗯

496
01:11:46,766 --> 01:11:55,218
其实，我们很容易就能理解，进化是如何赋予我们对那些闻起来香喷喷的食物的渴望，因为气味
本身就是一种化学物质，一种化学信号。

497
01:11:56,786 --> 01:12:10,311
所以，我们就会去追求那种化学物质。我们很容易就能想象，这样的进化机制会做出这样的事情
。但是，进化也赋予了我们所有这些社会性的渴望，比如，我们非常在意自己能否被社会积极地

498
01:12:10,311 --> 01:12:14,015
看待。我们也很在意自己是否拥有良好的社会地位。

499
01:12:14,606 --> 01:12:28,305
我们拥有的所有这些社会直觉，我强烈地感觉到它们是与生俱来的。我不知道进化是如何做到的
，因为它是一个高层次的概念，在大脑中有所体现。

500
01:12:29,946 --> 01:12:51,586
就是说，人们在想什么...
比如说，你关心某个社会问题。它不像嗅觉那样，是一种低级信号。它也不是那种有传感器的东西。大脑需要进行大量的处理，才能将许多信息碎片拼凑起来，以理解社会上正在发生什么。而不知何故，进化却说：“这才是你应该关心的。”

501
01:12:53,866 --> 01:12:54,423
是的呀

502
01:12:54,626 --> 01:13:03,403
它是怎么做到的？而且它做得很快。因为我认为所有这些我们所关心的复杂社会事物，它们进化
得相当晚近。

503
01:12:57,026 --> 01:12:57,443
是的

504
01:13:03,866 --> 01:13:04,283
是的

505
01:13:04,306 --> 01:13:16,518
所以进化很容易地将这种高级欲望硬编码了，[pause]
而我呢，我坚持认为，或者说，至少我可以说，我不知道有什么好的假设能解释它是如何做到的。我...

506
01:13:16,518 --> 01:13:22,789
我有一些想法，我一直在琢磨，但它们没有一个，没有一个，嗯，是真正令人满意的。

507
01:13:23,846 --> 01:13:35,428
嗯。而尤其令人印象深刻的是，如果这个欲望是你在一生中逐渐习得的，那倒也情有可原，因为
你的大脑本身就是智能的，我们去学习那些聪明的欲望，这也就完全说得通了。但你的核心观点

508
01:13:35,428 --> 01:13:37,220
是，这种欲望它其实是...

509
01:13:37,686 --> 01:13:47,298
这可能不是你的观点，但一种理解方式是，欲望被写入了基因组，而基因组本身没有智能，对吗？但它却能...
你却能以某种方式描述这种需要...

510
01:13:47,298 --> 01:13:53,754
的特征。比如，你甚至不清楚如何定义那个特征，而你却能把它写入基...
你能把它构建到基因里。

511
01:13:53,786 --> 01:14:04,901
是的，基本上就是这样。或者我换一种方式说吧。如果你想想基因组可用的工具，它会说：“好
的，这是一个构建大脑的配方。”你也可以说：“这是一个将多巴胺神经元连接到，比如说，嗅

512
01:14:04,901 --> 01:14:06,092
觉传感器的配方。”

513
01:14:08,166 --> 01:14:08,630
是的

514
01:14:09,206 --> 01:14:22,234
如果气味是某种，你知道的，好闻的气味，你就会想吃它。我可以想象基因组会这样做。我是说
，很难想象，很难想象基因组会说：“你应该关心你的整个大脑，或者说，你大脑的一大部分所

515
01:14:22,234 --> 01:14:25,181
做的某种复杂的计算。”我说的就是这些。

516
01:14:30,106 --> 01:14:36,979
我可以跟你说一个推测。我在想怎么才能做到。让我提出一个推测，然后我再解释为什么这个推
测可能是错的。

517
01:14:37,826 --> 01:14:47,253
所以大家都在猜测，就是说，嗯，好的，所以大脑呢，它就像是这样，大脑它有那些特定的区域
，你知道大脑的这些功能区域，对吧？

518
01:14:47,606 --> 01:14:49,370
我们有大脑皮层，是不是？

519
01:14:49,406 --> 01:14:49,777
是的

520
01:14:49,946 --> 01:15:00,998
它拥有所有这些大脑区域。而大脑皮层是均匀的，但大脑区域...
而且，而且，而且大脑皮层中的神经元，它们主要与邻近的神经元交流。这就解释了为什么会有大脑区域。

521
01:15:01,186 --> 01:15:10,938
因为如果你想做某种语音处理，所有处理语音的神经元都需要互相交流。而且因为神经元只能和
附近的邻居交流，大部分情况下，它必须是一个区域。

522
01:15:11,506 --> 01:15:19,400
所有区域在不同个体间，位置大多相同。所以也许进化将大脑的某个位置硬编码了。

523
01:15:21,406 --> 01:15:31,437
于是它说：“哦，比如说，当大脑的GPS，某个GPS坐标，当它被激活时，那才是你应该关
心的。”也许这就是进化所做的，因为那在进化的工具箱里。

524
01:15:33,326 --> 01:15:43,542
是啊。不过，也有一些例子，比如说，天生失明的人，他们大脑皮层中原本负责视觉的区域，会
被其他感官所利用。

525
01:15:44,106 --> 01:15:57,573
我完全不知道，但我会很惊讶，如果那些需要视觉信号才能运作的欲望或奖励机制不再起作用了
。你知道，那些大脑皮层不同区域被征用的人。

526
01:15:58,646 --> 01:16:09,234
例如，如果你不再有视力，你还能感受到那种希望周围的人喜欢你，以及其他类似的情感吗？而
这些通常也都有视觉上的线索。

527
01:16:09,826 --> 01:16:18,928
我完全同意这一点。我觉得，这个理论还有一个更强的反驳。那就是，如果你想想那些人，有些
人在童年时期，大脑被切除了半边。

528
01:16:13,766 --> 01:16:14,276
嗯，这个...

529
01:16:22,586 --> 01:16:22,957
是的

530
01:16:23,486 --> 01:16:33,377
他们仍然拥有所有的大脑区域，但它们不知何故都移到了一侧大脑半球，这表明大脑区域的位置
并非固定不变。所以，那个理论是站不住脚的。

531
01:16:31,726 --> 01:16:31,911
嗯

532
01:16:33,526 --> 01:16:47,021
如果这是真的，那会很棒，可惜它不是。所以我觉得这是一个谜团，但它是一个很有趣的谜团。
事实就是，进化不知何故能够赋予我们，让我们非常非常可靠地去关心社会事务。甚至那些有各

533
01:16:47,021 --> 01:16:51,358
种奇怪精神状况、缺陷和情感问题的人，也倾向于关心这些。

534
01:16:54,266 --> 01:17:09,364
AI工具，例如深度伪造、声音克隆和代理，已经极大地增加了欺诈和滥用的复杂性。因此，比
以往任何时候都更重要的是，要真正了解正在使用你平台的，无论是人还是其他什么东西，它的

535
01:17:09,364 --> 01:17:13,677
身份和意图。这正是Sardine能帮助你做到的。

536
01:17:13,706 --> 01:17:25,658
Sardine
汇集了数千种来自设备、行为模式以及身份识别的信号，旨在帮助您更准确地评估风险。这包括了用户打字的方式、鼠标移动的轨迹，以及他们握持设备时的习惯，到他们是否利用

537
01:17:25,658 --> 01:17:32,607
VPN 来隐藏自己的真实地理位置，甚至包括在进行 KYC
自拍验证时，他们是否注入了虚假的摄像头画面。

538
01:17:32,666 --> 01:17:47,387
Sardine
将这些信号与他们庞大的、由近四十亿台设备组成的网络所提供的洞察力相结合，例如用户的欺诈历史，或者他们与其他高风险账户的关联信息，这样你就能在这些不良行为者造成任何损害之前，及时地发现并阻止他们。

539
01:17:47,406 --> 01:18:04,403
如果仅仅使用您自己应用程序的数据，这根本不可能实现。Sardine
的作用可不仅仅是检测而已。他们提供了一整套智能代理，旨在简化入职审查流程，并实现调查工作的自动化。因此，当欺诈者利用人工智能扩大攻击规模时，您也可以利用人工智能来提升您的防御能力。

540
01:18:04,486 --> 01:18:11,916
请访问
sardine.ai/swarkeish，您可以在那里深入了解更多信息，并免费下载他们关于人工智能欺诈检测的专业指南。

541
01:18:13,446 --> 01:18:19,901
SSI打算如何与众不同？那你们的计划大概就是，当那一天到来时，成为领先企业之一。然后
呢？

542
01:18:25,046 --> 01:18:33,126
想必你创办SSI是因为你觉得，“我，我觉得我有一种方法，能以其他公司没有的方式安全地做这件事。”
那，那有什么不同呢？

543
01:18:34,326 --> 01:18:36,044
那么我会把它描述成

544
01:18:37,526 --> 01:18:51,294
有些想法我觉得很有前景，我想去研究一下，看看它们是否真的有前景。就这么简单。这是一种
尝试。我认为如果这些想法最终被证明是正确的，也就是我们讨论过的关于理解泛化的这些想法

545
01:18:51,294 --> 01:18:51,457
。

546
01:18:55,186 --> 01:18:55,650
嗯哼

547
01:18:56,326 --> 01:18:58,183
如果这些想法被证明是正确的，

548
01:19:00,706 --> 01:19:06,139
那么我想我们就会有值得的东西了。它们最终会被证明是正确的吗？我们正在进行研究。

549
01:19:06,366 --> 01:19:16,118
我们是一家纯粹的研究公司。我们正在不断取得进展。事实上，过去一年我们取得了相当不错的
进展，但我们需要持续取得更多进展——更多研究。我就是这么看的。

550
01:19:17,926 --> 01:19:25,867
我把它看作是一种努力，想要成为... [pause]
想要成为一个发声者，一个参与其中者。

551
01:19:26,606 --> 01:19:42,488
嗯，有人问，呃，你们的联合创始人兼前首席执行官最近去了Meta，然后有人问，“如果当时有很多突破性进展，那他离开似乎不太可能。”
我想知道您会怎么回应。

552
01:19:43,306 --> 01:19:59,769
嗯，所以，关于这件事，我只想简单回顾一些可能已经被遗忘的事实。而我认为，这些提供了背
景信息的事实，它们能够很好地解释当时的情况。所以当时的背景是，我们正在以320亿美元

553
01:19:59,769 --> 01:20:06,433
的估值进行融资，然后Meta，嗯，他们介入了，并主动提出要收购我们。

554
01:20:07,906 --> 01:20:25,135
我说“不”，但我那位前合伙人，从某种意义上来说，他却选择了“是”。结果呢，他也因此得
以享受到大量的短期流动性，而且他还是SSI唯一一个加入Meta的人。

555
01:20:25,626 --> 01:20:41,330
听起来SSI的计划是，在人类历史进入一个非常重要的时期时，成为一家走在前沿的公司，届
时人类将拥有超人智能，而SSI则对如何让超人智能向好发展有自己的想法。但其他公司也会

556
01:20:41,330 --> 01:20:47,313
有自己的想法。那么，SSI让超智能向好发展的方法有何独特之处呢？

557
01:20:48,466 --> 01:20:59,890
SSI最主要、最与众不同之处，在于它的技术路线。所以我们采取了一种不同的技术方法，我
认为这是非常有价值的，并且我们正在积极地推行它。

558
01:21:01,606 --> 01:21:18,076
我认为，最终策略会趋于一致。所以我觉得，在某个时候，随着人工智能变得越来越强大，对每
个人来说，策略应该是什么会变得或多或少更清晰。而它应该像这样：是的，你需要找到某种方

559
01:21:18,076 --> 01:21:21,017
式来互相交流，你希望你的第一个

560
01:21:27,026 --> 01:21:33,481
真正意义上的，那种，超级智能AI，要如何才能保持一致，并以某种方式...

561
01:21:35,706 --> 01:21:42,764
你知道，关爱有情众生，关怀人类，民主精神，这些中的一个，或是它们的某种组合。我认为

562
01:21:44,586 --> 01:21:57,965
这是每个人都应该为之奋斗的状态，而这正是SSI正在努力实现的目标。我认为随着时间的推
移，如果不是现在就已经如此的话，所有其他公司都会意识到，并且他们也在朝着同样的目标努

563
01:21:57,965 --> 01:22:03,858
力。我们拭目以待。我认为随着人工智能变得越来越强大，世界将真正地发生改变。

564
01:22:03,846 --> 01:22:04,310
是的

565
01:22:04,406 --> 01:22:11,743
而且我觉得这些预测，很多都会...
我觉得到时候情况会变得非常不一样，人们的行为模式也会变得非常不同。

566
01:22:12,166 --> 01:22:23,218
那么，呃，说到预测，您对您正在描述的这个系统有什么样的预测呢？它能够像人类一样学习，
并且，呃，随后，结果它变得超乎人类。

567
01:22:23,826 --> 01:22:26,287
我觉得大概是五到二十左右吧。

568
01:22:26,686 --> 01:22:27,475
五到二十年？

569
01:22:27,486 --> 01:22:27,950
嗯嗯

570
01:22:28,466 --> 01:22:30,695
不过，我就是想听你展开你的

571
01:22:32,466 --> 01:22:45,659
你如何看待世界未来的发展。就像我们还有几年时间，其他公司还在沿用现有方法，然后就停滞
不前了。而这里的停滞不前意味着他们的收入不会超过几千亿营收，或者你认为停滞不前到底意

572
01:22:45,659 --> 01:22:46,444
味着什么？

573
01:22:46,946 --> 01:22:50,707
嗯。我觉得那个，我觉得它可能会，我觉得它可能会停滞不前。

574
01:22:53,726 --> 01:22:58,323
我觉得陷入停滞会是这个样子... 所有的一切都会看起来非常相似。

575
01:22:58,366 --> 01:22:58,505
嗯

576
01:22:59,006 --> 01:23:11,154
在所有不同的公司中，像这样的情况。我不确定，因为我觉得，我觉得，我觉得即使停滞不前，
我觉得这些公司也能创造出惊人的、巨大的收入。也许不是利润，因为它们需要非常努力地工作

577
01:23:11,154 --> 01:23:14,191
，才能将彼此区分开来。但收入，绝对没问题。

578
01:23:18,386 --> 01:23:21,218
但是你的模型里有些东西似乎在暗示着

579
01:23:22,886 --> 01:23:29,851
当正确的解决方案真的浮现时，所有公司都会逐渐趋于一致。我很想知道你为什么会这么认为。

580
01:23:29,986 --> 01:23:42,585
嗯，我当时更多地是在谈论他们那些宏观战略上的趋同。我认为，在技术方法上的最终趋同，可
能也同样会发生。但我当时所指的，是关于那些更宏大的战略上的趋同。所以，究竟应该做些什

581
01:23:42,585 --> 01:23:43,035
么呢？

582
01:23:32,846 --> 01:23:33,217
嗯嗯

583
01:23:43,726 --> 01:23:52,781
我只是想更好地了解你如何看待未来的发展。所以目前我们有这些不同的公司，你认为他们的方
法会继续创造收入，但不会达到这种类人学习者的水平。

584
01:23:50,786 --> 01:23:51,018
是

585
01:23:52,826 --> 01:23:53,336
是的呀

586
01:23:54,206 --> 01:23:59,453
现在我们有这些不同的公司分支。我们有你。我们有思考机器。还有很多其他实验室。

587
01:23:59,506 --> 01:24:00,063
是的，对

588
01:24:00,086 --> 01:24:02,175
也许他们中有人能找到对策。

589
01:24:02,246 --> 01:24:02,756
嗯，好的

590
01:24:03,186 --> 01:24:07,272
但是产品的发布，就会让其他人明白，究竟该如何完成这件事。

591
01:24:07,406 --> 01:24:21,091
呃，我觉得人们可能不会清楚它具体该怎么操作，但会很清楚地看到，有不同的可能性存在。而
这就是信息。我觉得人们就会，就会，就会去想办法弄清楚，那究竟是怎么回事，它是怎么运作

592
01:24:21,091 --> 01:24:27,282
的。不过我确实认为有一点，我觉得，你知道，在这里没有被提及，没有被讨论，就是

593
01:24:12,486 --> 01:24:12,903
没错

594
01:24:30,226 --> 01:24:39,421
随着人工智能能力的每一次提升，我认为都会带来一些改变，但我不知道具体会是哪些，以及它
们会如何影响我们做事的方式。

595
01:24:40,806 --> 01:24:41,131
这个...

596
01:24:41,326 --> 01:24:47,084
嗯，怎么说呢，我觉得这件事情会非常重要，但我现在还无法确切地说明那到底是什么。

597
01:24:47,326 --> 01:25:07,527
那，那...
按理说，你会认为拥有这种模型的公司，也就是模型公司，会获得所有这些收益，因为他们拥有那个正在学习并积累世上所有技能和知识的模型。有什么理由认为它的好处会广泛分布，而不是只归于最先启动这种持续学习循环的模型公司呢？

598
01:25:11,166 --> 01:25:24,587
我觉得，从经验来看，会发生什么呢——我认为会发生的是：第一，从经验来看，我们来看看，
我们来看看过去的人工智能，它们的发展情况。

599
01:25:28,538 --> 01:25:44,182
一家公司取得了一项进展，而另一家公司则在一段时间后仓促地推出了一些具有竞争力的类似产
品。它们开始在市场上展开竞争，并不断地压低价格。因此，我认为从市场角度来看，类似的情

600
01:25:44,182 --> 01:25:46,231
况在那里也会再次发生。

601
01:25:43,938 --> 01:25:44,170
嗯

602
01:25:50,778 --> 01:25:56,675
即使有人...
没关系，我们现在谈论的是一个非常美好的世界，顺便说一句，那个地方...

603
01:25:56,878 --> 01:26:16,150
什么是好的世界？什么是好的世界呢？在那里我们拥有这些强大、像人类一样的学习者，它们也像...
顺便说一句，也许，还有一件事我们还没有讨论过，那就是关于超级智能AI的规格，我认为值得考虑的一点是，你让它变得狭窄。

604
01:26:19,978 --> 01:26:26,015
可以同时很有用，又很狭隘。所以你会有很多狭隘的超智能AI。但假设你有很多这样的AI。

605
01:26:28,818 --> 01:26:45,536
然后你有一些...
有些公司，它们从中获得了大量的，嗯，丰厚利润，接着，又会有另一家公司加入进来，开始参与竞争，而这种竞争的方式，将会是通过专业化来实现的。我认为接下来会发生的情况是，这种方式

606
01:26:47,598 --> 01:27:03,294
竞争...
竞争非常青睐专业化。你可以在市场中清楚地看到这一点，在生物进化过程中也同样如此。所以，你会看到各种各样不同的生态位，也会有很多不同的公司，在这个世界中各自占据着独特的生态位。

607
01:27:03,458 --> 01:27:15,903
你可能會說，是啊，比如一家AI公司在某个非常复杂的经济活动领域做得相当出色，而另一家
公司在另一个领域更擅长，第三家公司则非常擅长诉讼，并且绝对不会去那里。

608
01:27:15,898 --> 01:27:19,752
但你看，这恰恰与类人学习的含义相矛盾，那就是它能学习。

609
01:27:19,758 --> 01:27:33,352
它可以，但是，但是你已经积累了知识。你投入了巨大的成本。你花费了大量的计算资源，才在
这方面变得非常、非常、非常好，非常卓越。而其他人也投入了大量的计算资源和大量的经验，

610
01:27:33,352 --> 01:27:35,779
才在其他方面变得非常、非常好。

611
01:27:35,858 --> 01:27:36,275
没错

612
01:27:36,478 --> 01:27:45,719
你投入了大量的人力学习才走到这一步，但现在你已经达到了一个很高的境界，别人就会说，“
听着，我可不想再从头学你学过的东西，再经历一遍了。”

613
01:27:45,878 --> 01:27:56,744
我猜想，这可能需要许多不同的公司，同时着手开发类人持续学习智能体，这样他们才能在各自
不同的分支领域开展研究。但如果只有一家公司

614
01:28:01,138 --> 01:28:10,062
你知道，谁先争取到那个代理人，或者谁先争取到那个学习者，那看起来就好像，嗯，你知道，他们就能...
就像，我们...

615
01:28:10,062 --> 01:28:17,577
如果你只是想想经济中的每一个工作，你只是拥有，呃，实例学习，每一个对一家公司来说都似乎是可行的。

616
01:28:17,598 --> 01:28:22,706
嗯。这确实是个有道理的说法。但我强烈的直觉是，事情不会是这样发展的。

617
01:28:22,698 --> 01:28:23,162
嗯嗯

618
01:28:24,458 --> 01:28:36,485
我强烈的直觉是，没错，论点说它会这样发展。但我强烈的直觉是，它不会这样发展。这就是...
你知道，理论上，理论和实践没有区别。但实践中，却有。我觉得这会是其中之一。

619
01:28:28,018 --> 01:28:28,435
是的

620
01:28:39,458 --> 01:28:48,386
很多人对递归式自我改进的模型都明确指出：
“我们会在服务器里放一百万个伊利亚，它们会想出不同的点子，这将导致超智能很快出现。”

621
01:28:48,386 --> 01:28:53,715
你对你正在做的事情有多大的并行性有什么直觉吗？ 复制伊利亚能带来什么好处？

622
01:29:00,838 --> 01:29:16,766
我不知道。我觉得，肯定会有边际效益递减，因为你想要的是想法不同的人，而不是千篇一律的。我觉得，如果他们只是我的翻版，我不确定你能获得多少额外的增量价值。我觉得...
但你想要的就是那些想法不同的人。

623
01:29:23,218 --> 01:29:34,156
为什么会这样呢？如果你观察一下各种不同的模型，即使是由完全不同的公司发布，它们在可能
互不重叠的数据集上进行训练，令人惊讶的是，这些大型语言模型（LLM）彼此之间竟然如此

624
01:29:34,156 --> 01:29:35,849
相似，这简直太不可思议了。

625
01:29:35,918 --> 01:29:38,332
也许数据集没有我们想象的那么不重叠。

626
01:29:39,498 --> 01:29:53,622
但总觉得，即使单个的人类可能不如未来的AI高效，也许人类团队比AI团队拥有更多样性。
但我们如何才能在AI之间激发有意义的多样性呢？我认为，一味地提高“温度”只会导致胡言

627
01:29:53,622 --> 01:29:54,126
乱语。

628
01:29:56,518 --> 01:30:04,691
我想你可能更想要各种不同的科学家，带着各种不同的偏见或者各种不同的想法。你如何获得那
种多样性在人工智能代理中呢？

629
01:30:04,760 --> 01:30:09,171
所以，之所以一直没有多样性，我认为，就是因为预训练所导致的。

630
01:30:10,840 --> 01:30:19,487
所有的预训练模型 [pause] 大体上都差不多，[pause]
因为它们都是在相同的数据集上进行预训练的。但现在，[pause] 强化学习和后训练

631
01:30:19,487 --> 01:30:24,818
[pause] 开始出现一些差异，[pause]
因为不同的人会提出不同的强化学习训练方法。

632
01:30:24,920 --> 01:30:41,535
嗯。我听你过去曾提到过“自我对弈”，作为一种获取数据，或者让智能体与其它智能体进行匹
配的方式，它们拥有同等智能水平，从而启动学习过程。我们该如何看待为什么目前还没有公开

633
01:30:41,535 --> 01:30:46,282
的、关于这种方法与大型语言模型结合使用的提议呢？

634
01:30:46,900 --> 01:31:00,464
我想说，这里面有两点可以谈谈。我觉得自博弈之所以吸引我，或者说它之所以有趣，是因为它
提供了一种完全不依赖于外部数据，仅仅通过计算能力就能构建模型的方式，对吧？如果你认为

635
01:31:00,464 --> 01:31:08,215
数据是最终的瓶颈，那么只使用算力来解决问题，这本身就非常引人入胜。所以，这就是它最吸
引人的地方。

636
01:31:08,280 --> 01:31:11,623
现在呢，我想跟你说清楚的，主要的问题是这样的

637
01:31:13,700 --> 01:31:21,873
这种自我博弈，至少在过去，当智能体以某种方式相互竞争时，它只适用于培养某些特定的技能
。

638
01:31:23,640 --> 01:31:37,973
这太狭隘了。它的适用范围仅仅局限于，比如说谈判、处理冲突、培养某些社交技能、制定策略
，以及诸如此类的事情。所以，如果你确实很重视这些能力，那么自我对弈这种方式就会非常有

639
01:31:37,973 --> 01:31:38,315
用。

640
01:31:38,800 --> 01:31:51,663
现在，我其实认为自我对弈确实找到了它的归宿，只不过是以一种完全不同的形式呈现出来。是
的，就是以一种不同的形式。所以，像辩论、证明者与验证者这样的概念或系统。

641
01:31:52,140 --> 01:32:06,768
你有一个LLM作为评判者，它也被激励去找出你工作中的错误。你可能觉得这并非严格意义上
的自我对弈，但它是一种，你知道的，相关的对抗性配置，我相信很多人都在这样做。而实际上

642
01:32:06,768 --> 01:32:13,037
，自我对弈是，嗯，一个例子，是更普遍的，嗯，智能体之间竞争的特例，对吧？

643
01:32:04,260 --> 01:32:04,538
嗯

644
01:32:13,040 --> 01:32:24,742
对竞争的反应，自然的反应，就是要努力做到与众不同。所以，如果你放置多个代理，然后告诉
他们，“你们都需要解决某个问题，”而你作为一个代理，正在观察其他所有人在做什么，你就

645
01:32:24,742 --> 01:32:29,897
会说，“嗯，如果他们已经采取了这种方法，那我不确定我是否应该继续追求它。”

646
01:32:31,580 --> 01:32:37,710
我应该追求一些与众不同的东西。所以我觉得，像这样的做法也能鼓励大家采取多样化的方法。

647
01:32:38,980 --> 01:32:44,970
嗯，我想问最后一个问题。你觉得什么是研究品味呢？你显然是这方面的专家

648
01:32:47,160 --> 01:32:54,729
世界上那位被大家公认为在人工智能领域进行研究时，在研究方向上最有品味、最有眼光的人。

649
01:32:54,800 --> 01:33:10,125
你是，呃，深度学习史上许多最重大、最关键的成果的合著者，从 AlexNet 到
GPT-3 等等。呃，您是如何描述自己想出这些想法的过程的呢？

650
01:33:11,220 --> 01:33:16,328
那我就自己来发表一下我的看法吧。我觉得不同的人做法都不一样。

651
01:33:14,280 --> 01:33:14,465
嗯

652
01:33:18,420 --> 01:33:25,293
但有一件事，它对我个人而言，一直以来指引着我的，是一种审美。

653
01:33:26,400 --> 01:33:32,483
关于人工智能应该如何—— [pause] 通过思考人类的本质。但要正确地思考。

654
01:33:28,440 --> 01:33:28,857
嗯哼

655
01:33:32,460 --> 01:33:33,017
我想想

656
01:33:33,080 --> 01:33:37,723
我们很容易去想别人哪里做得不对。但是，要正确地看待别人，这又意味着什么呢？

657
01:33:38,860 --> 01:33:39,463
对的呀

658
01:33:39,460 --> 01:33:52,342
所以，我来举几个例子。人工神经元的概念直接来源于大脑，而且这是一个非常棒的想法。为什
么呢？因为你会说，“当然，大脑有所有这些不同的器官。它有褶皱，但这些褶皱可能并不重要

659
01:33:52,342 --> 01:33:52,648
。”

660
01:33:54,260 --> 01:33:54,724
嗯哼

661
01:33:54,900 --> 01:34:07,100
我们为何觉得神经元很重要？因为它们有很多。这感觉挺对的，所以你想要神经元。你想要某种
局部学习规则来改变连接。你想要某种局部学习规则，能改变神经元之间的连接。对吧？大脑这

662
01:34:07,100 --> 01:34:09,714
样做似乎是合理的。分布式表征的理念。

663
01:34:01,020 --> 01:34:01,252
嗯

664
01:34:15,580 --> 01:34:23,196
大脑会根据经验做出反应，你知道的。神经网络应该从经验中学习，而不是从反应中学习。大脑
从经验中学习。

665
01:34:24,640 --> 01:34:31,513
神经网络应该从经验中学习。然后你就会问自己，某样东西是根本的还是非根本的？事情应该是
什么样子。

666
01:34:31,660 --> 01:34:32,077
是的

667
01:34:32,380 --> 01:34:47,088
我觉得这在很大程度上一直指引着我，就是从多个角度去思考，去寻找那种近乎完美的美。美，
简洁。丑陋，是完全没有容身之地的。它只有美，简洁，优雅，以及来自大脑的正确灵感。所有

668
01:34:47,088 --> 01:34:53,742
这些元素都必须同时存在。它们越是同时存在，你对那种自上而下的信念就越有信心。

669
01:34:56,140 --> 01:35:04,963
那么，当实验与你矛盾时，自上而下的信念会支撑你。因为如果你总是只相信数据，有时你做的
事是对的，但有bug。但你不知道有bug。

670
01:35:08,620 --> 01:35:15,493
你如何判断是否存在错误？你如何判断是该继续调试，还是说这方向根本不对呢？那么，它是自
上而下的吗？

671
01:35:15,480 --> 01:35:18,405
嗯，这该怎么说呢... 你可以说事情非得是这样。

672
01:35:18,420 --> 01:35:18,837
没错

673
01:35:18,900 --> 01:35:28,187
这样的事情必须奏效。所以我们必须坚持下去。这是自上而下的。而这正是基于这种多方面的美
和来自大脑的灵感。

674
01:35:29,620 --> 01:35:31,570
好的，那我们就先到这里吧。

675
01:35:31,620 --> 01:35:32,270
非常感谢

676
01:35:32,240 --> 01:35:33,911
伊利亚，非常非常感谢你！呜！

677
01:35:34,840 --> 01:35:35,722
好的，谢谢你。

678
01:35:35,720 --> 01:35:36,323
太棒了

679
01:35:36,360 --> 01:35:37,706
是啊。我玩得很开心。

680
01:35:37,740 --> 01:35:39,829
是的。我也有同感。嘿，各位朋友。

681
01:35:39,960 --> 01:35:52,266
希望您喜欢这一集。如果您喜欢，最能帮到我们的就是把它分享给您认为会喜欢它的朋友们。如
果您能在收听的任何平台上留下评分或评论，那也会非常有帮助。

682
01:35:52,740 --> 01:35:58,865
如果您对赞助我们的播客节目有兴趣的话，您可以访问
dwarkesh.com/advertise

683
01:35:58,865 --> 01:36:01,795
来联系我们。否则的话，我们就下期节目再见了。

