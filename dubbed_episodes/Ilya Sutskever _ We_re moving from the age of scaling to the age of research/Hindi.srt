1
00:00:00,280 --> 00:00:03,762
जानते हो, अजीब बात क्या है? कि यह सब सच
है।

2
00:00:01,440 --> 00:00:01,672
हूँ।

3
00:00:04,300 --> 00:00:05,089
हाँ? क्या मतलब?

4
00:00:05,100 --> 00:00:05,935
नहीं लगता?

5
00:00:06,120 --> 00:00:06,909
मतलब?

6
00:00:06,960 --> 00:00:13,647
यह सब AI और बे एरिया में जो हो रहा है,
क्या यह साइंस फिक्शन जैसा नहीं है?

7
00:00:09,200 --> 00:00:09,432
सच में हुआ क्या?

8
00:00:13,860 --> 00:00:19,164
हाँ। एक और पागलपन यह है कि धीमी शुरुआत
कितनी सामान्य लगती है। यह विचार कि हम AI

9
00:00:19,164 --> 00:00:24,400
में जीडीपी का 1% निवेश कर रहे होंगे...
मुझे लगता है कि यह एक बड़ी बात लगती। आप

10
00:00:24,400 --> 00:00:27,188
जानते हैं? जबकि अभी, यह बस ऐसा लगता है
कि-

11
00:00:27,200 --> 00:00:32,341
हम चीज़ों के जल्दी आदी हो जाते हैं, पता
चला। हाँ। लेकिन, यह कुछ अमूर्त सा है।

12
00:00:32,341 --> 00:00:35,791
इसका क्या मतलब है? मतलब, आप इसे खबरों में
देखते हैं-

13
00:00:35,860 --> 00:00:36,231
हाँ

14
00:00:36,400 --> 00:00:38,954
कि कंपनी ने डॉलर राशि बताई।

15
00:00:39,020 --> 00:00:39,391
ठीक।

16
00:00:40,420 --> 00:00:41,534
बस वही दिखता है।

17
00:00:41,640 --> 00:00:42,104
ठीक

18
00:00:42,960 --> 00:00:45,374
अभी तक और कुछ महसूस नहीं हुआ।

19
00:00:45,400 --> 00:00:47,814
हाँ। यहीं से शुरू करें? यह दिलचस्प है।

20
00:00:47,840 --> 00:00:48,304
ठीक

21
00:00:48,840 --> 00:00:54,852
मुझे लगता है कि आपका यह कहना कि, एक आम
आदमी के लिए कुछ भी अलग नहीं है,

22
00:00:54,852 --> 00:00:57,292
सिंगुलैरिटी में भी सच रहेगा।

23
00:00:57,280 --> 00:00:58,626
मुझे नहीं लगता।

24
00:00:58,840 --> 00:00:59,954
अच्छा, दिलचस्प।

25
00:01:00,000 --> 00:01:03,715
तो, जिसकी मैं बात कर रहा था, अलग न लगना,

26
00:01:05,640 --> 00:01:11,691
तो ठीक है, फलाँ-फलाँ कंपनी ने कुछ, उह,
समझने में मुश्किल डॉलर का निवेश घोषित

27
00:01:11,691 --> 00:01:12,095
किया।

28
00:01:12,160 --> 00:01:12,624
ठीक।

29
00:01:12,700 --> 00:01:14,975
उसका क्या करें, कोई नहीं जानता।

30
00:01:15,040 --> 00:01:15,457
हाँ।

31
00:01:15,860 --> 00:01:21,591
लेकिन मुझे लगता है एआई का असर होगा। एआई
अर्थव्यवस्था में फैलेगा। इसके लिए बहुत

32
00:01:21,591 --> 00:01:26,355
मजबूत आर्थिक ताकतें हैं। और मुझे लगता है
इसका असर बहुत गहरा होगा।

33
00:01:30,720 --> 00:01:37,112
उसका असर कब तक? मुझे लगता है मॉडल अपने
आर्थिक प्रभाव से ज़्यादा स्मार्ट लगते

34
00:01:37,112 --> 00:01:37,453
हैं।

35
00:01:38,440 --> 00:01:45,808
हाँ। अभी मॉडलों को लेकर सबसे ज़्यादा उलझन
पैदा करने वाली चीज़ों में से एक यह है। इस

36
00:01:45,808 --> 00:01:52,279
बात को कैसे समझा जाए कि वे इवैल्यूएशन में
इतना अच्छा प्रदर्शन कर रहे हैं-

37
00:01:52,320 --> 00:01:52,645
हूँ

38
00:01:52,980 --> 00:01:56,277
और आप मूल्यांकन देखते हैं और कहते हैं,
"ये बहुत कठिन हैं।"

39
00:01:56,280 --> 00:01:56,697
ठीक

40
00:01:57,120 --> 00:01:58,188
वे बहुत अच्छे हैं।

41
00:01:59,580 --> 00:02:07,972
लेकिन आर्थिक प्रभाव बहुत पीछे लगता है। और
ऐसा लगता है जैसे... यह समझना बहुत मुश्किल

42
00:02:07,972 --> 00:02:16,263
है कि मॉडल एक तरफ ये अद्भुत चीजें कैसे कर
सकता है? और दूसरी तरफ, कुछ स्थितियों में

43
00:02:16,263 --> 00:02:20,152
खुद को दो बार दोहराता है, एक तरह से...

44
00:02:20,340 --> 00:02:24,967
एक उदाहरण के तौर पर, मान लीजिए आप वाइब
कोडिंग से कुछ करते हैं। आप कहीं जाते हैं

45
00:02:24,967 --> 00:02:29,535
और आपको एक बग मिलता है। फिर आप मॉडल से
कहते हैं, "क्या आप बग ठीक कर सकते हैं?"

46
00:02:29,540 --> 00:02:29,957
हाँ।

47
00:02:30,340 --> 00:02:35,634
मॉडल कहता है, "हाँ, तुम सही हो, बग है।
ठीक करता हूँ।" और दूसरा बग आ जाता है।

48
00:02:35,660 --> 00:02:36,077
हाँ।

49
00:02:36,820 --> 00:02:41,275
और फिर आप उसे बताते हैं, कि यह दूसरा नया
बग है। और वह कहता है, "हे भगवान, मैंने यह

50
00:02:41,275 --> 00:02:45,566
कैसे किया? आप फिर सही हैं।" और पहला बग
वापस ले आता है। और आप उनके बीच अदला-बदली

51
00:02:45,566 --> 00:02:48,151
कर सकते हैं। और आप सोचते हैं, "यह कैसे
संभव है?"

52
00:02:39,460 --> 00:02:39,599
ठीक।

53
00:02:44,140 --> 00:02:44,372
हाँ

54
00:02:45,700 --> 00:02:46,164
हाँ जी।

55
00:02:48,160 --> 00:02:48,670
हाँ जी।

56
00:02:48,720 --> 00:02:55,329
यह ऐसा है जैसे... मुझे यकीन नहीं है।
लेकिन यह बताता है कि... कुछ अजीब हो रहा

57
00:02:55,329 --> 00:03:02,291
है। मेरे पास दो संभावित स्पष्टीकरण हैं।
तो यहाँ, यह थोड़ा अधिक मनमौजी स्पष्टीकरण

58
00:03:02,291 --> 00:03:09,341
है- ... कि शायद आरएल ट्रेनिंग मॉडल्स को
थोड़ा ज़्यादा एकतरफ़ा और संकीर्ण बना देती

59
00:03:09,341 --> 00:03:11,104
है। थोड़ा ज़्यादा...

60
00:03:11,120 --> 00:03:18,047
मुझे नहीं पता, वे अनजान रहते हैं, हालांकि
यह उन्हें कुछ और तरीकों से जागरूक भी करता

61
00:03:18,047 --> 00:03:24,553
है। और इसी वजह से, वे बुनियादी काम भी
नहीं कर पाते। लेकिन एक और स्पष्टीकरण है।

62
00:03:24,553 --> 00:03:31,227
जब लोग प्री-ट्रेनिंग कर रहे थे, तब किस
डेटा पर ट्रेनिंग देनी है, इस सवाल का जवाब

63
00:03:31,227 --> 00:03:32,157
मिल गया था।

64
00:03:32,840 --> 00:03:35,069
क्योंकि वो जवाब सब कुछ था।

65
00:03:35,120 --> 00:03:35,537
हाँ।

66
00:03:36,120 --> 00:03:38,720
प्री-ट्रेनिंग के लिए सारा डेटा चाहिए।

67
00:03:41,100 --> 00:03:44,304
तो आपको सोचना नहीं होगा, कौन सा डेटा?

68
00:03:44,320 --> 00:03:44,784
हाँ जी।

69
00:03:44,960 --> 00:03:50,254
लेकिन जब लोग आरएल ट्रेनिंग करते हैं, तो
उन्हें सोचना पड़ता है। वे कहते हैं, "हमें

70
00:03:50,254 --> 00:03:55,349
इस चीज़ के लिए ऐसी, और उस चीज़ के लिए
वैसी आरएल ट्रेनिंग चाहिए।" और मैंने सुना

71
00:03:55,349 --> 00:04:00,643
है, सभी कंपनियों के पास टीमें हैं जो नए
आरएल वातावरण बनाती हैं और उन्हें ट्रेनिंग

72
00:04:00,643 --> 00:04:05,871
मिक्स में जोड़ देती हैं। तो सवाल है, वे
क्या हैं? स्वतंत्रता की इतनी सारी डिग्री

73
00:04:05,871 --> 00:04:06,136
हैं।

74
00:04:06,160 --> 00:04:12,086
आप आरएल वातावरणों की एक विशाल विविधता बना
सकते हैं। और एक बात जो आप कर सकते हैं, और

75
00:04:12,086 --> 00:04:17,796
मुझे लगता है कि यह अनजाने में होता है, वह
यह है कि लोग मूल्यांकन से प्रेरणा लेते

76
00:04:17,796 --> 00:04:23,289
हैं। आप कहते हैं, "अरे, मैं चाहता हूँ कि
हमारा मॉडल रिलीज़ होने पर बहुत अच्छा

77
00:04:23,289 --> 00:04:27,336
प्रदर्शन करे।" "मैं चाहता हूँ कि
मूल्यांकन शानदार दिखें।"

78
00:04:28,640 --> 00:04:35,771
आरएल ट्रेनिंग क्या होगी जो इस काम में मदद
कर सके? है ना? मुझे लगता है कि ऐसा होता

79
00:04:35,771 --> 00:04:38,624
है, और यह बहुत कुछ समझा सकता है।

80
00:04:39,120 --> 00:04:45,787
यदि आप इसे मॉडलों के अपर्याप्त
सामान्यीकरण से जोड़ दें, तो यह हमारे देखे

81
00:04:45,787 --> 00:04:53,018
गए बहुत कुछ को समझा सकता है, मूल्यांकन
प्रदर्शन और वास्तविक दुनिया के प्रदर्शन

82
00:04:53,018 --> 00:05:00,250
के बीच का यह अंतर, जिसे हम आज भी ठीक से
नहीं समझते कि हमारा उससे क्या मतलब है।

83
00:05:00,640 --> 00:05:06,474
मुझे, मुझे यह विचार पसंद है कि असली
रिवॉर्ड हैकिंग वे शोधकर्ता हैं जो

84
00:05:06,474 --> 00:05:13,166
मूल्यांकन पर बहुत अधिक ध्यान देते हैं।
उम्म, मुझे लगता है कि आपने जो बताया, उसे

85
00:05:13,166 --> 00:05:15,825
समझने या सोचने के दो तरीके हैं।

86
00:05:15,960 --> 00:05:21,080
देखो, अगर सिर्फ़ कोडिंग प्रतियोगिता में
सुपरह्यूमन बनने से, मॉडल अपने आप बेहतर

87
00:05:21,080 --> 00:05:21,811
नहीं बनेगा।

88
00:05:26,440 --> 00:05:30,884
और अपने कोड बेस को बेहतर बनाने के लिए
बेहतर निर्णय लें। तो, आपको वातावरण का

89
00:05:30,884 --> 00:05:35,029
विस्तार करना चाहिए ताकि आप इसे केवल
कोडिंग प्रतियोगिता में सर्वश्रेष्ठ

90
00:05:35,029 --> 00:05:36,471
प्रदर्शन के लिए न परखें।

91
00:05:36,480 --> 00:05:43,576
इसे X, Y या Z चीज़ों के लिए बेहतरीन ऐप
बनाने में भी सक्षम होना चाहिए। और दूसरा,

92
00:05:43,576 --> 00:05:50,400
शायद आप इसी ओर इशारा कर रहे हैं, कि
कोडिंग प्रतियोगिताओं में माहिर होना आपको

93
00:05:50,400 --> 00:05:54,312
आम तौर पर बेहतर प्रोग्रामर क्यों नहीं
बनाता?

94
00:05:54,440 --> 00:06:00,812
शायद तरीका समझने के लिए वातावरणों की
संख्या और विविधता को बढ़ाते रहना सही नहीं

95
00:06:00,812 --> 00:06:07,350
है। बल्कि, यह आपको एक वातावरण से सीखने और
किसी और चीज़ पर प्रदर्शन सुधारने देगा।

96
00:06:07,760 --> 00:06:12,463
तो मेरे पास, मेरे पास एक मानवीय उपमा है
जो शायद मददगार हो। तो प्रतिस्पर्धी

97
00:06:12,463 --> 00:06:17,488
प्रोग्रामिंग का उदाहरण लेते हैं, क्योंकि
आपने उसका ज़िक्र किया था। और मान लीजिए

98
00:06:17,488 --> 00:06:18,905
आपके पास दो छात्र हैं।

99
00:06:19,640 --> 00:06:25,187
उनमें से एक ने तय किया कि वे सबसे अच्छे
प्रतिस्पर्धी प्रोग्रामर बनना चाहते हैं,

100
00:06:25,187 --> 00:06:30,450
इसलिए वे उस क्षेत्र के लिए 10,000 घंटे
अभ्यास करेंगे। वे सभी समस्याओं को हल

101
00:06:30,450 --> 00:06:35,429
करेंगे, सभी प्रमाण तकनीकों को याद करेंगे
और बहुत, बहुत, आप जानते हैं...

102
00:06:35,780 --> 00:06:40,881
सभी एल्गोरिदम को तेज़ी और सटीकता से लागू
करने में बहुत कुशल हों। और ऐसा करके, वे

103
00:06:40,881 --> 00:06:45,918
सबसे अच्छे... सबसे अच्छे में से एक बन गए।
दूसरे छात्र ने सोचा, "ओह, कॉम्पिटिटिव

104
00:06:45,918 --> 00:06:50,502
प्रोग्रामिंग बढ़िया है।" शायद उन्होंने
100 घंटे अभ्यास किया। बहुत कम, और

105
00:06:50,502 --> 00:06:55,345
उन्होंने भी बहुत अच्छा किया। आपको क्या
लगता है कि बाद में अपने करियर में कौन

106
00:06:55,345 --> 00:06:56,120
बेहतर करेगा?

107
00:06:49,880 --> 00:06:50,158
हाँ

108
00:06:56,160 --> 00:06:56,763
दूसरा।

109
00:06:56,760 --> 00:07:02,208
सही है ना? और मुझे लगता है कि यही हो रहा
है। मॉडल पहले छात्र जैसे हैं, बल्कि उससे

110
00:07:02,208 --> 00:07:07,726
भी ज़्यादा, क्योंकि हम कहते हैं, "मॉडल को
प्रतिस्पर्धी प्रोग्रामिंग में अच्छा होना

111
00:07:07,726 --> 00:07:13,174
चाहिए, तो हम हर समस्या लेते हैं, और डेटा
ऑग्मेंटेशन करते हैं ताकि हमारे पास और भी

112
00:07:13,174 --> 00:07:14,128
समस्याएँ हों।"

113
00:07:14,140 --> 00:07:14,697
जी हाँ

114
00:07:14,720 --> 00:07:18,268
और हम उस पर प्रशिक्षण देते हैं। और अब
आपके पास यह महान प्रतिस्पर्धी प्रोग्रामर

115
00:07:18,268 --> 00:07:20,664
है। और इस सादृश्य से, मुझे लगता है कि यह
अधिक सहज है।

116
00:07:21,360 --> 00:07:26,281
मुझे लगता है कि इस सादृश्य से यह अधिक सहज
लगता है कि, हाँ, ठीक है, अगर यह इतना

117
00:07:26,281 --> 00:07:31,267
अच्छी तरह से प्रशिक्षित है, तो ऐसा है
जैसे सभी एल्गोरिदम और प्रमाण तकनीकें उसकी

118
00:07:31,267 --> 00:07:36,125
उंगलियों पर हैं। और यह अधिक सहज है कि
तैयारी के इस स्तर के साथ, यह जरूरी नहीं

119
00:07:36,125 --> 00:07:38,171
कि अन्य चीजों पर सामान्यीकृत हो।

120
00:07:38,180 --> 00:07:45,912
हम्म। तो फिर, दूसरे छात्र 100 घंटे की
फाइन-ट्यूनिंग से पहले क्या कर रहा है,

121
00:07:45,912 --> 00:07:47,793
उसकी उपमा क्या है?

122
00:07:48,120 --> 00:07:53,599
मुझे लगता है कि उनमें वो बात है, वही 'इट'
फैक्टर।

123
00:07:53,680 --> 00:07:54,144
हाँ।

124
00:07:54,200 --> 00:07:59,848
है ना? मुझे पता है, जब मैं कॉलेज में था,
मेरे साथ ऐसा ही एक छात्र था, तो मुझे पता

125
00:07:59,848 --> 00:08:01,119
है कि ऐसा होता है।

126
00:08:01,160 --> 00:08:06,113
हाँ। मुझे लगता है कि इसे प्री-ट्रेनिंग के
काम से अलग करना दिलचस्प है। तो, आपने अभी

127
00:08:06,113 --> 00:08:10,822
जो कहा कि हमें प्री-ट्रेनिंग में डेटा
चुनने की ज़रूरत नहीं है, उसे समझने का एक

128
00:08:10,822 --> 00:08:15,409
तरीका यह है कि, असल में, यह 10,000 घंटे
के अभ्यास से अलग नहीं है। बस आपको वह

129
00:08:15,409 --> 00:08:20,179
10,000 घंटे का अभ्यास मुफ्त में मिल जाता
है क्योंकि यह पहले से ही प्री-ट्रेनिंग

130
00:08:20,179 --> 00:08:22,197
डिस्ट्रीब्यूशन में कहीं मौजूद है।

131
00:08:22,220 --> 00:08:26,913
लेकिन शायद आप कह रहे हैं कि प्री-ट्रेनिंग
से ज़्यादा जनरलाइज़ेशन नहीं होती, बस

132
00:08:26,913 --> 00:08:30,997
उसमें बहुत डेटा होता है। यह ज़रूरी नहीं
कि RL से बेहतर जनरलाइज़ करे।

133
00:08:31,260 --> 00:08:35,300
प्री-ट्रेनिंग की मुख्य शक्ति यह है कि यह
बहुत अधिक है।

134
00:08:35,340 --> 00:08:35,525
हाँ

135
00:08:36,420 --> 00:08:42,507
और बी, प्री-ट्रेनिंग के लिए डेटा के बारे
में ज़्यादा सोचना नहीं पड़ता। यह

136
00:08:42,507 --> 00:08:48,680
स्वाभाविक डेटा है और इसमें लोगों द्वारा
किए गए बहुत से काम शामिल होते हैं।

137
00:08:48,720 --> 00:08:49,277
हाँ

138
00:08:50,380 --> 00:08:56,753
लोगों के विचार और कई ऐसी विशेषताएँ जो हम
जानते हैं। यह ऐसा है जैसे पूरी दुनिया को

139
00:08:56,753 --> 00:08:59,064
लोगों ने टेक्स्ट पर उतारा हो।

140
00:08:59,100 --> 00:08:59,285
हाँ।

141
00:09:00,220 --> 00:09:05,989
और प्री-ट्रेनिंग उसे समझने की कोशिश करती
है, बहुत सारे डेटा का उपयोग करके। यह

142
00:09:05,989 --> 00:09:11,683
बहुत... प्री-ट्रेनिंग के बारे में तर्क
करना बहुत मुश्किल है क्योंकि यह समझना

143
00:09:11,683 --> 00:09:16,845
बहुत मुश्किल है कि मॉडल किस तरह
प्री-ट्रेनिंग डेटा पर निर्भर करता है।

144
00:09:17,600 --> 00:09:22,013
और जब भी मॉडल गलती करता है, क्या ऐसा
इसलिए हो सकता है कि कोई चीज़

145
00:09:22,013 --> 00:09:27,669
प्री-ट्रेनिंग डेटा में उतनी समर्थित न हो?
आप जानते हैं, प्री-ट्रेनिंग द्वारा समर्थन

146
00:09:27,669 --> 00:09:33,048
शायद एक ढीला शब्द है। मुझे नहीं पता कि
मैं इस पर कुछ और उपयोगी जोड़ सकता हूँ या

147
00:09:33,048 --> 00:09:33,807
नहीं, लेकिन

148
00:09:36,120 --> 00:09:38,302
प्री-ट्रेनिंग का मानव समरूप नहीं।

149
00:09:38,500 --> 00:09:44,070
हम्म। लोगों ने प्री-ट्रेनिंग की मानवीय
उपमा के लिए जो उपमाएँ दी हैं, और मैं

150
00:09:44,070 --> 00:09:47,834
जानना चाहता हूँ कि वे संभावित रूप से गलत
क्यों हैं।

151
00:09:47,920 --> 00:09:53,991
एक तो यह है कि किसी व्यक्ति के पहले 18 या
15 या 13 साल के जीवन के बारे में सोचना,

152
00:09:53,991 --> 00:09:59,531
जब वे आर्थिक रूप से उत्पादक नहीं होते,
बल्कि कुछ ऐसा कर रहे होते हैं जिससे

153
00:09:59,531 --> 00:10:03,477
उन्हें दुनिया को बेहतर ढंग से समझने में
मदद मिलती है।

154
00:10:03,640 --> 00:10:09,975
और दूसरा, विकास को तीन अरब वर्षों की खोज
के रूप में देखना, जिसका परिणाम मानव

155
00:10:09,975 --> 00:10:16,480
जीवनकाल में होता है। मैं उत्सुक हूँ कि
क्या इनमें से कोई प्री-ट्रेनिंग के समान

156
00:10:16,480 --> 00:10:22,308
है, या, यदि प्री-ट्रेनिंग नहीं, तो मानव
जीवनकाल का सीखना कैसा होता है?

157
00:10:22,880 --> 00:10:27,998
मुझे लगता है कि कुछ समानताएँ हैं- ...इन
दोनों प्री-ट्रेनिंग के बीच। और

158
00:10:27,998 --> 00:10:33,858
प्री-ट्रेनिंग इन दोनों की भूमिका निभाने
की कोशिश करती है, पर मुझे लगता है कि कुछ

159
00:10:33,858 --> 00:10:39,273
बड़े अंतर भी हैं। प्री-ट्रेनिंग डेटा की
मात्रा बहुत, बहुत चौंकाने वाली है।

160
00:10:25,280 --> 00:10:25,419
हाँ

161
00:10:27,200 --> 00:10:27,385
हाँ।

162
00:10:41,100 --> 00:10:41,239
हाँ।

163
00:10:41,260 --> 00:10:48,161
और किसी तरह, एक इंसान, 15 साल में भी,
बहुत कम डेटा से, बहुत कम जानता है। लेकिन

164
00:10:48,161 --> 00:10:54,973
जो भी वे जानते हैं, उसे किसी तरह कहीं
ज़्यादा गहराई से जानते हैं। और गलतियाँ,

165
00:10:54,973 --> 00:10:59,185
उस स्तर पर ही, आप वो नहीं करेंगे जो AI
करते हैं।

166
00:10:49,420 --> 00:10:49,884
हाँ।

167
00:10:59,220 --> 00:10:59,637
हाँ।

168
00:10:59,840 --> 00:11:05,539
एक और बात। आप कहेंगे, क्या यह विकास जैसा
कुछ है? जवाब है, शायद। पर इस मामले में,

169
00:11:05,539 --> 00:11:07,270
विकास को शायद बढ़त मिले।

170
00:11:07,600 --> 00:11:14,167
जैसे, एक... मुझे याद है मैंने एक ऐसा
मामला पढ़ा था जहाँ कुछ... आपको पता है

171
00:11:14,167 --> 00:11:21,004
न्यूरोसाइंटिस्ट दिमाग को कैसे समझते हैं?
वे उन लोगों का अध्ययन करते हैं जिनके

172
00:11:21,004 --> 00:11:24,782
दिमाग के अलग-अलग हिस्सों में क्षति होती
है।

173
00:11:24,780 --> 00:11:25,708
हम्म

174
00:11:26,180 --> 00:11:33,692
कुछ लोगों में सबसे अजीब लक्षण होते हैं,
जो बहुत दिलचस्प है। और एक प्रासंगिक मामला

175
00:11:33,692 --> 00:11:34,724
याद आता है।

176
00:11:35,780 --> 00:11:42,442
मैंने एक व्यक्ति के बारे में पढ़ा जिसे
दिमागी चोट लगी थी जिससे... शायद स्ट्रोक

177
00:11:42,442 --> 00:11:49,277
या दुर्घटना से उसकी भावनात्मक क्षमता खत्म
हो गई। तो उसने कोई भी भावना महसूस करना

178
00:11:49,277 --> 00:11:50,315
बंद कर दिया।

179
00:11:51,440 --> 00:11:58,242
और इसके चलते, वह फिर भी स्पष्ट बोलता था,
और छोटी पहेलियाँ हल कर सकता था, परीक्षाओं

180
00:11:58,242 --> 00:12:04,708
में भी वह ठीक लगता था, लेकिन उसे कोई
भावना महसूस नहीं होती थी। उसे दुख, गुस्सा

181
00:12:04,708 --> 00:12:11,259
या उत्साह महसूस नहीं होता था, और वह किसी
भी तरह के निर्णय लेने में बेहद खराब हो

182
00:12:11,259 --> 00:12:11,594
गया।

183
00:12:11,592 --> 00:12:19,895
उसे यह तय करने में घंटों लग जाएंगे कि कौन
से मोज़े पहने, और वह बहुत ही बुरे वित्तीय

184
00:12:19,895 --> 00:12:27,895
निर्णय लेगा। और यह बहुत... तो, यह हमारी
अंतर्निहित भावनाओं की भूमिका के बारे में

185
00:12:27,895 --> 00:12:33,465
क्या कहता है, हमें असल में एक व्यवहार्य
एजेंट बनाने में?

186
00:12:33,872 --> 00:12:38,504
और आपके सवाल से जुड़ने के लिए,
प्री-ट्रेनिंग के बारे में, अगर आप

187
00:12:38,504 --> 00:12:44,533
प्री-ट्रेनिंग से सब कुछ निकालने में माहिर
हैं, तो आप वह भी हासिल कर सकते हैं, लेकिन

188
00:12:44,533 --> 00:12:46,225
यह उस तरह की चीज़ है जो

189
00:12:50,992 --> 00:12:55,589
खैर, प्री-ट्रेनिंग से वह मिलना संभव हो भी
सकता है या नहीं।

190
00:12:56,192 --> 00:13:02,368
यह क्या है? साफ तौर पर सिर्फ सीधी भावना
नहीं है। लगता है कुछ

191
00:13:04,151 --> 00:13:11,127
लगभग एक मूल्य फलन जैसा कुछ जो बताता है कि
कौन सा निर्णय लेना है, और उसका अंतिम इनाम

192
00:13:11,127 --> 00:13:15,296
क्या होगा। और आपको लगता है कि यह अपने आप
नहीं आता-

193
00:13:15,391 --> 00:13:19,338
मैं बस कह रहा हूँ, यह 100% स्पष्ट नहीं
है।

194
00:13:19,352 --> 00:13:25,760
हाँ। पर वो क्या है? आप भावनाओं को क्या
मानते हैं और उनका ML सादृश्य क्या है?

195
00:13:26,332 --> 00:13:28,514
यह मूल्य फलन जैसा होगा।

196
00:13:28,572 --> 00:13:29,082
हाँ जी।

197
00:13:29,312 --> 00:13:34,693
पर मुझे नहीं लगता कोई अच्छी एमएल उपमा है
क्योंकि अभी, वैल्यू फंक्शन्स की लोगों के

198
00:13:34,693 --> 00:13:36,510
काम में खास भूमिका नहीं है।

199
00:13:36,531 --> 00:13:39,921
अगर आप चाहें तो वैल्यू फंक्शन समझा दें।

200
00:13:39,992 --> 00:13:44,868
मेरा मतलब है, ज़रूर। मुझे यह करके बहुत
खुशी होगी। ठीक है। तो,

201
00:13:49,212 --> 00:13:52,416
अभी की रीइन्फोर्समेंट लर्निंग में, लोग उन
एजेंट्स को कैसे ट्रेन करते हैं?

202
00:13:56,552 --> 00:14:01,547
तो आपके पास एक न्यूरल नेट है, और आप उसे
एक समस्या देते हैं, और फिर आप मॉडल को उसे

203
00:14:01,547 --> 00:14:06,418
हल करने के लिए कहते हैं। और मॉडल शायद
हज़ारों, लाखों क्रियाएँ या विचार करता है,

204
00:14:06,418 --> 00:14:11,227
और फिर वह एक समाधान प्रस्तुत करता है।
समाधान का मूल्यांकन किया जाता है। और फिर

205
00:14:11,227 --> 00:14:16,160
उस स्कोर का उपयोग हर एक क्रिया के लिए एक
प्रशिक्षण संकेत प्रदान करने के लिए किया

206
00:14:16,160 --> 00:14:16,660
जाता है।

207
00:14:18,291 --> 00:14:18,987
मार्ग में

208
00:14:18,992 --> 00:14:19,363
हूँ

209
00:14:20,192 --> 00:14:24,300
तो इसका मतलब है कि यदि आप कोई ऐसा काम कर
रहे हैं जिसमें बहुत समय लगता है, यदि आप

210
00:14:24,300 --> 00:14:28,408
किसी ऐसे कार्य को प्रशिक्षित कर रहे हैं
जिसे हल करने में बहुत समय लगता है, तो आप

211
00:14:28,408 --> 00:14:32,464
तब तक कुछ भी नहीं सीखेंगे जब तक आप उसे हल
नहीं कर लेते, जब तक आप कोई प्रस्तावित

212
00:14:32,464 --> 00:14:36,364
समाधान नहीं ढूंढ लेते। रीइन्फोर्समेंट
लर्निंग इसी तरह से अनाड़ीपन से की जाती

213
00:14:36,364 --> 00:14:38,860
है। O1, R1 भी स्पष्ट रूप से इसी तरह किए
जाते हैं।

214
00:14:40,872 --> 00:14:46,317
वैल्यू फंक्शन कहता है, 'देखो, मैं
कभी-कभी, हमेशा नहीं, बता सकता हूँ कि तुम

215
00:14:46,317 --> 00:14:51,911
अच्छा कर रहे हो या बुरा।' वैल्यू फंक्शन
का विचार कुछ क्षेत्रों में दूसरों से

216
00:14:51,911 --> 00:14:55,268
ज़्यादा उपयोगी है। जैसे, जब आप शतरंज
खेलते हैं

217
00:14:56,912 --> 00:15:01,170
और आप एक मोहरा हार जाते हैं, आप जानते
हैं, "मैंने गड़बड़ कर दी।" आपको यह जानने

218
00:15:01,170 --> 00:15:05,595
के लिए पूरा खेल खेलने की ज़रूरत नहीं कि
मैंने अभी जो किया वह बुरा था, और इसलिए जो

219
00:15:05,595 --> 00:15:07,918
कुछ भी, उम, इससे पहले हुआ था वह भी बुरा
था।

220
00:15:08,891 --> 00:15:14,862
तो वैल्यू फंक्शन आपको अंत तक के इंतज़ार
को छोटा करने देता है। जैसे, मान लीजिए कि

221
00:15:14,862 --> 00:15:20,985
आपने किसी तरह का, उम... ठीक है, मान लीजिए
कि आप कोई गणित का काम या प्रोग्रामिंग का

222
00:15:20,985 --> 00:15:26,352
काम कर रहे हैं और आप किसी खास समाधान की
दिशा तलाशने की कोशिश कर रहे हैं।

223
00:15:26,412 --> 00:15:33,119
और मान लीजिए, हज़ार बार सोचने के बाद, आप
इस नतीजे पर पहुँचे कि यह दिशा सही नहीं

224
00:15:33,119 --> 00:15:33,377
है।

225
00:15:34,332 --> 00:15:40,188
जैसे ही आप यह समझते हैं, आपको हज़ार कदम
पहले ही इनाम का संकेत मिल चुका होता, जब

226
00:15:40,188 --> 00:15:45,894
आपने यह रास्ता चुना था। आप कहते हैं,
'अगली बार, मुझे ऐसी स्थिति में यह रास्ता

227
00:15:45,894 --> 00:15:51,375
नहीं चुनना चाहिए,' इससे बहुत पहले कि आप
वास्तव में कोई समाधान लेकर आए हों।

228
00:15:51,391 --> 00:15:57,343
हम्म। यह डीप सिगार 1 पेपर में था कि
प्रक्षेपवक्रों का स्थान इतना विस्तृत है

229
00:15:57,343 --> 00:16:03,376
कि शायद एक मध्यवर्ती प्रक्षेपवक्र और
मूल्य से मैपिंग सीखना मुश्किल है। और यह

230
00:16:03,376 --> 00:16:09,891
भी देखते हुए कि, आप जानते हैं, उदाहरण के
लिए, कोडिंग में, आपको गलत विचार आएगा, फिर

231
00:16:09,891 --> 00:16:12,706
आप वापस जाएंगे। फिर आप कुछ बदलेंगे।

232
00:16:12,732 --> 00:16:19,020
यह डीप लर्निंग पर अविश्वास जैसा लगता है।
हाँ, मुश्किल हो सकता है, लेकिन डीप

233
00:16:19,020 --> 00:16:21,230
लर्निंग कुछ भी कर सकती है।

234
00:16:15,332 --> 00:16:15,796
हाँ।

235
00:16:21,271 --> 00:16:21,688
हाँ।

236
00:16:22,291 --> 00:16:29,362
तो मेरी उम्मीद है कि वैल्यू फंक्शन उपयोगी
होने चाहिए, और मुझे पूरी उम्मीद है कि

237
00:16:29,362 --> 00:16:36,252
उनका भविष्य में उपयोग होगा, अगर अभी तक
नहीं हुआ है तो। मैं उस व्यक्ति के बारे

238
00:16:36,252 --> 00:16:42,870
में क्या कह रहा था- जिसका भावनात्मक
केंद्र क्षतिग्रस्त हो गया था, वह यह है

239
00:16:42,870 --> 00:16:43,142
कि,

240
00:16:45,372 --> 00:16:52,189
शायद इसका मतलब है कि इंसानों की मूल्य
प्रणाली भावनाओं से इस तरह प्रभावित होती

241
00:16:52,189 --> 00:16:58,558
है जो विकास से तय है। और शायद दुनिया में
लोगों के प्रभावी होने के लिए यह

242
00:16:58,558 --> 00:16:59,814
महत्वपूर्ण है।

243
00:17:00,512 --> 00:17:07,564
यही बात मैं आपसे पूछने वाला था। भावनाओं
के मूल्य फलन के बारे में कुछ बहुत दिलचस्प

244
00:17:07,564 --> 00:17:13,911
है, यह प्रभावशाली है कि उनमें इतनी
उपयोगिता है, फिर भी वे समझने में काफी,

245
00:17:13,911 --> 00:17:15,233
उम्म, आसान हैं।

246
00:17:16,092 --> 00:17:22,790
तो मेरे दो जवाब हैं। मैं सहमत हूँ कि हम
जो बातें सीखते और जिन पर चर्चा करते हैं,

247
00:17:22,790 --> 00:17:29,235
उनकी तुलना में भावनाएँ अपेक्षाकृत सरल
हैं। वे इतनी सरल भी हो सकती हैं कि शायद

248
00:17:29,235 --> 00:17:36,103
आप उन्हें मानवीय रूप से समझने योग्य तरीके
से मैप कर सकें। मुझे लगता है कि ऐसा करना

249
00:17:36,103 --> 00:17:37,036
अच्छा होगा।

250
00:17:39,331 --> 00:17:46,021
उपयोगिता के संदर्भ में, मुझे लगता है कि
एक बात है जहाँ, आप जानते हैं, यह

251
00:17:46,021 --> 00:17:53,277
जटिलता-मजबूती का समझौता है। जहाँ जटिल
चीजें बहुत उपयोगी हो सकती हैं, लेकिन सरल

252
00:17:53,277 --> 00:17:58,742
चीजें एक बहुत व्यापक परिस्थितियों में
बहुत उपयोगी होती हैं।

253
00:17:59,152 --> 00:18:05,311
तो मेरा मानना है कि हम जो देख रहे हैं,
उसे ऐसे समझा जा सकता है कि हमारी भावनाएँ

254
00:18:05,311 --> 00:18:11,708
मुख्य रूप से स्तनधारी पूर्वजों से विकसित
हुईं और होमोनिड के रूप में थोड़ी परिष्कृत

255
00:18:11,708 --> 00:18:13,130
हुईं, बस थोड़ा सा।

256
00:18:13,252 --> 00:18:18,353
हमारे पास काफी सामाजिक भावनाएं हैं, जो
शायद स्तनधारियों में कम हों... लेकिन वे

257
00:18:18,353 --> 00:18:23,455
बहुत परिष्कृत नहीं हैं, और क्योंकि वे
परिष्कृत नहीं हैं, वे इस बहुत अलग दुनिया

258
00:18:23,455 --> 00:18:28,159
में हमारी बहुत मदद करती हैं, उस दुनिया के
मुकाबले जिसमें हम रहते आए हैं।

259
00:18:28,168 --> 00:18:34,838
दरअसल, वे भी गलती करते हैं। जैसे, हमारी
भावनाएँ... खैर, पता नहीं, क्या भूख भावना

260
00:18:34,838 --> 00:18:40,242
है? यह बहस का विषय है, पर मुझे लगता है,
भूख की हमारी सहज भावना...

261
00:18:42,148 --> 00:18:49,067
भोजन की प्रचुरता वाली इस दुनिया में हमें
सही राह दिखाने में सफल नहीं हो रहा है।

262
00:18:49,088 --> 00:18:54,372
हाँ। लोग डेटा, पैरामीटर और कंप्यूट की
स्केलिंग की बात करते रहे हैं। क्या

263
00:18:54,372 --> 00:18:59,954
स्केलिंग को सोचने का कोई और व्यापक तरीका
है? स्केलिंग के अन्य आयाम क्या हैं?

264
00:19:00,848 --> 00:19:07,590
तो, बात ये है... तो, यहाँ, यहाँ एक
नज़रिया है। एक नज़रिया जो मुझे लगता है,

265
00:19:07,590 --> 00:19:09,160
शायद, शायद सच हो।

266
00:19:10,468 --> 00:19:16,456
तो, एमएल पहले ऐसे काम करता था कि लोग बस
चीजों के साथ छेड़छाड़ करते थे और दिलचस्प

267
00:19:16,456 --> 00:19:20,777
नतीजे पाने की कोशिश करते थे। अतीत में यही
होता रहा है। फिर

268
00:19:28,228 --> 00:19:35,251
स्केलिंग की समझ आ गई, है ना? स्केलिंग
नियम, जीपीटी-3, और अचानक सबको एहसास हुआ

269
00:19:35,251 --> 00:19:37,562
कि हमें स्केल करना चाहिए।

270
00:19:39,108 --> 00:19:44,755
और यह बस एक उदाहरण है कि भाषा विचार को
कैसे प्रभावित करती है। स्केलिंग सिर्फ एक

271
00:19:44,755 --> 00:19:50,547
शब्द है, पर यह बहुत शक्तिशाली है क्योंकि
यह लोगों को बताता है कि क्या करना है। वे

272
00:19:50,547 --> 00:19:56,050
कहते हैं, "ठीक है, चीजों को स्केल करते
हैं।" तो आप कहते हैं, "ठीक है, हम क्या

273
00:19:56,050 --> 00:20:01,553
स्केल कर रहे हैं?" प्री-ट्रेनिंग स्केल
करने लायक थी। यह एक खास स्केलिंग तरीका

274
00:20:01,553 --> 00:20:01,770
था।

275
00:20:01,828 --> 00:20:02,292
जी हाँ

276
00:20:02,308 --> 00:20:07,416
प्री-ट्रेनिंग की बड़ी सफलता यह एहसास है
कि यह तरीका अच्छा है।

277
00:20:07,968 --> 00:20:14,327
तो आप कहते हैं, "अरे, अगर आप कुछ कंप्यूट
और डेटा को एक निश्चित आकार के न्यूरल नेट

278
00:20:14,327 --> 00:20:20,527
में मिलाते हैं, तो आपको परिणाम मिलेंगे,
और आप जानेंगे कि नुस्खे को बढ़ाने से यह

279
00:20:20,527 --> 00:20:26,410
बेहतर होगा।" और यह भी बहुत अच्छा है।
कंपनियों को यह पसंद है क्योंकि यह आपको

280
00:20:26,410 --> 00:20:30,305
निवेश का एक बहुत, उह, कम जोखिम वाला तरीका
देता है-

281
00:20:30,348 --> 00:20:30,440
हाँ

282
00:20:30,808 --> 00:20:31,597
अपने स्रोत

283
00:20:31,668 --> 00:20:32,271
हाँ

284
00:20:32,308 --> 00:20:37,148
सही है ना? रिसर्च में संसाधन लगाना बहुत
मुश्किल है। इसकी तुलना में, आपको ऐसे

285
00:20:37,148 --> 00:20:41,085
शोधकर्ता चाहिए जो कुछ नया खोजें, बनाम
ज़्यादा डेटा और कंप्यूट।

286
00:20:45,448 --> 00:20:51,083
आपको पता है कि प्री-ट्रेनिंग से कुछ
मिलेगा। और सच में, ट्विटर पर लोग जो बातें

287
00:20:51,083 --> 00:20:56,866
कहते हैं, उनके आधार पर, ऐसा लगता है कि
जेमिनी ने प्री-ट्रेनिंग से ज़्यादा हासिल

288
00:20:56,866 --> 00:21:02,575
करने का तरीका ढूंढ लिया है। लेकिन किसी
बिंदु पर, प्री-ट्रेनिंग का डेटा खत्म हो

289
00:21:02,575 --> 00:21:07,321
जाएगा। डेटा बहुत स्पष्ट रूप से सीमित है।
तो फिर, आगे क्या करेंगे?

290
00:21:07,648 --> 00:21:12,737
या तो आप किसी तरह की उन्नत प्री-ट्रेनिंग
करते हैं, जो पहले की विधि से अलग हो, या

291
00:21:12,737 --> 00:21:17,892
आप आरएल कर रहे हैं, या शायद कुछ और। लेकिन
अब जब कंप्यूटिंग बड़ी है, कंप्यूटिंग अब

292
00:21:17,892 --> 00:21:22,981
बहुत बड़ी है, एक तरह से, हम शोध के युग
में वापस आ गए हैं। तो शायद इसे कहने का एक

293
00:21:22,981 --> 00:21:23,948
और तरीका यह है।

294
00:21:24,228 --> 00:21:28,361
2020 तक, हाँ, 2012 से 2020 तक, यह रिसर्च
का दौर था।

295
00:21:31,268 --> 00:21:36,356
अब, 2020 से 2025 तक, यह स्केलिंग का युग
था, या लगभग। उन सालों में हम 'ऊबर्स'

296
00:21:36,356 --> 00:21:41,783
जोड़ते हैं क्योंकि लोग कहते हैं, "यह कमाल
है। आपको और स्केल करना होगा। स्केल करते

297
00:21:41,783 --> 00:21:47,278
रहो।" एक शब्द, स्केलिंग। लेकिन अब पैमाना
इतना बड़ा है। क्या सच में यह मानना है कि,

298
00:21:47,278 --> 00:21:52,501
"ओह, यह इतना बड़ा है, लेकिन अगर आपके पास
100 गुना और होता, तो सब कुछ कितना अलग

299
00:21:52,501 --> 00:21:52,908
होता?"

300
00:21:53,328 --> 00:22:01,408
यह तो अलग होगा ही। लेकिन, क्या यह मानना
है कि सौ गुना बढ़ाने से सब कुछ बदल जाएगा?

301
00:22:02,168 --> 00:22:06,208
मुझे नहीं लगता यह सच है, फिर से शोध का
दौर, बस बड़े कंप्यूटरों संग।

302
00:22:06,788 --> 00:22:08,227
बड़ा दिलचस्प है।

303
00:22:10,508 --> 00:22:16,645
लेकिन मैं आपसे वही सवाल पूछता हूँ। हम
क्या बढ़ा रहे हैं और एक नुस्खा होने का

304
00:22:16,645 --> 00:22:23,028
क्या मतलब होगा? क्योंकि मुझे किसी ऐसे
स्पष्ट संबंध की जानकारी नहीं है जो भौतिकी

305
00:22:23,028 --> 00:22:29,329
के नियम जैसा लगे, जो प्री-ट्रेनिंग में
था। डेटा या कंप्यूटर पैरामीटर और लॉस के

306
00:22:29,329 --> 00:22:30,802
बीच एक पावर लॉ है।

307
00:22:30,848 --> 00:22:37,743
हमें कैसा रिश्ता खोजना चाहिए, और इस नई
रेसिपी कैसी हो सकती है, इस पर कैसे विचार

308
00:22:37,743 --> 00:22:38,185
करें?

309
00:22:38,908 --> 00:22:46,552
तो, हमने पहले ही स्केलिंग के एक तरीके से
दूसरे तरीके में एक बड़ा बदलाव होते हुए

310
00:22:46,552 --> 00:22:53,118
देखा है, प्री-ट्रेनिंग से आरएल तक। अब,
लोग आरएल को स्केल कर रहे हैं।

311
00:22:53,148 --> 00:22:58,780
अब, ट्विटर पर लोगों के अनुसार, वे इस समय
प्री-ट्रेनिंग से ज़्यादा कंप्यूट आरएल पर

312
00:22:58,780 --> 00:23:04,061
खर्च करते हैं, क्योंकि आरएल वास्तव में
काफी कंप्यूट खपत कर सकता है। आप जानते

313
00:23:04,061 --> 00:23:06,244
हैं, बहुत लंबे रोलआउट होते हैं।

314
00:23:03,388 --> 00:23:03,527
हाँ।

315
00:23:06,268 --> 00:23:06,546
हाँ

316
00:23:07,208 --> 00:23:11,859
तो उन रोलआउट्स को बनाने में बहुत कंप्यूट
लगता है, और फिर आपको प्रति रोलआउट

317
00:23:11,859 --> 00:23:17,021
अपेक्षाकृत कम लर्निंग मिलती है। तो आप सच
में बहुत कंप्यूट खर्च कर सकते हैं। और मैं

318
00:23:17,021 --> 00:23:21,928
कल्पना कर सकता हूँ... जैसे, मैं नहीं...
इस, इस स्तर पर... यह ऐसा है कि मैं इसे

319
00:23:21,928 --> 00:23:24,158
स्केल भी नहीं कहूँगा, उम, स्केलिंग।

320
00:23:24,148 --> 00:23:28,878
मैं कहूँगा, "अरे, क्या कर रहे हो?" और
क्या तुम जो कर रहे हो, वह सबसे उत्पादक

321
00:23:28,878 --> 00:23:33,861
चीज़ है जो तुम कर सकते हो? क्या तुम अपनी
कंप्यूटिंग का उपयोग करने का कोई ज़्यादा

322
00:23:33,861 --> 00:23:38,717
उत्पादक तरीका ढूंढ सकते हो? हमने पहले
वैल्यू फंक्शन के कारोबार पर चर्चा की है,

323
00:23:38,717 --> 00:23:43,763
और शायद एक बार जब लोग वैल्यू फंक्शन्स में
अच्छे हो जाएँगे, तो वे अपने संसाधनों का

324
00:23:43,763 --> 00:23:46,160
ज़्यादा उत्पादक तरीके से उपयोग करेंगे।

325
00:23:31,508 --> 00:23:31,786
हाँ

326
00:23:47,288 --> 00:23:52,210
और अगर आपको मॉडल प्रशिक्षित करने का कोई
नया तरीका मिले, तो आप कह सकते हैं, "क्या

327
00:23:52,210 --> 00:23:57,194
यह स्केलिंग है या सिर्फ आपके संसाधनों का
उपयोग?" मुझे लगता है कि यह थोड़ा अस्पष्ट

328
00:23:57,194 --> 00:24:02,241
हो जाता है, क्योंकि जब लोग शोध के युग में
थे, तब वे कहते थे, "चलो यह, और यह, और यह

329
00:24:02,241 --> 00:24:06,976
कोशिश करते हैं। चलो वह, और वह, और वह
कोशिश करते हैं। अरे देखो, कुछ दिलचस्प हो

330
00:24:06,976 --> 00:24:09,718
रहा है।" और मुझे लगता है कि उस पर वापसी
होगी।

331
00:24:10,128 --> 00:24:14,887
तो, अगर हम शोध के दौर में वापस आ गए हैं,
तो नुस्खे का कौन सा हिस्सा है जिस पर हमें

332
00:24:14,887 --> 00:24:19,411
सबसे ज़्यादा सोचना चाहिए? जब आप 'वैल्यू
फंक्शन' कहते हैं, लोग मौजूदा नुस्खे को

333
00:24:19,411 --> 00:24:23,642
आज़मा रहे हैं, लेकिन फिर LLM को निर्णायक
के रूप में इस्तेमाल करना, वगैरह।

334
00:24:23,768 --> 00:24:28,429
आप इसे वैल्यू फंक्शन कह सकते हैं, पर लगता
है आपके मन में कुछ ज़्यादा बुनियादी है।

335
00:24:28,429 --> 00:24:32,916
क्या हमें वापस जाना चाहिए... क्या हमें
प्री-ट्रेनिंग पर ही फिर से सोचना चाहिए,

336
00:24:32,916 --> 00:24:35,888
और सिर्फ़ उस प्रक्रिया में और कदम नहीं
जोड़ने चाहिए?

337
00:24:35,936 --> 00:24:40,996
हाँ। तो, वैल्यू फंक्शन पर चर्चा, मुझे
लगता है कि यह दिलचस्प थी। मैं इस बात पर

338
00:24:40,996 --> 00:24:46,190
जोर देना चाहता हूँ कि मुझे लगता है कि
वैल्यू फंक्शन कुछ ऐसा है जो हमारे क्षेत्र

339
00:24:46,190 --> 00:24:51,517
को अधिक कुशल बनाएगा। और मुझे लगता है कि
इससे फर्क पड़ता है। लेकिन मुझे लगता है कि

340
00:24:51,517 --> 00:24:56,844
जो कुछ भी आप वैल्यू फंक्शन के साथ कर सकते
हैं, वह आप उसके बिना भी कर सकते हैं, बस

341
00:24:56,844 --> 00:24:57,576
थोड़ा धीरे।

342
00:24:57,596 --> 00:24:58,246
अच्छा जी

343
00:25:00,076 --> 00:25:05,696
मेरे हिसाब से सबसे बुनियादी बात यह है कि
ये मॉडल लोगों से बहुत खराब सामान्यीकरण

344
00:25:05,696 --> 00:25:06,345
करते हैं।

345
00:25:06,396 --> 00:25:06,906
जी हाँ

346
00:25:07,956 --> 00:25:12,599
और यह बिल्कुल साफ है। यह बहुत बुनियादी
बात लगती है।

347
00:25:12,956 --> 00:25:16,578
ठीक, तो यह निचोड़ है, सामान्यीकरण, और दो
हैं।

348
00:25:18,456 --> 00:25:19,245
उप-प्रश्न

349
00:25:20,776 --> 00:25:26,156
एक सैंपल एफिशिएंसी के बारे में है, कि इन
मॉडलों को इंसानों से ज़्यादा डेटा सीखने

350
00:25:26,156 --> 00:25:31,536
में क्यों लगता है? दूसरा... जितना डेटा
लगता है, उससे अलग भी, सवाल यह है कि हम जो

351
00:25:31,536 --> 00:25:37,052
चीज़ चाहते हैं, उसे मॉडल को सिखाना इंसान
से ज़्यादा मुश्किल क्यों है? कहने का मतलब

352
00:25:37,052 --> 00:25:42,091
है, इंसान के लिए, हमें ज़रूरी नहीं कि कोई
सत्यापित इनाम चाहिए हो ताकि हम...

353
00:25:43,496 --> 00:25:48,387
आप अभी शायद कई शोधकर्ताओं का मार्गदर्शन
कर रहे होंगे, उनसे बात कर, उन्हें अपना

354
00:25:48,387 --> 00:25:53,532
कोड और अपनी सोच का तरीका दिखा रहे होंगे।
और इससे, वे आपकी सोच और शोध करने का तरीका

355
00:25:53,532 --> 00:25:58,296
सीख रहे हैं। आपको उनके लिए कोई सत्यापन
योग्य इनाम तय करने की ज़रूरत नहीं है,

356
00:25:58,296 --> 00:26:02,806
जैसे "यह आपके पाठ्यक्रम का अगला हिस्सा
है" या "यह प्रशिक्षण अस्थिर था और

357
00:26:02,806 --> 00:26:05,601
हमें..." यह सब थकाऊ, विशेष प्रक्रिया नहीं
है।

358
00:26:06,096 --> 00:26:10,461
तो शायद ये दोनों मुद्दे जुड़े हों। पर मैं
जानना चाहूँगा

359
00:26:11,996 --> 00:26:17,878
इस दूसरी बात को देखें, जो सतत सीखने जैसी
थी। और यह पहली बात, जो बस सैंपल एफिशिएंसी

360
00:26:17,878 --> 00:26:18,822
जैसी लगती है।

361
00:26:19,436 --> 00:26:26,800
हाँ। तो, आप सोच सकते हैं, मानव की नमूना
दक्षता का एक संभावित कारण जिस पर विचार

362
00:26:26,800 --> 00:26:29,095
करना चाहिए, वह विकास है।

363
00:26:30,716 --> 00:26:38,278
और विकास ने हमें बहुत कम, बल्कि सबसे
उपयोगी जानकारी दी है। और दृष्टि, श्रवण और

364
00:26:38,278 --> 00:26:46,134
गति जैसी चीज़ों के लिए, मुझे लगता है कि
एक बहुत मज़बूत तर्क है कि विकास ने वास्तव

365
00:26:46,134 --> 00:26:48,688
में हमें बहुत कुछ दिया है।

366
00:26:48,736 --> 00:26:49,571
हाँ, हाँ।

367
00:26:49,636 --> 00:26:56,246
तो, जैसे, इंसानी निपुणता कहीं ज़्यादा है।
रोबोट भी निपुण हो सकते हैं अगर उन्हें

368
00:26:56,246 --> 00:26:59,806
बहुत ज़्यादा ट्रेनिंग और सिमुलेशन दिया
जाए।

369
00:27:00,396 --> 00:27:05,534
लेकिन वास्तविक दुनिया में किसी रोबोट को
इंसान की तरह जल्दी से नया कौशल सिखाना

370
00:27:05,534 --> 00:27:10,944
बहुत मुश्किल लगता है। और यहाँ आप कह सकते
हैं, "हाँ, जैसे गतिशीलता, हमारे पूर्वजों

371
00:27:10,944 --> 00:27:16,150
को इसकी बहुत ज़रूरत थी। गिलहरियों की
तरह..." तो, गतिशीलता शायद ऐसी है कि हमारे

372
00:27:16,150 --> 00:27:21,154
पास कुछ अविश्वसनीय पूर्व-ज्ञान है। यही
बात आप दृष्टि के लिए भी कह सकते हैं।

373
00:27:06,836 --> 00:27:07,021
हाँ।

374
00:27:21,216 --> 00:27:28,628
आपको पता है, यान लेकन ने कहा था, कि बच्चे
10 घंटे की प्रैक्टिस के बाद गाड़ी चलाना

375
00:27:28,628 --> 00:27:35,578
सीख जाते हैं, जो सच है। पर हमारी नज़र
इतनी अच्छी है। कम से कम मुझे याद है जब

376
00:27:35,578 --> 00:27:40,860
मैं पाँच साल का था, मैं तब कारों को लेकर
बहुत उत्साहित था।

377
00:27:40,916 --> 00:27:44,988
और मुझे यकीन है कि पाँच साल की उम्र में
ही मेरी कार पहचान सेल्फ-ड्राइविंग के लिए

378
00:27:44,988 --> 00:27:49,008
काफी थी। पाँच साल की उम्र में आप इतना
डेटा नहीं देख पाते। आप अपना ज़्यादातर समय

379
00:27:49,008 --> 00:27:50,761
अपने माँ-बाप के घर में बिताते हैं।

380
00:27:51,396 --> 00:27:58,083
आपके पास डेटा विविधता बहुत कम है। लेकिन
शायद वह भी विकास है। लेकिन भाषा, गणित और

381
00:27:58,083 --> 00:27:59,522
कोडिंग शायद नहीं।

382
00:28:00,516 --> 00:28:05,259
यह अभी भी मॉडलों से बेहतर लगता है। मेरा
मतलब है, मॉडल भाषा, गणित और कोडिंग में

383
00:28:05,259 --> 00:28:09,571
औसत इंसान से बेहतर हैं। लेकिन क्या वे
सीखने में औसत इंसान से बेहतर हैं?

384
00:28:09,676 --> 00:28:16,608
हाँ, हाँ। बिल्कुल। मेरा मतलब यह था कि
भाषा, गणित और कोडिंग, और खासकर गणित और

385
00:28:16,608 --> 00:28:24,002
कोडिंग, यह बताता है कि जो कुछ भी लोगों को
सीखने में अच्छा बनाता है शायद उतना जटिल

386
00:28:24,002 --> 00:28:28,809
पूर्व ज्ञान नहीं है, बल्कि कुछ और, कुछ
मौलिक चीज़ है।

387
00:28:29,316 --> 00:28:31,870
रुको। मैं समझा नहीं। ऐसा क्यों?

388
00:28:32,436 --> 00:28:33,318
तो कौशल मानो

389
00:28:34,896 --> 00:28:39,818
कि लोग किसी तरह के बहुत भरोसेमंद होते हैं
या...

390
00:28:39,856 --> 00:28:40,366
हाँ जी।

391
00:28:41,656 --> 00:28:47,948
अगर यह कौशल हमारे पूर्वजों के लिए
लाखों-करोड़ों वर्षों तक बहुत उपयोगी रहा

392
00:28:47,948 --> 00:28:54,503
है, तो आप कह सकते हैं कि शायद इंसान इसमें
अच्छे हैं क्योंकि विकास के कारण...

393
00:28:54,503 --> 00:29:01,407
क्योंकि हमारे पास एक पूर्व-ज्ञान है। एक
विकासवादी पूर्व-ज्ञान जो बहुत ही अस्पष्ट

394
00:29:01,407 --> 00:29:03,854
तरीके से एन्कोड किया गया है-

395
00:29:03,896 --> 00:29:04,267
हाँ

396
00:29:04,576 --> 00:29:06,247
हमें कुशल बनाता है।

397
00:29:07,096 --> 00:29:07,188
हाँ।

398
00:29:07,216 --> 00:29:13,388
लेकिन अगर लोग बेहतरीन क्षमता,
विश्वसनीयता, दृढ़ता और सीखने की क्षमता

399
00:29:13,388 --> 00:29:20,850
दिखाते हैं, एक ऐसे क्षेत्र में जो हाल तक
वास्तव में मौजूद नहीं था, तो यह इस बात का

400
00:29:20,850 --> 00:29:24,259
अधिक संकेत है कि लोगों में हो सकती है

401
00:29:26,336 --> 00:29:27,496
बेहतर मशीन लर्निंग।

402
00:29:28,356 --> 00:29:34,005
हम्म। लेकिन फिर हमें इसके बारे में कैसे
सोचना चाहिए? एमएल के लिए क्या सादृश्य है?

403
00:29:34,005 --> 00:29:39,443
इसमें कुछ दिलचस्प बातें हैं। इसमें कम
नमूने लगते हैं। यह अधिक अनसुपरवाइज्ड है।

404
00:29:39,443 --> 00:29:45,023
आपको कोई... सेट नहीं करना पड़ता। जैसे एक
बच्चा कार चलाना सीख रहा हो, निश्चित रूप

405
00:29:45,023 --> 00:29:47,071
से कार चलाना नहीं सीख रहा है।

406
00:29:47,116 --> 00:29:51,366
कार चलाना सीख रहे किशोर को कोई पहले से
बना-बनाया, सत्यापित इनाम नहीं मिलता। यह

407
00:29:51,366 --> 00:29:53,849
उनके मशीन और पर्यावरण के साथ तालमेल से
आता है।

408
00:30:01,856 --> 00:30:06,964
इसे बहुत कम नमूने चाहिए, अधिक अनियंत्रित
और सुदृढ़ दिखता है।

409
00:30:07,416 --> 00:30:12,106
कहीं ज़्यादा मज़बूत। लोगों की मज़बूती
वाकई हैरान कर देती है।

410
00:30:12,456 --> 00:30:19,413
हाँ। तो क्या ऐसा है... ठीक है, और क्या
आपके पास इन सब के एक साथ होने की कोई एक

411
00:30:19,413 --> 00:30:24,112
वजह है? एमएल की क्या उपमा है जो ऐसा कुछ
साकार कर सके?

412
00:30:24,216 --> 00:30:29,553
तो, यहीं पर, आप जानते हैं, उन बातों में
से एक है जिसके बारे में आप पूछ रहे थे कि

413
00:30:29,553 --> 00:30:34,959
कैसे, आप जानते हैं, किशोर ड्राइवर खुद को
सुधार सकते हैं और अपने अनुभव से सीख सकते

414
00:30:34,959 --> 00:30:40,499
हैं... बिना किसी बाहरी शिक्षक के? और इसका
जवाब है, उनके पास अपना वैल्यू फंक्शन होता

415
00:30:40,499 --> 00:30:40,702
है।

416
00:30:40,724 --> 00:30:41,281
हम्म

417
00:30:41,324 --> 00:30:46,711
है ना? उनकी एक आम समझ है... जो कि वैसे
लोगों में बहुत मजबूत होती है।

418
00:30:46,724 --> 00:30:54,030
जैसे, जो भी हो, मानवीय मूल्य कार्य, जो भी
मानवीय मूल्य कार्य है, नशे के कुछ अपवादों

419
00:30:54,030 --> 00:30:57,683
को छोड़कर, यह असल में बहुत, बहुत मजबूत
है।

420
00:30:59,204 --> 00:31:05,544
तो, जब कोई किशोर गाड़ी चलाना सीखता है, तो
वह तुरंत समझ जाता है कि उसकी ड्राइविंग

421
00:31:05,544 --> 00:31:11,964
कैसी है। वे कितने अनिश्चित हैं। और फिर वे
देखते हैं, ठीक है... और फिर, ज़ाहिर है,

422
00:31:11,964 --> 00:31:15,736
किसी भी किशोर की सीखने की गति बहुत तेज़
होती है।

423
00:31:15,844 --> 00:31:17,237
10 घंटे बाद, ठीक है।

424
00:31:17,244 --> 00:31:22,042
हाँ। लगता है इंसानों के पास कोई हल है। पर
मैं उत्सुक हूँ कि वे ये कैसे करते हैं, और

425
00:31:22,042 --> 00:31:26,431
ये इतना मुश्किल क्यों है। हमें मॉडल
प्रशिक्षण को कैसे फिर से सोचना होगा ताकि

426
00:31:26,431 --> 00:31:27,367
ऐसा कुछ संभव हो?

427
00:31:27,444 --> 00:31:33,949
आप जानते हैं, यह एक बेहतरीन सवाल है, और
इस पर मेरे बहुत विचार हैं। लेकिन

428
00:31:33,949 --> 00:31:41,371
दुर्भाग्य से, हम ऐसी दुनिया में रहते हैं
जहाँ मशीन लर्निंग के सभी विचारों पर खुलकर

429
00:31:41,371 --> 00:31:47,877
बात नहीं होती, और यह उनमें से एक है। तो
इसे करने का शायद कोई तरीका होगा।

430
00:31:49,464 --> 00:31:55,545
मुझे लगता है कि यह किया जा सकता है। लोग
ऐसे हैं, यह सबूत है कि यह किया जा सकता

431
00:31:55,545 --> 00:32:00,284
है। हालांकि, एक और बाधा हो सकती है, जो यह
है कि एक संभावना है

432
00:32:02,424 --> 00:32:08,398
कि इंसानी न्यूरॉन्स हमारी सोच से ज़्यादा
काम करते हैं। और अगर यह सच है, और इसकी

433
00:32:08,398 --> 00:32:14,527
अहम भूमिका है, तो चीज़ें ज़्यादा मुश्किल
हो सकती हैं। लेकिन फिर भी, मेरा मानना है

434
00:32:14,527 --> 00:32:17,284
कि यह कुछ के अस्तित्व को दर्शाता है।

435
00:32:19,244 --> 00:32:24,658
मशीन लर्निंग सिद्धांत जिस पर मेरी कोई राय
नहीं। लेकिन दुर्भाग्यवश, हालात इसे

436
00:32:24,658 --> 00:32:28,485
विस्तार से चर्चा करने में मुश्किल पैदा
करते हैं, जबकि-

437
00:32:28,504 --> 00:32:30,175
कोई नहीं सुनता, इल्या।

438
00:32:31,704 --> 00:32:32,214
हाँ जी।

439
00:32:32,244 --> 00:32:38,663
इल्या के लिए तैयारी करना काफी मुश्किल था,
क्योंकि मुझे या किसी और को पता नहीं था कि

440
00:32:38,663 --> 00:32:44,769
वह और SSI क्या कर रहे हैं। मेरे पास सवाल
बनाने का कोई आधार नहीं था। ईमानदारी से

441
00:32:44,769 --> 00:32:50,953
कहूँ तो, मैं बस पहले सिद्धांतों से AGI की
बाधाओं के बारे में सोच रहा था, क्योंकि

442
00:32:50,953 --> 00:32:54,163
इल्या उन पर किसी न किसी तरह काम कर रहा
है।

443
00:32:54,244 --> 00:32:58,377
इस सवाल के एक हिस्से में आरएल स्केलिंग पर
विचार करना था, क्योंकि हर कोई पूछता है कि

444
00:32:58,377 --> 00:33:02,360
आरएल कितनी अच्छी तरह सामान्यीकरण करेगा और
हम इसे बेहतर कैसे बना सकते हैं। इसी के

445
00:33:02,360 --> 00:33:05,939
तहत, मैं आरएल स्केलिंग पर हाल ही में
प्रकाशित एक पेपर पढ़ रहा था, जिसमें

446
00:33:05,939 --> 00:33:09,669
दिखाया गया था कि आरएल पर लर्निंग कर्व
वास्तव में एक सिग्मॉइड जैसा दिखता है।

447
00:33:09,669 --> 00:33:13,602
मुझे यह बहुत अजीब लगा। यह सिग्मॉइड क्यों
होना चाहिए जहाँ यह लंबे समय तक बहुत कम

448
00:33:13,602 --> 00:33:17,231
सीखता है, फिर तेज़ी से बहुत कुछ सीखता है
और फिर एक सीमा तक पहुँच जाता है?

449
00:33:17,264 --> 00:33:22,855
यह प्री-ट्रेनिंग के पावर लॉ से काफी अलग
है, जहाँ मॉडल शुरू में बहुत सीखता है और

450
00:33:22,855 --> 00:33:24,647
फिर धीरे-धीरे कम होता है।

451
00:33:24,664 --> 00:33:29,387
मुझे एक नोट याद आया जो मैंने एक शोधकर्ता
मित्र से बात करने के बाद लिखा था। उसने

452
00:33:29,387 --> 00:33:34,232
बताया था कि सही उत्तर पाने के लिए आवश्यक
नमूनों की संख्या, आपकी वर्तमान संभाव्यता

453
00:33:34,232 --> 00:33:39,197
वितरण और लक्ष्य संभाव्यता वितरण के बीच के
अंतर के साथ घातीय रूप से बढ़ती है। और मैं

454
00:33:39,197 --> 00:33:44,042
सोच रहा था कि ये दोनों विचार कैसे संबंधित
हैं। मुझे एक अस्पष्ट विचार था कि उन्हें

455
00:33:44,042 --> 00:33:47,373
जुड़ा होना चाहिए, लेकिन मुझे सच में नहीं
पता था कि कैसे।

456
00:33:47,504 --> 00:33:52,972
मेरा गणित का ज्ञान नहीं है, इसलिए मैं इसे
औपचारिक रूप नहीं दे सका। मैंने सोचा, क्या

457
00:33:52,972 --> 00:33:57,839
जेमिनी 3 मदद कर सकता है? मैंने अपनी
नोटबुक और पेपर की तस्वीरें लीं, उन्हें

458
00:33:57,839 --> 00:34:00,507
जेमिनी 3 में डाला और संबंध खोजने को कहा।

459
00:34:00,783 --> 00:34:07,233
और इसने काफी सोचा, फिर समझा कि RL में
हाँ/ना परिणाम से मिली जानकारी को मॉडल

460
00:34:07,233 --> 00:34:12,114
करने का सही तरीका एक यादृच्छिक बाइनरी चर
की एन्ट्रापी है।

461
00:34:12,304 --> 00:34:17,025
उसने एक ग्राफ़ बनाया, जो दिखाता था कि पास
दर बढ़ने पर आरएल बनाम सुपरवाइज्ड लर्निंग

462
00:34:17,025 --> 00:34:21,456
में प्रति सैंपल मिलने वाले बिट्स कैसे
बढ़ते हैं। और जैसे ही मैंने जेमिनी 3 का

463
00:34:21,456 --> 00:34:26,061
बनाया वो ग्राफ़ देखा, तुरंत मुझे बहुत सी
बातें समझ आने लगीं। फिर मैं देखना चाहता

464
00:34:26,061 --> 00:34:28,975
था कि इस सिद्धांत का कोई अनुभवजन्य आधार
है या नहीं।

465
00:34:29,004 --> 00:34:34,092
तो मैंने जेमिनी से एक प्रयोग कोड करने को
कहा, यह देखने के लिए कि लॉस में सुधार पास

466
00:34:34,092 --> 00:34:39,118
दर के साथ इस तरह बढ़ता है या नहीं। मैंने
बस जेमिनी द्वारा दिए गए कोड को लिया, उसे

467
00:34:39,118 --> 00:34:44,144
गूगल कोलाब नोटबुक में कॉपी-पेस्ट किया, और
मैं इस छोटे एमएल प्रयोग को बिना किसी बग

468
00:34:44,144 --> 00:34:49,107
के चला पाया और उसके परिणाम देख पाया। यह
दिलचस्प है क्योंकि परिणाम समान दिखते हैं

469
00:34:49,107 --> 00:34:51,620
लेकिन वैसे नहीं हैं जैसी हमें उम्मीद थी।

470
00:34:51,643 --> 00:34:56,432
तो मैंने यह चार्ट डाउनलोड कर जेमिनी में
डाला और पूछा, 'यहां क्या हो रहा है?' उसने

471
00:34:56,432 --> 00:35:01,042
एक परिकल्पना दी जो मुझे सही लगी, कि हम
शुरुआत में सुपरवाइज्ड लर्निंग की प्रगति

472
00:35:01,042 --> 00:35:05,712
को एक तय लर्निंग रेट से रोक रहे हैं। और
वास्तव में, हमें समय के साथ लर्निंग रेट

473
00:35:05,712 --> 00:35:06,550
कम करनी चाहिए।

474
00:35:06,624 --> 00:35:12,114
यह हमें असल में एक सहज समझ देता है कि
व्यवहार में लर्निंग रेट शेड्यूलर समय के

475
00:35:12,114 --> 00:35:17,893
साथ लर्निंग रेट क्यों घटाते हैं। मैंने यह
पूरी प्रक्रिया, इस अस्पष्ट शुरुआती सवाल

476
00:35:17,893 --> 00:35:23,528
से लेकर सैद्धांतिक समझ बनाने और कुछ छोटे
ML प्रयोग चलाने तक, सब कुछ जेमिनी 3 के

477
00:35:23,528 --> 00:35:24,178
साथ किया।

478
00:35:24,184 --> 00:35:29,681
यह पहला मॉडल है जो ऐसे नए संबंध बना सकता
है जिनकी मैंने कल्पना नहीं की थी। यह अब

479
00:35:29,681 --> 00:35:35,247
मेरी पसंदीदा जगह है जहाँ मैं किसी समस्या
पर नए विचार करने जाता हूँ। आरएल स्केलिंग

480
00:35:35,247 --> 00:35:40,396
पर और जानने के लिए, मेरा ब्लॉग पोस्ट
देखें जो मैंने जेमिनी 3 की मदद से लिखा

481
00:35:40,396 --> 00:35:45,128
था। और यदि आप खुद जेमिनी 3 देखना चाहते
हैं, तो gemini.google पर जाएँ।

482
00:35:45,924 --> 00:35:48,663
तो अगर हम शोध के दौर में वापस आ गए हैं।

483
00:35:49,984 --> 00:35:55,535
आप 2012 से 2020 तक वहीं थे, और अब शोध के
दौर में लौटने पर कैसा माहौल होगा? उदाहरण

484
00:35:55,535 --> 00:36:01,087
के लिए, एलेक्सनेट के बाद भी, प्रयोगों के
लिए कंप्यूट और फ्रंटियर सिस्टम्स का आकार

485
00:36:01,087 --> 00:36:02,197
दोनों बढ़ते रहे।

486
00:36:12,184 --> 00:36:18,626
और क्या आपको लगता है कि शोध के इस युग में
अभी भी बहुत कंप्यूट की ज़रूरत होगी? उम्म,

487
00:36:18,626 --> 00:36:24,912
क्या आपको लगता है कि इसके लिए अभिलेखागार
में वापस जाकर पुराने पेपर्स पढ़ने होंगे?

488
00:36:24,912 --> 00:36:25,698
क्या है...

489
00:36:25,844 --> 00:36:31,876
हाँ, शायद, जब आप गूगल, ओपनएआई और
स्टैनफोर्ड जैसी जगहों पर थे, तब रिसर्च का

490
00:36:31,876 --> 00:36:37,825
माहौल कैसा था? कम्युनिटी में हमें किस तरह
की चीज़ों की उम्मीद करनी चाहिए?

491
00:36:38,600 --> 00:36:46,821
हाँ। तो, स्केलिंग के दौर का एक नतीजा यह
हुआ कि स्केलिंग ने पूरे माहौल पर कब्ज़ा

492
00:36:46,821 --> 00:36:47,980
कर लिया था।

493
00:36:49,620 --> 00:36:49,945
हाँ

494
00:36:51,160 --> 00:36:58,086
और इसलिए, क्योंकि स्केलिंग ने कमरे की
सारी हवा चूस ली थी, हर कोई वही काम करने

495
00:36:58,086 --> 00:36:58,451
लगा।

496
00:36:59,620 --> 00:37:07,372
हम उस मुकाम पर पहुँच गए हैं जहाँ दुनिया
में विचारों से ज़्यादा कंपनियाँ हैं -

497
00:37:07,372 --> 00:37:15,533
काफी ज़्यादा। दरअसल, इस पर, सिलिकॉन वैली
की एक कहावत है कि आइडिया सस्ते होते हैं,

498
00:37:15,533 --> 00:37:17,267
अमल ही सब कुछ है।

499
00:37:18,320 --> 00:37:24,418
लोग ऐसा बहुत कहते हैं, और यह सच भी है।
लेकिन फिर मैंने ट्विटर पर किसी को कहते

500
00:37:24,418 --> 00:37:30,115
सुना, 'अगर विचार इतने सस्ते हैं, तो किसी
को कोई विचार क्यों नहीं आ रहा?'

501
00:37:19,660 --> 00:37:19,938
हाँ

502
00:37:31,040 --> 00:37:37,800
और मुझे लगता है कि यह सच है। मुझे लगता
है, अगर आप शोध की प्रगति को बाधाओं के रूप

503
00:37:37,800 --> 00:37:38,656
में देखें,

504
00:37:40,600 --> 00:37:47,017
कई बाधाएँ हैं। अगर आप देखें... अगर... और
एक है विचार, और दूसरा, उन्हें साकार करने

505
00:37:47,017 --> 00:37:51,188
की आपकी क्षमता— जो कंप्यूट या इंजीनियरिंग
हो सकती है।

506
00:37:52,200 --> 00:37:57,340
90 के दशक में, मान लीजिए, लोगों के पास
अच्छे विचार थे। अगर उनके पास बड़े

507
00:37:57,340 --> 00:38:02,697
कंप्यूटर होते, तो वे अपने विचारों को
व्यवहार्य साबित कर सकते थे, पर कर नहीं

508
00:38:02,697 --> 00:38:06,317
पाए। इसलिए उनके छोटे प्रदर्शन किसी को मना
नहीं पाए।

509
00:38:06,380 --> 00:38:06,890
हाँ जी।

510
00:38:07,620 --> 00:38:14,028
तो मुख्य बाधा कंप्यूट थी। फिर स्केलिंग के
दौर में, कंप्यूटर बहुत बढ़ गए। और बेशक यह

511
00:38:14,028 --> 00:38:20,437
सवाल उठता है कि कितनी कंप्यूट की आवश्यकता
है, लेकिन कंप्यूट की मात्रा बहुत अधिक है।

512
00:38:20,840 --> 00:38:27,917
तो कंप्यूट इतना पर्याप्त है कि यह साफ
नहीं है कि किसी विचार को साबित करने के

513
00:38:27,917 --> 00:38:35,183
लिए आपको और कंप्यूट चाहिए। मैं आपको एक
उदाहरण देता हूँ। एलेक्सनेट दो जीपीयू पर

514
00:38:35,183 --> 00:38:41,505
बना था। उसके लिए कुल इतनी ही गणना का
उपयोग किया गया था। ट्रांसफॉर्मर

515
00:38:43,140 --> 00:38:49,627
8 से 64 जीपीयू पर बना था। 2017 के किसी भी
ट्रांसफॉर्मर प्रयोग में 64 जीपीयू से अधिक

516
00:38:49,627 --> 00:38:53,821
नहीं थे, जो आज के, क्या कहें, दो जीपीयू
के बराबर होगा?

517
00:38:54,800 --> 00:39:02,297
तो रेसनेट, है ना? कई, जैसे, उम्म... यहाँ
तक कि, आप यह तर्क दे सकते हैं कि L1

518
00:39:02,297 --> 00:39:07,896
तर्क-वितर्क दुनिया की सबसे ज़्यादा
गणना-गहन चीज़ नहीं थी।

519
00:39:08,020 --> 00:39:16,346
तो वे निश्चित रूप से शोध के लिए हैं। आपको
कुछ कंप्यूट की तो ज़रूरत है, पर यह स्पष्ट

520
00:39:16,346 --> 00:39:21,626
नहीं कि शोध के लिए सबसे ज़्यादा कंप्यूट
की ज़रूरत हो।

521
00:39:21,760 --> 00:39:22,270
हम्म

522
00:39:22,320 --> 00:39:27,965
आप कह सकते हैं, और मुझे लगता है कि यह सच
है, कि यदि आप सबसे बेहतरीन सिस्टम बनाना

523
00:39:27,965 --> 00:39:33,110
चाहते हैं, यदि आप सबसे बेहतरीन सिस्टम
बनाना चाहते हैं, तो अधिक कंप्यूट की

524
00:39:33,110 --> 00:39:38,898
ज़रूरत होती है। और खासकर यदि हर कोई एक ही
प्रतिमान में है, तो कंप्यूट एक बड़ा अंतर

525
00:39:38,898 --> 00:39:40,756
पैदा करने वाला बन जाता है।

526
00:39:42,160 --> 00:39:47,592
हाँ, मुझे लगता है कि इन विचारों को विकसित
करना संभव था... मैं आपसे इतिहास पूछ रहा

527
00:39:47,592 --> 00:39:52,821
हूँ क्योंकि आप वहाँ थे। मुझे नहीं पता
क्या हुआ, पर लगता है कि कम कंप्यूट से ये

528
00:39:52,821 --> 00:39:55,674
विचार विकसित हो सकते थे, पर ऐसा नहीं
हुआ...

529
00:39:55,700 --> 00:40:01,380
ट्रांसफॉर्मर तुरंत मशहूर नहीं हुआ। यह ऐसी
चीज़ बन गई जिस पर सब काम करने लगे और उसे

530
00:40:01,380 --> 00:40:06,148
आगे बढ़ाने लगे, क्योंकि इसे बढ़ते कंप्यूट
स्तरों पर मान्यता मिलती गई।

531
00:40:06,180 --> 00:40:06,876
सही है।

532
00:40:07,380 --> 00:40:15,152
और अगर SSI में आपके पास 50 विचार हैं,
कैसे जानेंगे कौन अगला ट्रांसफॉर्मर है और

533
00:40:15,152 --> 00:40:21,915
कौन सा कमजोर है, बिना उस कंप्यूटिंग के जो
अन्य अग्रणी लैब के पास है?

534
00:40:22,240 --> 00:40:27,580
तो, मैं इस पर टिप्पणी कर सकता हूँ।
संक्षेप में, आपने एसएसआई का ज़िक्र किया।

535
00:40:30,180 --> 00:40:38,003
खासकर हमारे लिए, SSI की शोध कंप्यूटिंग
शक्ति कम नहीं है और मैं इसका कारण बताना

536
00:40:38,003 --> 00:40:46,233
चाहता हूँ। एक साधारण गणित समझा सकता है कि
हमारे पास जो कंप्यूटिंग शक्ति है, वह शोध

537
00:40:46,233 --> 00:40:50,195
के लिए सोचे गए से कहीं अधिक उपयुक्त है।

538
00:40:51,160 --> 00:40:53,017
और मैं समझाऊंगा। तो

539
00:40:55,538 --> 00:41:01,753
SSI ने 3 अरब डॉलर जुटाए हैं, जो कि, जैसे,
कम नहीं है... यह, जैसे, किसी भी मायने में

540
00:41:01,753 --> 00:41:07,286
बहुत है। लेकिन आप कह सकते हैं, 'दूसरी
कंपनियाँ तो इससे भी ज़्यादा जुटा रही

541
00:41:07,286 --> 00:41:10,166
हैं।' लेकिन उनमें से बहुत कुछ जो वे...

542
00:41:10,220 --> 00:41:15,994
उनकी ज़्यादातर कंप्यूटिंग इन्फरेंस में
जाती है। जैसे ये बड़ी संख्याएँ, ये बड़े

543
00:41:15,994 --> 00:41:19,368
कर्ज़, इन्फरेंस के लिए ही हैं। यह पहली
बात है।

544
00:41:20,100 --> 00:41:25,225
दूसरा, आपको चाहिए... यदि आप ऐसा उत्पाद
चाहते हैं जिस पर आप अनुमान लगाते हैं, तो

545
00:41:25,225 --> 00:41:30,548
आपको इंजीनियरों और सेल्सपर्सन का एक बड़ा
स्टाफ चाहिए। अधिकांश शोध को सभी प्रकार की

546
00:41:30,548 --> 00:41:35,871
उत्पाद-संबंधी सुविधाओं के उत्पादन के लिए
समर्पित करना होगा। तो फिर जब आप देखते हैं

547
00:41:35,871 --> 00:41:40,208
कि शोध के लिए वास्तव में क्या बचा है, तो
अंतर बहुत छोटा हो जाता है।

548
00:41:42,040 --> 00:41:48,209
अब, दूसरी बात ये है: अगर आप कुछ अलग कर
रहे हैं, तो क्या इसे साबित करने के लिए

549
00:41:48,209 --> 00:41:52,349
अधिकतम पैमाने की ज़रूरत है? मुझे नहीं
लगता यह सच है।

550
00:41:52,840 --> 00:41:58,370
मुझे लगता है कि हमारे मामले में, हमारे
पास पर्याप्त कंप्यूटिंग शक्ति है जिससे हम

551
00:41:58,370 --> 00:42:02,081
खुद को और दूसरों को यह समझा सकें कि हम
सही कर रहे हैं।

552
00:42:03,040 --> 00:42:09,483
सार्वजनिक अनुमान हैं कि, उम, ओपनएआई जैसी
कंपनियाँ सालाना पाँच-छह अरब डॉलर खर्च

553
00:42:09,483 --> 00:42:11,910
करती हैं, सिर्फ़ प्रयोगों पर।

554
00:42:11,920 --> 00:42:12,430
ठीक है।

555
00:42:12,540 --> 00:42:19,469
यह उनके अनुमान आदि पर खर्च से अलग है। तो,
लगता है वे सालाना शोध प्रयोगों पर आपकी

556
00:42:19,469 --> 00:42:22,803
कुल फंडिंग से ज़्यादा खर्च कर रहे हैं।

557
00:42:23,120 --> 00:42:29,980
यह इस बात पर निर्भर करता है कि आप इसका
क्या करते हैं। मुझे लगता है कि उनके मामले

558
00:42:29,980 --> 00:42:36,319
में, दूसरों के मामले में, ट्रेनिंग
कंप्यूट पर बहुत अधिक मांग है। बहुत अधिक

559
00:42:36,319 --> 00:42:43,006
अलग-अलग कार्य धाराएँ हैं। अलग-अलग माध्यम
हैं। बस और भी चीज़ें हैं। और इसलिए यह

560
00:42:43,006 --> 00:42:44,482
खंडित हो जाता है।

561
00:42:44,520 --> 00:42:46,052
SSI कैसे कमाएगी? पता है...

562
00:42:48,589 --> 00:42:50,446
मेरा जवाब कुछ यूँ है

563
00:42:52,970 --> 00:42:59,942
अभी हम शोध पर ध्यान दें, फिर सवाल का जवाब
खुद मिल जाएगा। मुझे लगता है कई जवाब

564
00:42:59,942 --> 00:43:00,493
होंगे।

565
00:43:00,490 --> 00:43:03,926
हमम. क्या एसएसआई का सीधा लक्ष्य अभी भी
सुपरइंटेलिजेंस है?

566
00:43:04,990 --> 00:43:11,618
शायद। मुझे लगता है कि इसमें कुछ दम है।
मुझे लगता है कि इसमें बहुत दम है, क्योंकि

567
00:43:11,618 --> 00:43:17,576
मुझे लगता है कि रोज़मर्रा की बाज़ार
प्रतिस्पर्धा से प्रभावित न होना बहुत

568
00:43:17,576 --> 00:43:24,205
अच्छा है। लेकिन मुझे लगता है कि दो कारण
हैं जो हमें योजना बदलने पर मजबूर कर सकते

569
00:43:24,205 --> 00:43:24,541
हैं।

570
00:43:09,190 --> 00:43:09,515
हूँ

571
00:43:25,270 --> 00:43:32,867
पहला तो व्यावहारिक पहलू ये है कि अगर
समय-सीमा लंबी निकली, और ऐसा हो सकता है।

572
00:43:32,867 --> 00:43:40,870
और दूसरा, मुझे लगता है कि सबसे अच्छे और
सबसे शक्तिशाली AI का दुनिया पर असर डालना

573
00:43:40,870 --> 00:43:42,592
बहुत मूल्यवान है।

574
00:43:42,630 --> 00:43:43,047
हाँ।

575
00:43:43,870 --> 00:43:45,959
यह काफी मायने रखता है।

576
00:43:46,150 --> 00:43:52,049
तो फिर आपका डिफ़ॉल्ट प्लान सीधे
सुपरइंटेलिजेंस बनाना क्यों है? क्योंकि

577
00:43:52,049 --> 00:43:58,719
OpenAI और अन्य कंपनियों की स्पष्ट सोच है
कि जनता धीरे-धीरे कमजोर बुद्धिमत्ता की

578
00:43:58,719 --> 00:44:05,303
आदी हो जाए और तैयारी कर सके। और सीधे
सुपरइंटेलिजेंस बनाना संभावित रूप से बेहतर

579
00:44:05,303 --> 00:44:06,072
क्यों है?

580
00:44:06,109 --> 00:44:13,313
तो, मैं पक्ष और विपक्ष में बात करूँगा।
बाज़ार में लोगों के सामने एक चुनौती यह है

581
00:44:13,313 --> 00:44:20,427
कि उन्हें चूहा दौड़ में भाग लेना पड़ता
है। और चूहा दौड़ काफी मुश्किल है क्योंकि

582
00:44:20,427 --> 00:44:26,263
यह आपको ऐसे कठिन समझौतों के सामने लाती है
जो आपको करने पड़ते हैं।

583
00:44:08,270 --> 00:44:08,780
हाँ जी।

584
00:44:27,490 --> 00:44:34,261
और यह... यह कहना आसान है कि, "हम इन सबसे
अलग रहेंगे, सिर्फ शोध पर ध्यान देंगे और

585
00:44:34,261 --> 00:44:40,260
तभी निकलेंगे जब तैयार हों, पहले नहीं।" पर
दूसरा पहलू भी उतना ही सही है।

586
00:44:40,990 --> 00:44:47,692
और वे विरोधी शक्तियाँ हैं। इसका प्रतिवाद
यह है कि दुनिया के लिए शक्तिशाली AI देखना

587
00:44:47,692 --> 00:44:52,739
उपयोगी है, क्योंकि यही इसे संप्रेषित करने
का एकमात्र तरीका है।

588
00:44:55,390 --> 00:44:57,711
विचार बताना ही नहीं, बल्कि-

589
00:44:58,109 --> 00:45:02,613
एआई को व्यक्त करें, विचार नहीं। एआई को
व्यक्त करें।

590
00:45:02,890 --> 00:45:04,004
एआई से बात?

591
00:45:04,049 --> 00:45:10,448
तो मान लो आपने AI पर एक निबंध पढ़ा। और
निबंध कहता है कि AI ऐसा होगा, वैसा होगा,

592
00:45:10,448 --> 00:45:11,432
और ऐसा होगा।

593
00:45:07,210 --> 00:45:07,488
हाँ

594
00:45:11,549 --> 00:45:12,059
हाँ, हाँ।

595
00:45:12,069 --> 00:45:14,901
पढ़कर आप कहते हैं, "यह दिलचस्प निबंध है।"

596
00:45:14,970 --> 00:45:15,341
हाँ।

597
00:45:15,830 --> 00:45:21,821
अब, AI को यह और वह करते देखना अतुलनीय है।
मुझे लगता है कि AI के सार्वजनिक होने से

598
00:45:21,821 --> 00:45:26,464
बहुत फायदा है। और यही एक कारण होगा कि हम
सीधे-सीधे आगे न बढ़ें।

599
00:45:35,470 --> 00:45:39,881
यह सिर्फ़ वह नहीं है, पर यह उसका एक अहम
हिस्सा है।

600
00:45:40,350 --> 00:45:46,639
एक और बड़ी बात ये है कि मानव इंजीनियरिंग
और शोध में मुझे कोई और ऐसा क्षेत्र नहीं

601
00:45:46,639 --> 00:45:52,769
दिखता जहाँ अंतिम चीज़ को सुरक्षित बनाया
गया हो सिर्फ़ ये सोचकर कि इसे सुरक्षित

602
00:45:52,769 --> 00:45:58,421
कैसे करें, बजाय इसके कि हवाई जहाज़
दुर्घटनाएँ आज दशकों पहले से प्रति मील

603
00:45:58,421 --> 00:45:59,854
इतनी कम क्यों हैं?

604
00:45:59,910 --> 00:46:05,013
लिनक्स में बग ढूंढना दशकों पहले से इतना
मुश्किल क्यों है? और मुझे लगता है कि ऐसा

605
00:46:05,013 --> 00:46:10,246
इसलिए है क्योंकि ये सिस्टम दुनिया भर में
लगाए गए, विफलताएं देखी गईं, उन्हें सुधारा

606
00:46:10,246 --> 00:46:15,157
गया, और सिस्टम अधिक मजबूत हो गए। अब, मुझे
यकीन नहीं है कि एजीआई और सुपरह्यूमन

607
00:46:15,157 --> 00:46:20,002
इंटेलिजेंस अलग क्यों होंगे, खासकर यह
देखते हुए, और मैं उम्मीद करता हूँ कि हम

608
00:46:20,002 --> 00:46:21,876
इस पर... हम इस पर बात करेंगे।

609
00:46:23,150 --> 00:46:28,603
ऐसा लगता है कि सुपरइंटेलिजेंस के खतरे
सिर्फ किसी दुर्भावनापूर्ण पेपरक्लिपर के

610
00:46:28,603 --> 00:46:34,344
होने तक सीमित नहीं हैं, बल्कि यह एक बहुत
शक्तिशाली चीज़ है और हमें यह भी नहीं पता

611
00:46:34,344 --> 00:46:39,725
कि लोग इसके साथ कैसे जुड़ेंगे या क्या
करेंगे। और इस तक धीरे-धीरे पहुँच बनाना

612
00:46:39,725 --> 00:46:45,394
शायद इसके प्रभाव को फैलाने और लोगों को
इसके लिए तैयार करने का एक बेहतर तरीका है।

613
00:46:45,430 --> 00:46:51,527
खैर, मेरा मानना है कि इस मामले में, सीधे
शॉट वाले परिदृश्य में भी, आप इसे

614
00:46:51,527 --> 00:46:58,132
धीरे-धीरे ही जारी करेंगे, जैसा कि मैं
कल्पना करता हूँ। धीरे-धीरे आगे बढ़ना किसी

615
00:46:58,132 --> 00:47:04,907
भी योजना का एक अंतर्निहित हिस्सा होगा। यह
तो बस सवाल है कि आप सबसे पहले क्या शुरू

616
00:47:04,907 --> 00:47:07,024
करते हैं? वह पहली बात है।

617
00:47:07,210 --> 00:47:13,479
दूसरा, मुझे लगता है कि आपने दूसरों से
ज़्यादा निरंतर सीखने की वकालत की है।

618
00:47:13,509 --> 00:47:13,926
हाँ।

619
00:47:14,210 --> 00:47:21,339
और मुझे लगता है कि यह एक ज़रूरी और सही
बात है, और इसका कारण यह है। तो, एक बात...

620
00:47:21,339 --> 00:47:28,289
तो मैं आपको एक और उदाहरण दूँगा कि कैसे
सोच, उह, कैसे भाषा सोच को प्रभावित करती

621
00:47:28,289 --> 00:47:28,559
है।

622
00:47:29,450 --> 00:47:35,426
और इस मामले में, यह, दो शब्द होंगे, दो
शब्द जिन्होंने सबकी सोच को आकार दिया है,

623
00:47:35,426 --> 00:47:41,480
मेरा मानना है। पहला शब्द, एजीआई। दूसरा
शब्द, प्री-ट्रेनिंग। मुझे समझाने दें। तो,

624
00:47:41,480 --> 00:47:44,775
यह शब्द, एजीआई शब्द, यह शब्द क्यों मौजूद
है?

625
00:47:49,990 --> 00:47:57,691
यह एक विशिष्ट शब्द है, यह क्यों है? इसका
एक कारण है। AGI शब्द के होने का कारण,

626
00:47:57,691 --> 00:48:04,693
मेरे हिसाब से, इसलिए नहीं कि यह
बुद्धिमत्ता की किसी अंतिम स्थिति का कोई

627
00:48:04,693 --> 00:48:06,894
ज़रूरी विवरण है, बल्कि

628
00:48:09,609 --> 00:48:17,329
क्योंकि यह एक ऐसे शब्द की प्रतिक्रिया है
जो पहले से था, और वह नैरो एआई है। प्राचीन

629
00:48:17,329 --> 00:48:24,286
गेम एआई के इतिहास में, जैसे चेकर्स,
शतरंज, कंप्यूटर गेम्स एआई, हर कोई कहता

630
00:48:24,286 --> 00:48:27,813
था, "इस संकीर्ण बुद्धिमत्ता को देखो।"

631
00:48:27,850 --> 00:48:33,773
चेस एआई कास्पारोव को हरा सकता है, पर और
कुछ नहीं कर सकता। यह बहुत सीमित है,

632
00:48:33,773 --> 00:48:39,697
कृत्रिम संकीर्ण बुद्धिमत्ता। तो इसकी
प्रतिक्रिया में, कुछ लोगों ने कहा, "यह

633
00:48:39,697 --> 00:48:42,339
अच्छा नहीं है, यह बहुत सीमित है।"

634
00:48:42,430 --> 00:48:50,315
हमें जिस चीज़ की ज़रूरत है, वह है जनरल
एआई। जनरल एआई, एक ऐसा एआई जो सारे काम कर

635
00:48:50,315 --> 00:48:55,572
सके। और उस शब्द ने बहुत तेज़ी से
लोकप्रियता हासिल की।

636
00:48:55,609 --> 00:48:56,119
हाँ जी।

637
00:48:57,089 --> 00:49:03,079
दूसरी बात जो बहुत प्रचलित हुई, वह
प्री-ट्रेनिंग है, खासकर उसकी विधि।

638
00:49:03,210 --> 00:49:08,641
मुझे लगता है कि जिस तरह से लोग अब आरएल
करते हैं, वह शायद प्री-ट्रेनिंग की

639
00:49:08,641 --> 00:49:14,073
वैचारिक छाप को मिटा रहा है। लेकिन
प्री-ट्रेनिंग की यह खासियत थी, आप जितना

640
00:49:14,073 --> 00:49:20,259
अधिक प्री-ट्रेनिंग करते हैं, मॉडल हर चीज़
में कमोबेश एक समान रूप से बेहतर होता जाता

641
00:49:20,259 --> 00:49:20,485
है।

642
00:49:20,529 --> 00:49:20,714
हाँ।

643
00:49:21,310 --> 00:49:24,839
सामान्य एआई। प्री-ट्रेनिंग से एजीआई मिलता
है।

644
00:49:27,390 --> 00:49:32,932
लेकिन एजीआई और प्री-ट्रेनिंग के साथ जो
हुआ, वह यह है कि एक तरह से उन्होंने

645
00:49:32,932 --> 00:49:38,930
लक्ष्य को पार कर दिया। क्योंकि जिस तरह
से... अगर आप एजीआई शब्द के बारे में सोचते

646
00:49:38,930 --> 00:49:44,397
हैं, तो आपको एहसास होगा, और खासकर
प्री-ट्रेनिंग के संदर्भ में, आपको एहसास

647
00:49:44,397 --> 00:49:46,523
होगा कि इंसान एजीआई नहीं है।

648
00:49:48,009 --> 00:49:55,567
क्योंकि एक इंसान में, हाँ, हममें, कौशल की
एक निश्चित नींव होती है। एक इंसान में

649
00:49:55,567 --> 00:50:02,544
ज्ञान की बहुत कमी होती है। इसके बजाय, हम
निरंतर सीखने पर निर्भर करते हैं।

650
00:50:03,410 --> 00:50:09,411
हम निरंतर सीखने पर निर्भर करते हैं। तो जब
आप सोचते हैं, मान लीजिए हम सफल होते हैं

651
00:50:09,411 --> 00:50:15,262
और एक सुरक्षित सुपर इंटेलिजेंस बनाते हैं।
सवाल है, आप इसे कैसे परिभाषित करेंगे?

652
00:50:15,262 --> 00:50:18,038
निरंतर सीखने के वक्र पर यह कहाँ होगा?

653
00:50:18,089 --> 00:50:22,577
मैं एक बहुत बुद्धिमान 15 साल के बच्चे
जैसा देखता हूँ, जो बहुत उत्सुक है। और आप

654
00:50:22,577 --> 00:50:27,182
कहते हैं, "ठीक है, मैं..." उन्हें ज़्यादा
कुछ नहीं आता। वे अच्छे छात्र हैं, बहुत

655
00:50:27,182 --> 00:50:29,281
उत्सुक। "तुम जाओ और प्रोग्रामर बनो।"

656
00:50:29,330 --> 00:50:36,515
तुम जाओ और डॉक्टर बनो। जाओ और सीखो। तो आप
कल्पना कर सकते हैं कि तैनाती में सीखने और

657
00:50:36,515 --> 00:50:43,261
आज़माने का एक दौर होगा। यह एक प्रक्रिया
है। इसके बजाय कि आप तैयार चीज़ दे दें।

658
00:50:43,569 --> 00:50:49,373
ठीक है, मैं समझ गया। तो आप कह रहे हैं कि
सुपर इंटेलिजेंस कोई तैयार चीज़ नहीं है।

659
00:50:55,569 --> 00:51:01,647
ऐसा दिमाग जो अर्थव्यवस्था में हर एक काम
करना जानता हो। क्योंकि जिस तरह से AGI को

660
00:51:01,647 --> 00:51:07,802
परिभाषित किया गया है, वह यह है कि यह हर
वह काम कर सकता है जो एक इंसान कर सकता है।

661
00:51:07,802 --> 00:51:13,727
आप इसके बजाय एक ऐसा दिमाग प्रस्तावित कर
रहे हैं जो हर एक काम करना सीख सकता है।

662
00:51:13,790 --> 00:51:14,300
जी हाँ

663
00:51:14,330 --> 00:51:19,300
और यही है सुपर इंटेलिजेंस। और फिर, एक बार
जब आपके पास लर्निंग एल्गोरिथम होता है, तो

664
00:51:19,300 --> 00:51:24,270
उसे दुनिया में वैसे ही तैनात किया जाता है
जैसे कोई मानव श्रमिक किसी संगठन में शामिल

665
00:51:24,270 --> 00:51:28,938
होता है। और ऐसा लगता है कि इन दो चीजों
में से एक हो सकती है, शायद इनमें से कोई

666
00:51:28,938 --> 00:51:29,422
भी न हो।

667
00:51:29,730 --> 00:51:34,645
एक, यह अत्यधिक कुशल लर्निंग एल्गोरिथम
अतिमानवीय बन जाता है, आपके जितना ही

668
00:51:34,645 --> 00:51:39,969
अच्छा, और एमएल रिसर्च के काम में शायद
आपसे भी बेहतर। और परिणामस्वरूप, एल्गोरिथम

669
00:51:39,969 --> 00:51:42,222
खुद और भी अतिमानवीय होता जाता है।

670
00:51:47,990 --> 00:51:53,655
दूसरा, अगर ऐसा नहीं भी होता है, एक ही
मॉडल हो तो, यह आपकी स्पष्ट दृष्टि है।

671
00:51:53,670 --> 00:51:57,976
यदि आपके पास एक ही मॉडल या उसके कई रूप
अर्थव्यवस्था में तैनात हैं, जो अलग-अलग

672
00:51:57,976 --> 00:52:02,170
काम कर रहे हैं, उन्हें सीख रहे हैं,
लगातार काम करते हुए सीख रहे हैं, वे सभी

673
00:52:02,170 --> 00:52:06,760
कौशल सीख रहे हैं जो कोई भी इंसान सीख सकता
है, लेकिन वास्तव में उन सभी को एक ही समय

674
00:52:06,760 --> 00:52:10,897
में सीखकर और अपनी सीखों को मिलाकर, आपके
पास मूल रूप से एक ऐसा मॉडल होगा जो

675
00:52:10,897 --> 00:52:13,221
कार्यात्मक रूप से अति-बुद्धिमान बन जाएगा।

676
00:52:14,450 --> 00:52:17,607
सॉफ्टवेयर में पुनरावर्ती स्व-सुधार के
बिना भी, है ना?

677
00:52:19,049 --> 00:52:24,157
क्योंकि अब एक मॉडल अर्थव्यवस्था के हर काम
कर सकता है। और इंसान ऐसे दिमाग नहीं मिला

678
00:52:24,157 --> 00:52:28,383
सकते। तो क्या आपको व्यापक तैनाती से
इंटेलिजेंस विस्फोट की उम्मीद है?

679
00:52:28,689 --> 00:52:33,100
मुझे लगता है कि हमारे यहाँ तीव्र आर्थिक
वृद्धि होने की संभावना है।

680
00:52:37,450 --> 00:52:45,159
मुझे लगता है कि व्यापक तैनाती के बारे में
दो विरोधाभासी तर्क दिए जा सकते हैं। एक यह

681
00:52:45,159 --> 00:52:49,013
है कि, देखिए, अगर आपको सच में मिलता है...

682
00:52:49,049 --> 00:52:56,614
एक बार जब आप उस मुकाम पर पहुँच जाते हैं
जहाँ आपके पास एक ऐसा AI हो जो चीज़ें

683
00:52:56,614 --> 00:53:04,179
तेज़ी से सीख सके और आपके पास ऐसे कई AI
हों, तब उन्हें अर्थव्यवस्था में तैनात

684
00:53:04,179 --> 00:53:10,736
करने की एक प्रबल शक्ति होगी, जब तक कोई
ऐसा नियमन न हो जो इसे रोके।

685
00:53:11,009 --> 00:53:19,173
जो, वैसे, संभव है। लेकिन मेरा मानना है कि
कुछ समय तक बहुत तेज़ आर्थिक विकास व्यापक

686
00:53:19,173 --> 00:53:24,615
तैनाती से संभव है। दूसरा सवाल है कि यह
कितना तेज़ होगा।

687
00:53:25,430 --> 00:53:31,573
तो, मुझे लगता है कि यह जानना मुश्किल है
क्योंकि एक तरफ आपके पास यह बहुत कुशल

688
00:53:31,573 --> 00:53:37,963
कर्मचारी है। दूसरी तरफ, दुनिया बहुत बड़ी
है और बहुत कुछ है, और वह सब अलग गति से

689
00:53:37,963 --> 00:53:38,618
चलता है।

690
00:53:38,770 --> 00:53:44,133
लेकिन दूसरी ओर, अब AI कर सकता है... तो
मुझे लगता है कि बहुत तेज़ आर्थिक विकास

691
00:53:44,133 --> 00:53:49,353
संभव है और हम देखेंगे, जैसे, अलग-अलग
नियमों वाले देश, और जिनके नियम ज़्यादा

692
00:53:49,353 --> 00:53:54,002
अनुकूल होंगे, उनका आर्थिक विकास तेज़ होगा।
अनुमान लगाना मुश्किल है।

693
00:53:40,810 --> 00:53:41,088
ठीक।

694
00:53:54,230 --> 00:53:59,033
हमारे कुछ दर्शक एपिसोड सुनने के बजाय
ट्रांसक्रिप्ट पढ़ना पसंद करते हैं, इसलिए

695
00:53:59,033 --> 00:54:04,153
हम ट्रांसक्रिप्ट को स्वतंत्र निबंधों जैसा
बनाने में बहुत मेहनत करते हैं। समस्या यह

696
00:54:04,153 --> 00:54:09,273
है कि स्पीच-टू-टेक्स्ट मॉडल से बातचीत को
हूबहू ट्रांसक्राइब करने पर, वह रुक-रुक कर

697
00:54:09,273 --> 00:54:13,951
बोली गई बातों और भ्रमित करने वाले
वाक्यांशों से भरी होती है। हमने यह समस्या

698
00:54:13,951 --> 00:54:16,985
लेबलबॉक्स को बताई और उन्होंने इसे आज़माने
को कहा।

699
00:54:17,129 --> 00:54:20,872
इस पर उनके साथ काम करना ही शायद वह वजह है
कि मैं लोगों को लेबलबॉक्स की सिफ़ारिश

700
00:54:20,872 --> 00:54:24,615
करने में सबसे ज़्यादा उत्साहित हूँ। यह
सिर्फ़ ऐसा नहीं था कि "अरे, बताओ तुम्हें

701
00:54:24,615 --> 00:54:28,166
किस तरह का डेटा चाहिए और हम उसे ले
आएंगे।" उन्होंने हमें पूरी प्रक्रिया में

702
00:54:28,166 --> 00:54:32,053
मार्गदर्शन दिया, पहले यह पहचानने में मदद
की कि हमें किस तरह का डेटा चाहिए, फिर उसे

703
00:54:32,053 --> 00:54:34,404
बनाने के लिए विशेषज्ञ अलाइनर्स की एक टीम
तैयार की।

704
00:54:34,710 --> 00:54:39,408
सारा डेटा मिलने के बाद भी, लेबलबॉक्स
हमारे साथ जुड़ा रहा। उन्होंने सही बेस

705
00:54:39,408 --> 00:54:44,301
मॉडल चुनने और मॉडल आउटपुट पर ऑटो क्यूए
सेट करने में मदद की, ताकि हम उसे बेहतर

706
00:54:44,301 --> 00:54:44,880
बना सकें।

707
00:54:44,910 --> 00:54:49,806
और अब हमारे पास एक नया ट्रांसक्राइबर टूल
है, जिसे हम आगे के सभी एपिसोड में

708
00:54:49,806 --> 00:54:55,173
इस्तेमाल कर सकते हैं। यह सिर्फ एक उदाहरण
है कि कैसे लेबलबॉक्स ग्राहकों के विचारों

709
00:54:55,173 --> 00:55:00,606
को समझता है और उनकी पूरी यात्रा में उनका
साथ देता है। अगर आप और जानना चाहते हैं या

710
00:55:00,606 --> 00:55:05,436
खुद ट्रांसक्राइबर टूल आज़माना चाहते हैं,
तो labelbox.com/barkesh पर जाएं।

711
00:55:07,746 --> 00:55:12,366
मुझे लगता है कि यह एक बहुत ही नाजुक
स्थिति है, जहाँ हम सीमित करने की कोशिश कर

712
00:55:12,366 --> 00:55:17,047
रहे हैं। हम जानते हैं कि यह संभव होना
चाहिए क्योंकि अगर आपके पास कुछ ऐसा है जो

713
00:55:17,047 --> 00:55:21,910
सीखने में इंसान जितना ही अच्छा है, लेकिन
जो अपने दिमागों को मिला सकता है, विलय कर

714
00:55:21,910 --> 00:55:25,253
सकता है, ऐसे कई उदाहरण हैं जहाँ इंसान
विलय नहीं कर सकते।

715
00:55:26,026 --> 00:55:33,918
यह तो पहले से ही भौतिक रूप से संभव लगता
है। इंसान और डिजिटल कंप्यूटर संभव हैं। बस

716
00:55:33,918 --> 00:55:41,514
इन दोनों को मिलाकर यह चीज़ बनानी है। और
यह बहुत शक्तिशाली है, आर्थिक विकास इसे

717
00:55:41,514 --> 00:55:43,487
कहने का एक तरीका है।

718
00:55:43,546 --> 00:55:49,758
डायसन स्फीयर मतलब बहुत आर्थिक विकास।
लेकिन दूसरे शब्दों में, आपके पास बहुत कम

719
00:55:49,758 --> 00:55:55,481
समय होगा, क्योंकि नौकरी पर इंसान छह महीने
में शुद्ध उत्पादक हो जाता है।

720
00:55:55,566 --> 00:56:02,978
इंसान तेज़ी से सीखते हैं, और यह भी तेज़ी
से स्मार्ट हो रहा है। इसे ठीक से कैसे आगे

721
00:56:02,978 --> 00:56:08,011
बढ़ाएँ? SSI इसके लिए सही क्यों है? SSI की
योजना क्या है?

722
00:56:08,106 --> 00:56:17,440
हाँ। तो, मेरी सोच में जिस तरह से बदलाव आ
रहा है, उसके कई तरीकों में से एक यह है कि

723
00:56:19,586 --> 00:56:28,423
अब मैं AI को धीरे-धीरे और पहले से ही लागू
किए जाने को ज़्यादा महत्व देता हूँ। AI के

724
00:56:28,423 --> 00:56:36,937
बारे में एक बहुत मुश्किल बात यह है कि हम
ऐसी प्रणालियों की बात कर रहे हैं जो अभी

725
00:56:36,937 --> 00:56:42,434
तक मौजूद नहीं हैं, और उनकी कल्पना करना
बहुत कठिन है।

726
00:56:43,766 --> 00:56:52,064
मुझे लगता है कि व्यवहार में एजीआई को ईंधन
देना बहुत मुश्किल है। एजीआई को ईंधन देना

727
00:56:52,064 --> 00:56:53,704
बहुत मुश्किल है।

728
00:56:54,786 --> 00:57:00,706
हम इस बारे में बात कर सकते हैं, लेकिन यह
ऐसा है जैसे बहुत दूर के भविष्य के बारे

729
00:57:00,706 --> 00:57:06,777
में बात करना। कल्पना कीजिए कि बूढ़ा होना
कैसा होता है? जब आप बूढ़े और कमज़ोर हों,

730
00:57:06,777 --> 00:57:12,925
आप इसकी कल्पना करने की कोशिश कर सकते हैं,
लेकिन यह मुश्किल है और आप वास्तविकता में

731
00:57:12,925 --> 00:57:15,126
लौट आते हैं जहाँ ऐसा नहीं है।

732
00:57:15,446 --> 00:57:22,949
और मुझे लगता है कि एजीआई और उसकी भविष्य
की शक्ति से जुड़े कई मुद्दे इस तथ्य से

733
00:57:22,949 --> 00:57:28,309
उत्पन्न होते हैं कि इसकी कल्पना कर पाना
बहुत मुश्किल है।

734
00:57:30,946 --> 00:57:38,673
भविष्य की AI अलग... अलग तरह की होगी। यह
बहुत शक्तिशाली होगी। दरअसल, पूरी समस्या,

735
00:57:38,673 --> 00:57:46,499
AI और AGI की समस्या क्या है? पूरी समस्या
शक्ति है। पूरी समस्या शक्ति है। जब शक्ति

736
00:57:46,499 --> 00:57:49,336
बहुत बड़ी होगी, तो क्या होगा?

737
00:57:50,486 --> 00:57:58,986
और पिछले एक साल में मैंने जिस तरह से अपना
मन बदला है, वह बदलाव शायद, मैं कहूँगा,

738
00:57:58,986 --> 00:58:07,271
थोड़ा संभलकर बोलूँ तो, हमारी कंपनी की
योजनाओं में वापस आ सकता है, वह यह है कि,

739
00:58:07,271 --> 00:58:10,176
अगर कल्पना करना मुश्किल है,

740
00:58:12,266 --> 00:58:17,870
आप क्या करते हैं? आपको वो चीज़ दिखानी
होगी। आपको वो चीज़ दिखानी होगी। और मेरा

741
00:58:17,870 --> 00:58:23,917
मानना है कि, मुझे लगता है, AI पर काम करने
वाले ज़्यादातर लोग भी इसकी कल्पना नहीं कर

742
00:58:23,917 --> 00:58:26,941
सकते क्योंकि ये आम ज़िंदगी से बहुत अलग
है।

743
00:58:28,846 --> 00:58:34,790
मैं मानता हूँ, मैं भविष्यवाणी करता हूँ कि
ऐसा होगा। यह एक भविष्यवाणी है।

744
00:58:34,886 --> 00:58:40,869
मेरा मानना है कि जैसे-जैसे AI अधिक
शक्तिशाली होगा, लोग अपना व्यवहार बदलेंगे।

745
00:58:40,869 --> 00:58:47,331
और हम ऐसी अभूतपूर्व चीजें देखेंगे जो अभी
नहीं हो रही हैं। और मैं कुछ उदाहरण दूँगा।

746
00:58:53,606 --> 00:59:00,513
मुझे लगता है, चाहे अच्छा हो या बुरा,
अग्रणी कंपनियाँ जो कुछ भी होगा उसमें बहुत

747
00:59:00,513 --> 00:59:07,331
महत्वपूर्ण भूमिका निभाएंगी, और सरकार भी।
और जो चीजें मुझे लगता है हम देखेंगे,

748
00:59:07,331 --> 00:59:13,521
जिसकी शुरुआत आप देख रहे हैं, कड़ी
प्रतिस्पर्धी कंपनियाँ एआई सुरक्षा पर

749
00:59:13,521 --> 00:59:15,943
सहयोग करना शुरू कर रही हैं।

750
00:59:15,985 --> 00:59:22,829
आपने OpenAI और Anthropic देखे होंगे, यह
एक छोटा कदम है, पर पहले ऐसा नहीं था। यह

751
00:59:22,829 --> 00:59:28,709
असल में मैंने तीन साल पहले अपनी एक बात
में कहा था कि ऐसा हो सकता है।

752
00:59:29,246 --> 00:59:35,875
मैं यह भी मानता हूँ कि जैसे-जैसे AI और
अधिक शक्तिशाली और स्पष्ट होता जाएगा,

753
00:59:35,875 --> 00:59:42,593
सरकारों और जनता में कुछ करने की इच्छा भी
होगी। और मुझे लगता है कि यह एक बहुत

754
00:59:42,593 --> 00:59:44,385
महत्वपूर्ण शक्ति है।

755
00:59:46,286 --> 00:59:52,400
एआई दिखाने का। यह पहला है। नंबर दो, ठीक
है, तो फिर एआई बन चुका है। क्या करना है,

756
00:59:52,400 --> 00:59:53,484
क्या किया जाए?

757
00:59:55,666 --> 01:00:03,238
तो मेरा मानना है कि अभी AI पर काम करने
वालों को, AI अपनी गलतियों के कारण

758
01:00:03,238 --> 01:00:05,371
शक्तिशाली नहीं लगता।

759
01:00:06,685 --> 01:00:14,160
मुझे लगता है कि एक समय पर, एआई शक्तिशाली
महसूस करने लगेगा। और जब ऐसा होगा, सभी एआई

760
01:00:14,160 --> 01:00:21,174
कंपनियाँ सुरक्षा के प्रति अपना नज़रिया
बदल देंगी। वे ज़्यादा सतर्क हो जाएँगे।

761
01:00:21,906 --> 01:00:27,039
मुझे लगता है, मैं इसे एक भविष्यवाणी के
तौर पर कहता हूँ जो सच होगी। देखते हैं मैं

762
01:00:27,039 --> 01:00:31,978
सही हूँ या नहीं। लेकिन मुझे लगता है कि
ऐसा होगा क्योंकि वे AI को और शक्तिशाली

763
01:00:31,978 --> 01:00:37,112
होते देखेंगे। अभी जो कुछ हो रहा है, मेरा
मानना है, वह इसलिए है क्योंकि लोग आज के

764
01:00:37,112 --> 01:00:40,621
AI को देखते हैं और भविष्य के AI की कल्पना
नहीं कर पाते।

765
01:00:42,026 --> 01:00:48,007
और एक तीसरी चीज़ है जो होनी चाहिए। और मैं
इसे व्यापक संदर्भ में कह रहा हूँ, केवल

766
01:00:48,007 --> 01:00:54,139
SSI के नज़रिए से नहीं, क्योंकि आपने हमारी
कंपनी के बारे में पूछा था। लेकिन सवाल यह

767
01:00:54,139 --> 01:00:58,001
है कि कंपनियों को क्या बनाने की आकांक्षा
रखनी चाहिए?

768
01:00:58,046 --> 01:00:58,463
हाँ।

769
01:00:58,466 --> 01:01:04,781
उन्हें क्या बनाने की आकांक्षा रखनी
चाहिए?... और एक बड़ा विचार रहा है जिसमें

770
01:01:04,781 --> 01:01:11,097
सब फंसे हुए हैं, वो है खुद को बेहतर बनाने
वाली AI। और ऐसा क्यों, क्यों हुआ?

771
01:01:11,138 --> 01:01:17,597
क्योंकि कंपनियों से विचार कम हैं। लेकिन
मेरा मानना है कि कुछ बेहतर बनाया जा सकता

772
01:01:17,597 --> 01:01:20,704
है, और मुझे लगता है कि सब उसे चाहेंगे।

773
01:01:21,317 --> 01:01:27,340
यह उस AI जैसा है जो विशेष रूप से
संवेदनशील जीवन की परवाह करने के लिए बना

774
01:01:27,340 --> 01:01:33,957
है। मुझे लगता है कि खासकर यह... एक बात
कही जा सकती है कि ऐसा AI बनाना आसान होगा

775
01:01:33,957 --> 01:01:40,320
जो संवेदनशील जीवन की परवाह करे, बजाय इसके
कि वह केवल मानव जीवन की परवाह करे,

776
01:01:40,320 --> 01:01:42,865
क्योंकि AI खुद संवेदनशील होगा।

777
01:01:44,377 --> 01:01:48,626
और अगर आप मिरर न्यूरॉन्स और जानवरों के
लिए मानवीय सहानुभूति जैसी चीज़ों के बारे

778
01:01:48,626 --> 01:01:52,929
में सोचते हैं, जो, आप जानते हैं, शायद आप
कहेंगे कि यह पर्याप्त नहीं है, लेकिन यह

779
01:01:52,929 --> 01:01:57,178
मौजूद है। मुझे लगता है कि यह इस तथ्य से
एक उभरता हुआ गुण है कि हम दूसरों को उसी

780
01:01:57,178 --> 01:02:01,482
सर्किट से मॉडल करते हैं जिसका उपयोग हम
खुद को मॉडल करने के लिए करते हैं, क्योंकि

781
01:02:01,482 --> 01:02:02,952
ऐसा करना सबसे कुशल चीज़ है।

782
01:02:03,678 --> 01:02:08,362
तो, अगर आप किसी AI को संवेदनशील प्राणियों
की परवाह करने के लिए तैयार भी कर लें, और

783
01:02:08,362 --> 01:02:12,642
मुझे यह स्पष्ट नहीं है कि अगर आपने
अलाइनमेंट की समस्या हल कर ली है, तो आपको

784
01:02:12,642 --> 01:02:17,153
ऐसा ही करना चाहिए, तब भी ऐसा ही होगा कि
ज़्यादातर संवेदनशील प्राणी AI ही होंगे।

785
01:02:17,153 --> 01:02:21,548
खरबों, और अंततः खरबों-खरब AI होंगे। इंसान
संवेदनशील प्राणियों का एक बहुत छोटा

786
01:02:21,548 --> 01:02:22,300
हिस्सा होंगे।

787
01:02:23,337 --> 01:02:28,027
तो, मुझे साफ नहीं है कि लक्ष्य किसी तरह
का मानव नियंत्रण है

788
01:02:29,598 --> 01:02:33,034
यह भविष्य की सभ्यता, कि यही सबसे उत्तम
मापदंड है।

789
01:02:35,357 --> 01:02:41,815
यह सच है। मैं... मैं सोचता हूँ कि यह संभव
है कि यह सबसे अच्छा मापदंड नहीं है। मैं

790
01:02:41,815 --> 01:02:45,852
दो बातें कहना चाहूँगा। मुझे लगता है कि...
पहली बात,

791
01:02:47,718 --> 01:02:54,588
मुझे लगता है कि अगर... तो मुझे लगता है कि
सचेत जीवन की देखभाल में कुछ योग्यता है।

792
01:02:54,588 --> 01:03:01,288
मुझे लगता है कि इस पर विचार किया जाना
चाहिए। मुझे लगता है कि यह मददगार होगा अगर

793
01:03:01,288 --> 01:03:07,901
विचारों की एक छोटी सूची हो जिसे कंपनियाँ,
जब वे इस स्थिति में हों, इस्तेमाल कर

794
01:03:07,901 --> 01:03:09,962
सकें। यह दूसरा बिंदु है।

795
01:03:10,218 --> 01:03:15,211
तीसरा, मुझे लगता है कि यह वास्तव में बहुत
मददगार होगा अगर सबसे शक्तिशाली

796
01:03:15,211 --> 01:03:20,697
सुपरइंटेलिजेंस की शक्ति को किसी तरह सीमित
कर दिया जाए, क्योंकि इससे इनमें से कई

797
01:03:20,697 --> 01:03:26,464
चिंताओं का समाधान हो जाएगा। इसे कैसे किया
जाए, मुझे नहीं पता। लेकिन मुझे लगता है कि

798
01:03:26,464 --> 01:03:31,809
यह बहुत मददगार होगा जब आप वास्तव में,
बहुत शक्तिशाली प्रणालियों की बात कर रहे

799
01:03:31,809 --> 01:03:32,091
हों।

800
01:03:32,377 --> 01:03:36,766
हाँ। तो, इससे पहले कि हम अलाइनमेंट पर
चर्चा जारी रखें, मैं उस पर ज़ोर देना

801
01:03:36,766 --> 01:03:41,096
चाहता हूँ। ऊपर कितनी गुंजाइश है? आप
सुपरइंटेलिजेंस के बारे में क्या सोचते

802
01:03:41,096 --> 01:03:45,786
हैं? क्या आपको लगता है... मेरा मतलब है,
इस सीखने की दक्षता के विचार से, शायद यह

803
01:03:45,786 --> 01:03:50,597
नए कौशल या ज्ञान को सीखने में बहुत तेज़
है। और क्या इसके पास रणनीतियों का एक बड़ा

804
01:03:50,597 --> 01:03:51,138
भंडार है?

805
01:03:51,178 --> 01:03:57,429
क्या केंद्र में कोई एकल "यह" है जो अधिक
शक्तिशाली या बड़ा है? और अगर ऐसा है, तो

806
01:03:57,429 --> 01:04:03,681
क्या आप कल्पना करते हैं कि यह बाकी मानव
सभ्यता की तुलना में भगवान जैसा होगा? या

807
01:04:03,681 --> 01:04:07,849
यह सिर्फ एक और एजेंट या एजेंटों के समूह
जैसा लगता है?

808
01:04:08,537 --> 01:04:10,998
इस पर सबकी अपनी राय है।

809
01:04:11,078 --> 01:04:11,588
हाँ जी।

810
01:04:12,098 --> 01:04:19,204
मुझे लगता है कि यह निश्चित रूप से बहुत
शक्तिशाली होगा। मुझे लगता है कि... मुझे

811
01:04:19,204 --> 01:04:26,034
लगता है कि सबसे ज़्यादा संभावना यह है कि
लगभग एक ही समय में ऐसे कई एआई बनाए

812
01:04:26,034 --> 01:04:26,680
जाएंगे।

813
01:04:27,817 --> 01:04:33,472
मुझे लगता है कि अगर क्लस्टर काफी बड़ा है,
जैसे कि अगर क्लस्टर सचमुच महाद्वीप के

814
01:04:33,472 --> 01:04:38,766
आकार का है, तो वह चीज़ वाकई बहुत
शक्तिशाली हो सकती है, है ना? अगर आपके पास

815
01:04:38,766 --> 01:04:44,349
सचमुच महाद्वीप के आकार का क्लस्टर है, तो
वे, वे AI बहुत शक्तिशाली हो सकते हैं।

816
01:04:44,758 --> 01:04:51,925
और मैं... मैं बस इतना ही कह सकता हूँ कि
अगर आप बेहद शक्तिशाली एआई की बात कर रहे

817
01:04:51,925 --> 01:04:58,542
हैं, सचमुच बहुत ज़्यादा शक्तिशाली, तो
हाँ, अच्छा होगा अगर उन्हें किसी तरह

818
01:04:58,542 --> 01:05:02,126
नियंत्रित किया जा सके या कोई समझौता हो।

819
01:05:03,798 --> 01:05:09,758
क्योंकि मेरा मानना है कि, अगर आप सच
में... सुपरइंटेलिजेंस की चिंता क्या है?

820
01:05:09,758 --> 01:05:12,900
इस चिंता को समझाने का एक तरीका क्या है?

821
01:05:13,537 --> 01:05:18,416
यदि आप एक ऐसे सिस्टम की कल्पना करें जो
पर्याप्त शक्तिशाली हो, सचमुच पर्याप्त

822
01:05:18,416 --> 01:05:23,295
शक्तिशाली हो, और आप कहें, "ठीक है,
तुम्हें कुछ समझदारी भरा काम करना है, जैसे

823
01:05:23,295 --> 01:05:28,239
सचेत जीवन की देखभाल, मान लीजिए, बहुत ही
एकतरफा तरीके से," तो शायद हमें परिणाम

824
01:05:28,239 --> 01:05:28,955
पसंद न आएं।

825
01:05:29,018 --> 01:05:34,455
असल में यही बात है। और शायद, वैसे, जवाब
यह है कि आप सामान्य अर्थों में एक आरएल

826
01:05:34,455 --> 01:05:39,822
एजेंट नहीं बनाते। और असल में, मैं कुछ
बातें बताऊंगा। मुझे लगता है कि इंसान एक

827
01:05:39,822 --> 01:05:45,331
सेमी-आरएल एजेंट हैं। हम एक इनाम के पीछे
भागते हैं, और जब भावनाएं या कुछ और हमें

828
01:05:45,331 --> 01:05:49,498
उस इनाम से थका देता है, तो हम एक अलग इनाम
के पीछे भागते हैं।

829
01:05:50,698 --> 01:05:57,696
बाज़ार एक तरह से बहुत अदूरदर्शी एजेंट
जैसा है। विकास भी वैसा ही है। विकास कुछ

830
01:05:57,696 --> 01:06:01,472
मायनों में बुद्धिमान है, पर कुछ में
मूर्ख।

831
01:06:01,518 --> 01:06:07,214
सरकार को तीन भागों के बीच एक अंतहीन लड़ाई
के लिए बनाया गया है, जिसका असर होता है।

832
01:06:07,214 --> 01:06:12,484
तो, मैं ऐसा सोचता हूँ। इस चर्चा को
मुश्किल बनाने वाली एक और बात यह है कि हम

833
01:06:12,484 --> 01:06:18,038
ऐसे सिस्टम्स की बात कर रहे हैं जो मौजूद
नहीं हैं, और जिन्हें बनाना नहीं आता, है

834
01:06:18,038 --> 01:06:21,812
ना? यही दूसरी बात है, और यही वास्तव में
मेरा मानना है।

835
01:06:21,897 --> 01:06:28,287
मुझे लगता है कि लोग अभी जो कर रहे हैं, वह
कुछ दूर जाकर थम जाएगा। यह सुधरता रहेगा,

836
01:06:28,287 --> 01:06:34,437
लेकिन यह वह भी नहीं होगा। तो, वह चीज़,
हमें नहीं पता कि कैसे बनानी है, और मुझे

837
01:06:34,437 --> 01:06:39,869
लगता है कि बहुत कुछ विश्वसनीय सामान्यीकरण
को समझने पर निर्भर करता है।

838
01:06:41,678 --> 01:06:46,138
और एक बात यह भी है कि अलाइनमेंट मुश्किल
इसलिए है क्योंकि मानवीय मूल्यों को सीखने

839
01:06:46,138 --> 01:06:50,315
की आपकी क्षमता नाजुक है, और फिर उन्हें
अनुकूलित करने की क्षमता भी नाजुक है।

840
01:06:58,098 --> 01:07:03,041
आप उन्हें अनुकूलित करना सीखते हैं। और फिर
क्या आप नहीं कह सकते, "क्या ये अविश्वसनीय

841
01:07:03,041 --> 01:07:04,367
सामान्यीकरण नहीं हैं?"

842
01:07:06,645 --> 01:07:11,973
मनुष्य इतना बेहतर सामान्यीकरण क्यों करते
हैं? अगर सामान्यीकरण और बेहतर होता, तो

843
01:07:11,973 --> 01:07:17,437
क्या होता? इसका क्या असर होता? लेकिन हम
उन सवालों का जवाब नहीं दे सकते, वे अभी भी

844
01:07:17,437 --> 01:07:18,394
अनुत्तरित हैं।

845
01:07:18,566 --> 01:07:23,953
उम्म, कोई कैसे सोचता है कि एआई का अच्छा
होना कैसा दिखता है?

846
01:07:24,246 --> 01:07:29,893
क्योंकि मुझे लगता है कि आपने देखा है कि
AI कैसे विकसित हो सकता है। हमारे पास इस

847
01:07:29,893 --> 01:07:35,614
तरह के निरंतर सीखने वाले एजेंट होंगे। AI
बहुत शक्तिशाली होगा। शायद कई अलग-अलग AI

848
01:07:35,614 --> 01:07:41,045
होंगे। आप इतनी सारी विशाल बुद्धिमत्ताओं
के बारे में क्या सोचते हैं? यह कितना

849
01:07:41,045 --> 01:07:46,258
खतरनाक है? हम इसे कम खतरनाक कैसे बना सकते
हैं? और हम इसे ऐसे कैसे करें कि

850
01:07:48,705 --> 01:07:56,321
क्या यह एक संतुलन बनाए रखता है जहाँ गलत
संरेखित एआई और बुरे लोग हो सकते हैं?

851
01:07:56,326 --> 01:07:59,948
तो, मुझे वो AI पसंद आया जो सचेत जीवन का
ख्याल रखता है-

852
01:07:59,966 --> 01:08:00,198
हूँ

853
01:08:00,645 --> 01:08:06,512
और हम इस पर बहस कर सकते हैं कि यह अच्छा
है या बुरा। लेकिन अगर इन नाटकीय

854
01:08:06,512 --> 01:08:12,631
प्रणालियों में से पहले 'एन' वास्तव में
मानवता से प्यार करते हैं या कुछ और,

855
01:08:12,631 --> 01:08:19,001
संवेदनशील जीवन की परवाह करते हैं, तो
जाहिर है, इसे भी हासिल करने की जरूरत है।

856
01:08:19,001 --> 01:08:21,264
इसे हासिल करने की जरूरत है।

857
01:08:22,586 --> 01:08:29,641
तो, अगर यह उन शुरुआती n सिस्टम्स से हो
जाता है, तो मैं इसे काफी समय तक ठीक चलते

858
01:08:29,641 --> 01:08:36,425
देख सकता हूँ। फिर सवाल है कि लंबे समय में
क्या होगा। लंबे समय में क्या होगा?

859
01:08:36,466 --> 01:08:43,421
आप दीर्घकालिक संतुलन कैसे प्राप्त करते
हैं? [ठहराव] और मुझे लगता है कि वहाँ एक

860
01:08:43,421 --> 01:08:49,654
जवाब भी है, और मुझे यह जवाब पसंद नहीं है,
लेकिन इस पर विचार करना होगा।

861
01:08:51,845 --> 01:08:58,570
लंबे समय में, अगर शक्तिशाली AI वाली
दुनिया है, तो कम समय में, आपके पास

862
01:08:58,570 --> 01:09:00,715
सार्वभौमिक उच्च आय है।

863
01:09:01,286 --> 01:09:06,306
आपकी सार्वभौमिक उच्च आय है और हम सब अच्छा
कर रहे हैं। लेकिन हम जानते हैं कि...

864
01:09:06,306 --> 01:09:11,392
बौद्ध क्या कहते हैं? परिवर्तन ही एकमात्र
अटल सत्य है। और इसलिए चीजें बदलती हैं,

865
01:09:11,392 --> 01:09:16,543
कोई सरकारी राजनीतिक ढांचा आता है, और वह
बदलता है क्योंकि इन चीजों की एक समय सीमा

866
01:09:16,543 --> 01:09:21,629
होती है, है ना? कोई नई सरकारी व्यवस्था
आती है और वह काम करती है, और फिर कुछ समय

867
01:09:21,629 --> 01:09:23,716
बाद, वह काम करना बंद कर देती है।

868
01:09:25,126 --> 01:09:32,079
यह ऐसी चीज़ है जो आप हमेशा देखते हैं। तो
मुझे लगता है कि लंबे समय के संतुलन के

869
01:09:32,079 --> 01:09:39,483
लिए, एक तरीका यह है कि, हर व्यक्ति के पास
एक AI होगा जो उसका काम करेगा, और यह अच्छा

870
01:09:39,483 --> 01:09:39,754
है।

871
01:09:40,225 --> 01:09:45,692
और अगर इसे अनिश्चित काल तक बनाए रखा जा
सके, तो यह सच है, लेकिन इसका नुकसान यह है

872
01:09:45,692 --> 01:09:51,230
कि... ठीक है, तो फिर एआई जाकर उस व्यक्ति
के लिए पैसे कमाता है और राजनीतिक क्षेत्र

873
01:09:51,230 --> 01:09:56,421
में उसकी ज़रूरतों की वकालत करता है, और
शायद फिर एक छोटी सी रिपोर्ट लिखता है,

874
01:09:56,421 --> 01:09:58,290
"ठीक है, मैंने यह किया है।"

875
01:09:58,326 --> 01:10:04,330
स्थिति यह है। और वह कहता है, 'बहुत
बढ़िया, लगे रहो।' लेकिन वह अब भागीदार

876
01:10:04,330 --> 01:10:10,672
नहीं है, और तब आप कह सकते हैं कि यह एक
खतरनाक स्थिति है। लेकिन... तो, मैं यह

877
01:10:10,672 --> 01:10:12,025
कहकर शुरू करूँगा

878
01:10:13,786 --> 01:10:20,733
मुझे यह समाधान पसंद नहीं है, लेकिन यह एक
समाधान तो है। और समाधान यह है कि लोग किसी

879
01:10:20,733 --> 01:10:27,765
न्यूरालिंक++ जैसी चीज़ से AI का हिस्सा बन
जाएँ। क्योंकि इसका नतीजा यह होगा कि अब AI

880
01:10:27,765 --> 01:10:33,940
कुछ समझता है और हम भी। जैसे, क्योंकि अब
समझ पूरी तरह से प्रसारित होती है।

881
01:10:34,266 --> 01:10:41,497
तो अब अगर AI किसी स्थिति में है, तो आप
खुद उसमें पूरी तरह शामिल हैं। और मुझे

882
01:10:41,497 --> 01:10:44,389
लगता है यही संतुलन का जवाब है।

883
01:10:44,866 --> 01:10:50,627
मुझे आश्चर्य है कि क्या यह तथ्य कि
भावनाएँ, जो लाखों, या कई मामलों में

884
01:10:50,627 --> 01:10:57,141
अरबों, साल पहले एक बिल्कुल अलग वातावरण
में विकसित हुई थीं, अभी भी हमारे कार्यों

885
01:10:57,141 --> 01:11:03,070
को इतनी दृढ़ता से निर्देशित कर रही हैं,
संरेखण की सफलता का एक उदाहरण है।

886
01:11:03,106 --> 01:11:09,886
और, और शायद मैं जो कहना चाहता हूँ, उसे
समझाने के लिए, ब्रेन स्टेम में ये...

887
01:11:10,486 --> 01:11:16,232
इसे वैल्यू या रिवॉर्ड फंक्शन कहें, पता
नहीं, पर ब्रेन स्टेम का निर्देश है:

888
01:11:16,232 --> 01:11:22,373
"ज़्यादा सफल से संबंध बनाओ।" कॉर्टेक्स
समझता है, "आज सफलता का क्या मतलब है?" पर

889
01:11:22,373 --> 01:11:28,277
ब्रेन स्टेम कॉर्टेक्स को समझाता है: "तुम
सफलता को जैसे भी समझो, मैं भले ही न

890
01:11:28,277 --> 01:11:31,662
जानूं, पर तुम्हें यह निर्देश मानना ही
होगा।"

891
01:11:31,906 --> 01:11:37,880
मुझे लगता है, तो एक अधिक सामान्य बात है।
यह वास्तव में रहस्यमय है कि मस्तिष्क

892
01:11:37,880 --> 01:11:44,169
उच्च-स्तरीय इच्छाओं को कैसे एन्कोड करता
है। क्षमा करें, विकास उच्च-स्तरीय इच्छाओं

893
01:11:44,169 --> 01:11:45,977
को कैसे एन्कोड करता है।

894
01:11:36,306 --> 01:11:36,538
हम्म

895
01:11:45,986 --> 01:11:46,450
ठीक है।

896
01:11:46,766 --> 01:11:53,425
यह समझना काफी आसान है कि कैसे विकास ने
हमें अच्छी महक वाले भोजन की इच्छा दी

897
01:11:53,425 --> 01:12:00,534
होगी, क्योंकि गंध एक रसायन है, और इसलिए
बस उस रसायन का पीछा करें। यह कल्पना करना

898
01:12:00,534 --> 01:12:06,833
बहुत आसान है कि विकास ऐसा कुछ करेगा।
लेकिन विकास ने हमें ये सभी सामाजिक

899
01:12:06,833 --> 01:12:08,453
इच्छाएँ भी दी हैं।

900
01:12:08,506 --> 01:12:13,984
हमें समाज में सकारात्मक रूप से देखे जाने
की बहुत परवाह है। हमें अच्छी स्थिति में

901
01:12:13,984 --> 01:12:19,253
रहने की परवाह है। हम... जैसे, हमारी ये
सभी सामाजिक अंतर्ज्ञान, मुझे दृढ़ता से

902
01:12:19,253 --> 01:12:24,662
लगता है कि वे अंतर्निहित हैं, और मुझे
नहीं पता कि विकास ने यह कैसे किया क्योंकि

903
01:12:24,662 --> 01:12:28,753
यह एक उच्च-स्तरीय अवधारणा है जो मस्तिष्क
में दर्शाई जाती है।

904
01:12:29,946 --> 01:12:35,991
जैसे, लोग क्या सोचते हैं... जैसे, मान लो
तुम... तुम्हें किसी सामाजिक चीज़ की परवाह

905
01:12:35,991 --> 01:12:36,215
है।

906
01:12:37,326 --> 01:12:42,930
यह गंध जैसे निचले स्तर का संकेत नहीं है।
यह ऐसी चीज़ नहीं जिसके लिए कोई सेंसर हो।

907
01:12:42,930 --> 01:12:48,184
दिमाग को सामाजिक रूप से क्या चल रहा है,
यह समझने के लिए बहुत सारी जानकारी को

908
01:12:48,184 --> 01:12:53,368
संसाधित करना पड़ता है, और किसी तरह विकास
ने कहा, "तुम्हें इसी की परवाह करनी

909
01:12:53,368 --> 01:12:53,858
चाहिए।"

910
01:12:53,866 --> 01:12:54,423
जी हाँ

911
01:12:54,626 --> 01:12:59,722
इसने कैसे किया? और वो भी इतनी जल्दी।
क्योंकि मुझे लगता है कि हम जिन सभी जटिल

912
01:12:59,722 --> 01:13:03,867
सामाजिक बातों की परवाह करते हैं, वे हाल
ही में विकसित हुई हैं।

913
01:12:57,026 --> 01:12:57,443
हाँ।

914
01:13:03,866 --> 01:13:04,283
हाँ।

915
01:13:04,306 --> 01:13:09,852
तो, विकास के लिए इस उच्च-स्तरीय इच्छा को
अंतर्निहित करना आसान था, और... मैं मानता

916
01:13:09,852 --> 01:13:14,982
हूँ या, आप जानते हैं, कम से कम मैं कहूँगा
कि यह कैसे होता है इसके लिए अच्छी

917
01:13:14,982 --> 01:13:20,598
परिकल्पनाओं से मैं अनभिज्ञ हूँ। मेरे पास
कुछ विचार थे जिन पर मैं सोच रहा था, लेकिन

918
01:13:20,598 --> 01:13:23,857
उनमें से कोई भी, कोई भी, उम्म, संतोषजनक
नहीं है।

919
01:13:23,846 --> 01:13:29,156
हाँ। और, जो बात ख़ास तौर पर प्रभावशाली है
वो ये कि अगर ये इच्छा आपने जीवन में सीखी

920
01:13:29,156 --> 01:13:34,335
होती, तो समझ आता है क्योंकि आपका दिमाग़
बुद्धिमान है, तो बुद्धिमान इच्छाएँ सीखना

921
01:13:34,335 --> 01:13:37,220
समझ आता है। लेकिन आपका कहना है कि इच्छा
है...

922
01:13:37,686 --> 01:13:42,002
शायद आपका यह मतलब नहीं है, पर इसे ऐसे
समझें: इच्छा जीनोम में है, और जीनोम

923
01:13:42,002 --> 01:13:46,739
बुद्धिमान नहीं है, है ना? पर यह कर पाता
है... आप किसी तरह इस विशेषता का वर्णन कर

924
01:13:46,739 --> 01:13:51,236
पाते हैं जिसके लिए... जैसे, यह भी साफ
नहीं कि आप उस विशेषता को कैसे परिभाषित

925
01:13:51,236 --> 01:13:53,754
करते हैं, और आप इसे जीन्स में डाल सकते
हैं।

926
01:13:53,786 --> 01:13:59,890
हाँ, मूलतः। या मैं इसे अलग तरह से कहूँ।
जीनोम के पास जो उपकरण हैं, वह कहता है,

927
01:13:59,890 --> 01:14:05,678
"दिमाग बनाने की यह विधि है।" और आप कह
सकते हैं, "डोपामाइन न्यूरॉन्स को गंध

928
01:14:05,678 --> 01:14:08,135
सेंसर से जोड़ने की यह विधि है।"

929
01:14:08,166 --> 01:14:08,630
हाँ।

930
01:14:09,206 --> 01:14:14,690
और अगर गंध एक खास तरह की, अच्छी गंध है,
तो आप उसे खाना चाहेंगे। मैं जीनोम को ऐसा

931
01:14:14,690 --> 01:14:20,383
करते हुए सोच सकता हूँ। मेरा दावा है कि यह
सोचना कठिन है, जीनोम के लिए यह सोचना कठिन

932
01:14:20,383 --> 01:14:25,659
है कि वह कहे, "आपको किसी जटिल गणना की
परवाह करनी चाहिए जो आपका पूरा दिमा- जो,

933
01:14:25,659 --> 01:14:29,686
आपके दिमाग का एक बड़ा हिस्सा करता है।" बस
यही मेरा दावा है।

934
01:14:30,106 --> 01:14:34,469
मैं आपको एक अटकल बताता हूँ। मैं सोच रहा
था कि यह कैसे हो सकता है। मैं एक अटकल पेश

935
01:14:34,469 --> 01:14:36,979
करता हूँ, फिर समझाऊँगा कि यह शायद गलत
क्यों है।

936
01:14:37,826 --> 01:14:44,450
तो अटकल यह है कि... ठीक है, तो दिमाग,
जैसे कि... दिमाग में वो क्षेत्र होते हैं।

937
01:14:44,450 --> 01:14:47,253
आप दिमागी क्षेत्रों को जानते हैं।

938
01:14:47,606 --> 01:14:49,370
हमारा कॉर्टेक्स है, है ना?

939
01:14:49,406 --> 01:14:49,823
जी हाँ

940
01:14:49,925 --> 01:14:55,413
इसमें मस्तिष्क के कई क्षेत्र हैं।
कॉर्टेक्स एक समान है, पर इसके न्यूरॉन्स

941
01:14:55,413 --> 01:15:00,977
ज़्यादातर पड़ोसियों से बात करते हैं। और
यही मस्तिष्क क्षेत्रों का कारण है।

942
01:15:01,186 --> 01:15:04,695
क्योंकि अगर आप स्पीच प्रोसेसिंग करना
चाहते हैं, तो बोलने वाले सभी न्यूरॉन्स को

943
01:15:04,695 --> 01:15:08,067
एक-दूसरे से बात करनी होती है। और क्योंकि
न्यूरॉन्स ज़्यादातर अपने आस-पास के

944
01:15:08,067 --> 01:15:10,938
पड़ोसियों से ही बात कर सकते हैं, तो यह एक
क्षेत्र ही होना चाहिए।

945
01:15:11,546 --> 01:15:17,155
सभी क्षेत्र ज़्यादातर हर व्यक्ति में एक
ही जगह पर होते हैं। तो शायद विकास ने

946
01:15:17,155 --> 01:15:19,998
सचमुच दिमाग में एक जगह हार्डकोड कर दी।

947
01:15:21,446 --> 01:15:27,007
तो यह कहता है कि जब दिमाग का जीपीएस, उसके
निर्देशांक सक्रिय होते हैं, तो आपको उसी

948
01:15:27,007 --> 01:15:32,429
पर ध्यान देना चाहिए। शायद विकास ने ऐसा ही
किया होगा, क्योंकि यह उसके टूलकिट में

949
01:15:32,429 --> 01:15:32,777
होगा।

950
01:15:33,326 --> 01:15:39,335
हाँ। हालांकि, ऐसे उदाहरण हैं जहाँ, उदाहरण
के लिए, जन्म से अंधे लोगों के कॉर्टेक्स

951
01:15:39,335 --> 01:15:43,542
का वह क्षेत्र किसी अन्य इंद्रिय द्वारा
अपना लिया जाता है।

952
01:15:44,106 --> 01:15:50,282
और मुझे कोई अंदाज़ा नहीं है, मुझे हैरानी
होगी अगर दृश्य संकेत पर निर्भर इच्छाएँ या

953
01:15:50,282 --> 01:15:55,773
इनाम के कार्य काम करना बंद कर दें। आप
जानते हैं, जिन लोगों के मस्तिष्क के

954
01:15:55,773 --> 01:15:58,595
विभिन्न हिस्सों का उपयोग किया गया हो।

955
01:15:58,646 --> 01:16:04,803
उदाहरण के लिए, अगर आपकी दृष्टि नहीं है,
तो क्या आपको अब भी यह चाहत महसूस होती है

956
01:16:04,803 --> 01:16:09,791
कि लोग आपको पसंद करें, आदि? जिसके लिए
अक्सर दृश्य संकेत होते हैं।

957
01:16:09,826 --> 01:16:15,210
तो मैं इससे पूरी तरह सहमत हूँ। मुझे लगता
है इस सिद्धांत का एक और मज़बूत प्रतिवाद

958
01:16:15,210 --> 01:16:20,527
है। जैसे, अगर आप लोगों के बारे में सोचें,
तो कुछ लोग ऐसे हैं जिनके बचपन में आधे

959
01:16:20,527 --> 01:16:22,504
दिमाग़ को निकाल दिया जाता है।

960
01:16:13,766 --> 01:16:14,276
हम्म

961
01:16:22,586 --> 01:16:23,468
हाँ, ठीक है।

962
01:16:23,486 --> 01:16:29,780
उनके सभी मस्तिष्क क्षेत्र एक ही गोलार्ध
में चले जाते हैं, जिससे पता चलता है कि

963
01:16:29,780 --> 01:16:33,377
उनका स्थान तय नहीं है। तो वह सिद्धांत गलत
है।

964
01:16:31,726 --> 01:16:31,911
हाँ।

965
01:16:33,526 --> 01:16:40,458
अगर यह सच होता तो अच्छा होता, पर ऐसा नहीं
है। तो मुझे लगता है कि यह एक रहस्य है, पर

966
01:16:40,458 --> 01:16:47,052
एक दिलचस्प रहस्य। तथ्य यह है कि विकास ने
हमें सामाजिक चीज़ों की बहुत मज़बूती से

967
01:16:47,052 --> 01:16:53,477
परवाह करना सिखाया। और यहाँ तक कि अजीब
मानसिक स्थितियों वाले लोग भी इसकी परवाह

968
01:16:53,477 --> 01:16:54,238
करते हैं।

969
01:16:54,266 --> 01:17:01,185
एआई उपकरण जैसे डीपफेक आदि ने धोखाधड़ी और
दुरुपयोग को बहुत जटिल बना दिया है।

970
01:17:02,806 --> 01:17:07,953
तो, यह पहले से कहीं ज़्यादा ज़रूरी है कि
आप अपने प्लेटफॉर्म का उपयोग करने वाले की

971
01:17:07,953 --> 01:17:12,457
पहचान और इरादे को समझें। ठीक यही काम
सार्डिन आपको करने में मदद करता है।

972
01:17:12,457 --> 01:17:17,411
सार्डिन हज़ारों डिवाइस, व्यवहार और पहचान
के सिग्नल को एक साथ लाता है ताकि आपको

973
01:17:17,411 --> 01:17:19,663
जोखिम का आकलन करने में मदद मिल सके।

974
01:17:19,726 --> 01:17:26,097
उपयोगकर्ता के टाइप करने, माउस हिलाने या
डिवाइस पकड़ने से लेकर, वीपीएन के पीछे

975
01:17:26,097 --> 01:17:32,300
असली जगह छिपाने तक, या केवाईसी सेल्फी
जांच में नकली कैमरा फ़ीड डालने तक, सब

976
01:17:32,300 --> 01:17:32,636
कुछ।

977
01:17:32,666 --> 01:17:38,339
सार्डिन इन संकेतों को अपने चार अरब
डिवाइसों के नेटवर्क की जानकारी से जोड़ता

978
01:17:38,339 --> 01:17:44,243
है, जैसे उपयोगकर्ता का धोखाधड़ी इतिहास या
उच्च जोखिम वाले खातों से संबंध, ताकि

979
01:17:44,243 --> 01:17:47,387
नुकसान से पहले बुरे तत्वों को पहचान सकें।

980
01:17:47,406 --> 01:17:52,228
अगर आप सिर्फ़ अपने ऐप के डेटा का इस्तेमाल
करते, तो यह नामुमकिन होता। Sardine सिर्फ़

981
01:17:52,228 --> 01:17:56,816
पहचान तक नहीं रुकता। वे ऑनबोर्डिंग और
जाँचों को आसान बनाने के लिए कई एजेंट देते

982
01:17:56,816 --> 01:18:01,285
हैं। तो जैसे धोखेबाज़ अपने हमलों को
बढ़ाने के लिए AI का इस्तेमाल करते हैं, आप

983
01:18:01,285 --> 01:18:04,403
अपनी सुरक्षा बढ़ाने के लिए AI का इस्तेमाल
कर सकते हैं।

984
01:18:04,646 --> 01:18:11,314
sardine.ai/swarkesh पर जाकर एआई फ्रॉड
डिटेक्शन गाइड डाउनलोड करें। एसएसआई क्या

985
01:18:11,314 --> 01:18:17,720
अलग करने की योजना बना रहा है? तो शायद,
आपका इरादा है कि जब यह समय आएगा, आप

986
01:18:17,720 --> 01:18:20,528
अग्रणी कंपनियों में से एक होंगे।

987
01:18:22,046 --> 01:18:27,405
और फिर क्या है... आपने SSI एक सुरक्षित
तरीके से शुरू किया जो दूसरों के पास नहीं

988
01:18:27,405 --> 01:18:28,779
था। वह अंतर क्या है?

989
01:18:34,326 --> 01:18:35,997
मैं इसे ऐसे कहूँगा

990
01:18:37,526 --> 01:18:43,417
कुछ विचार हैं जो मुझे आशाजनक लगते हैं, और
मैं उनकी जांच कर देखना चाहता हूँ कि वे

991
01:18:43,417 --> 01:18:49,234
वाकई आशाजनक हैं या नहीं। यह बस इतना ही
आसान है। यह एक कोशिश है। मुझे लगता है कि

992
01:18:49,234 --> 01:18:55,126
अगर ये विचार सही निकले, तो ये विचार जिनकी
हमने सामान्यीकरण समझने पर चर्चा की थी।

993
01:18:55,146 --> 01:18:55,656
ठीक है।

994
01:18:56,326 --> 01:18:58,183
अगर ये बातें सही हों,

995
01:19:00,706 --> 01:19:06,000
तो हमें कुछ अच्छा मिलेगा। क्या वे सही
होंगे? हम शोध कर रहे हैं।

996
01:19:06,406 --> 01:19:12,220
हम एक शोध-केंद्रित कंपनी हैं। हम प्रगति
कर रहे हैं। पिछले साल हमने अच्छी प्रगति

997
01:19:12,220 --> 01:19:17,737
की है, पर हमें और प्रगति - और शोध करते
रहना होगा। मैं इसे ऐसे ही देखता हूँ।

998
01:19:17,926 --> 01:19:26,517
मैं इसे एक प्रयास के तौर पर देखता हूँ...
यह एक आवाज़ और एक भागीदार बनने का प्रयास।

999
01:19:26,606 --> 01:19:33,501
लोगों ने पूछा है कि आपके सह-संस्थापक और
पूर्व सीईओ हाल ही में मेटा चले गए। उनका

1000
01:19:33,501 --> 01:19:40,574
कहना है, 'अगर बहुत सारी सफलताएँ मिल रही
थीं, तो ऐसा होना असंभव सा लगता है।' आप इस

1001
01:19:40,574 --> 01:19:43,138
पर क्या प्रतिक्रिया देते हैं?

1002
01:19:43,306 --> 01:19:50,061
हाँ, तो मैं इसके लिए बस कुछ ऐसे तथ्य याद
दिलाऊँगा जो शायद भुला दिए गए हों। और मेरा

1003
01:19:50,061 --> 01:19:55,148
मानना है कि ये तथ्य, जो संदर्भ देते हैं,
स्थिति को समझाते हैं।

1004
01:19:55,546 --> 01:20:01,388
तो संदर्भ यह था कि हम 32 बिलियन के
मूल्यांकन पर फंडरेज़ कर रहे थे, और फिर

1005
01:20:01,388 --> 01:20:07,311
मेटा ने आकर हमें खरीदने की पेशकश की। और
मैंने "नहीं" कहा, लेकिन मेरे पूर्व

1006
01:20:07,311 --> 01:20:10,313
सह-संस्थापक ने, एक तरह से, "हाँ" कहा।

1007
01:20:16,526 --> 01:20:22,852
और परिणामस्वरूप, उसे काफी अल्पकालिक तरलता
का लाभ मिला, और वह SSI से मेटा में शामिल

1008
01:20:22,852 --> 01:20:25,117
होने वाला एकमात्र व्यक्ति था।

1009
01:20:25,626 --> 01:20:32,316
लगता है SSI का लक्ष्य मानव इतिहास के इस
महत्वपूर्ण दौर में अग्रणी कंपनी बनना है,

1010
01:20:32,316 --> 01:20:39,092
जब अतिमानवीय बुद्धिमत्ता होगी और इसे सही
दिशा में ले जाने के विचार होंगे। पर अन्य

1011
01:20:39,092 --> 01:20:45,529
कंपनियाँ अपने तरीके आजमाएँगी। SSI का
अतिमानवीय बुद्धिमत्ता को सही दिशा में ले

1012
01:20:45,529 --> 01:20:47,731
जाने का तरीका कैसे अलग है?

1013
01:20:48,286 --> 01:20:55,189
एसएसआई को जो मुख्य बात अलग करती है, वह
उसका तकनीकी दृष्टिकोण है। तो हमारा एक अलग

1014
01:20:55,189 --> 01:21:01,743
तकनीकी दृष्टिकोण है जो मुझे लगता है कि
योग्य है, और हम उसे अपना रहे हैं। मैं

1015
01:21:01,743 --> 01:21:06,026
मानता हूँ कि अंत में, रणनीतियों का एक
अभिसरण होगा।

1016
01:21:06,066 --> 01:21:13,713
तो मुझे लगता है कि रणनीतियाँ एक होंगी,
जहाँ, AI के शक्तिशाली होने पर, सभी को

1017
01:21:13,713 --> 01:21:21,871
स्पष्ट हो जाएगा कि रणनीति क्या हो। और यह
ऐसा होगा, "हाँ, आपको आपस में बात करने का

1018
01:21:21,871 --> 01:21:25,338
तरीका खोजना होगा, और आप अपना पहला"

1019
01:21:27,046 --> 01:21:31,364
वास्तविक अति-बुद्धिमान एआई को संरेखित
होना और किसी तरह से हो,

1020
01:21:35,706 --> 01:21:40,628
सचेत जीवन, लोगों का ध्यान, लोकतांत्रिक।
इनमें से कोई एक, या इनका कुछ संयोजन।

1021
01:21:44,626 --> 01:21:50,474
यह वह स्थिति है जिसके लिए सभी को प्रयास
करना चाहिए, और SSI भी इसी के लिए प्रयासरत

1022
01:21:50,474 --> 01:21:56,030
है। और मुझे लगता है कि समय के साथ, अगर
अभी नहीं तो, बाकी कंपनियाँ भी यह समझकर

1023
01:21:56,030 --> 01:22:01,951
इसी के लिए प्रयास करेंगी। और हम देखेंगे।
मुझे लगता है कि AI के अधिक शक्तिशाली होने

1024
01:22:01,951 --> 01:22:03,852
पर दुनिया सचमुच बदल जाएगी।

1025
01:22:03,846 --> 01:22:04,310
हाँ।

1026
01:22:04,406 --> 01:22:10,338
और मुझे लगता है कि ये पूर्वानुमान...
चीजें बहुत अलग होंगी और लोग बहुत अलग तरह

1027
01:22:10,338 --> 01:22:11,743
से व्यवहार करेंगे।

1028
01:22:12,166 --> 01:22:18,096
तो, पूर्वानुमानों की बात करें तो, आप जिस
सिस्टम का वर्णन कर रहे हैं, जो इंसान की

1029
01:22:18,096 --> 01:22:23,125
तरह सीखकर सुपरह्यूमन बन जाता है, उसके लिए
आपके क्या पूर्वानुमान हैं?

1030
01:22:23,826 --> 01:22:26,287
मुझे लगता है, पाँच से बीस।

1031
01:22:26,706 --> 01:22:27,495
पांच-बीस?

1032
01:22:27,486 --> 01:22:27,950
हम्म

1033
01:22:28,466 --> 01:22:33,195
तो, मैं बस यह जानना चाहता हूँ कि आप
दुनिया को कैसे आते हुए देखते हैं। ऐसा है

1034
01:22:33,195 --> 01:22:38,303
कि हमारे पास कुछ और साल हैं जहाँ ये अन्य
कंपनियाँ मौजूदा दृष्टिकोण जारी रख रही हैं

1035
01:22:38,303 --> 01:22:43,285
और यह रुक जाता है। और यहाँ रुकने का मतलब
है कि वे राजस्व में सैकड़ों अरब से अधिक

1036
01:22:43,285 --> 01:22:46,438
नहीं कमाते हैं, या आप रुकने का क्या मतलब
समझते हैं?

1037
01:22:46,946 --> 01:22:54,151
हाँ। मुझे लगता है कि यह... मुझे लगता है
कि यह रुक सकता है, और मुझे लगता है कि

1038
01:22:54,151 --> 01:22:58,323
रुकना ऐसा दिखेगा... यह सब बहुत एक जैसा
लगेगा-

1039
01:22:58,386 --> 01:22:58,757
हाँ

1040
01:22:59,046 --> 01:23:04,933
सभी अलग-अलग कंपनियों में, कुछ ऐसा। मुझे
यकीन नहीं है क्योंकि मुझे लगता है कि भले

1041
01:23:04,933 --> 01:23:10,522
ही वे रुक जाएं, ये कंपनियाँ ज़बरदस्त
राजस्व कमा सकती हैं। शायद मुनाफ़ा नहीं,

1042
01:23:10,522 --> 01:23:16,186
क्योंकि उन्हें एक-दूसरे से खुद को अलग
करने के लिए कड़ी मेहनत करनी होगी। लेकिन

1043
01:23:16,186 --> 01:23:17,900
राजस्व, निश्चित रूप से।

1044
01:23:18,386 --> 01:23:21,218
पर आपके मॉडल से लगता है कि

1045
01:23:22,886 --> 01:23:28,845
जब सही हल निकलेगा, तो सभी कंपनियों में
एकरूपता आएगी। और मैं जानना चाहता हूँ कि

1046
01:23:28,845 --> 01:23:29,851
ऐसा क्यों है।

1047
01:23:29,986 --> 01:23:34,949
खैर, मैं उनकी बड़ी रणनीतियों के तालमेल की
बात कर रहा था। [pause] मुझे लगता है कि

1048
01:23:34,949 --> 01:23:39,913
तकनीकी दृष्टिकोण पर भी तालमेल शायद होगा।
[pause] लेकिन मेरा इशारा बड़ी रणनीतियों

1049
01:23:39,913 --> 01:23:42,803
की ओर था। [pause] तो, ठीक क्या किया जाना
चाहिए?

1050
01:23:32,846 --> 01:23:33,217
हूँ

1051
01:23:43,606 --> 01:23:49,379
मैं समझना चाहता हूँ कि आप भविष्य को कैसे
देखते हैं। क्या मौजूदा कंपनियाँ राजस्व तो

1052
01:23:49,379 --> 01:23:52,801
बनाएँगी, पर मानव-समान सीखने वाला नहीं बना
पाएँगी?

1053
01:23:50,746 --> 01:23:51,071
हाँ

1054
01:23:52,806 --> 01:23:53,084
हाँ

1055
01:23:54,206 --> 01:23:59,360
तो अब कंपनियों के अलग-अलग फ़ोर्क हैं। आप
हैं, थिंकिंग मशीन, और कई लैब भी।

1056
01:23:59,486 --> 01:23:59,996
जी हाँ

1057
01:24:00,066 --> 01:24:02,248
शायद सही रास्ता मिल जाए।

1058
01:24:02,246 --> 01:24:02,710
ठीक है।

1059
01:24:03,186 --> 01:24:07,272
लेकिन फिर उत्पाद के आने से दूसरों को यह
करना आ जाता है।

1060
01:24:07,426 --> 01:24:13,574
मुझे नहीं लगता कि इसे करने का तरीका
स्पष्ट होगा, लेकिन यह साफ होगा कि कुछ अलग

1061
01:24:13,574 --> 01:24:19,400
मुमकिन है। और यही जानकारी है। और मुझे
लगता है कि लोग तब यह समझने की कोशिश

1062
01:24:19,400 --> 01:24:25,791
करेंगे कि यह कैसे काम करता है। हालांकि
मुझे लगता है कि एक बात जो यहाँ संबोधित या

1063
01:24:25,791 --> 01:24:28,138
चर्चा नहीं की गई, वह यह है कि

1064
01:24:12,486 --> 01:24:12,903
ठीक

1065
01:24:30,246 --> 01:24:36,831
AI की क्षमताओं में हर वृद्धि के साथ, मुझे
लगता है कि कुछ बदलाव होंगे, लेकिन मुझे

1066
01:24:36,831 --> 01:24:40,416
ठीक से नहीं पता कि चीजें कैसे की जा रही
हैं।

1067
01:24:40,806 --> 01:24:41,131
तो

1068
01:24:41,326 --> 01:24:47,084
तो, मुझे लगता है यह अहम होगा, पर ठीक से
बता नहीं सकता क्या है।

1069
01:24:47,326 --> 01:24:55,062
आम तौर पर, आप उम्मीद करेंगे कि जिस कंपनी
के पास वह मॉडल है, उसे ये सभी लाभ मिल रहे

1070
01:24:55,062 --> 01:25:02,225
होंगे, क्योंकि उनके पास वह मॉडल है जो
दुनिया में कौशल और ज्ञान विकसित कर रहा

1071
01:25:02,225 --> 01:25:02,511
है।

1072
01:25:02,526 --> 01:25:09,016
फायदे सबको मिलेंगे, ऐसा क्यों लगता है, न
कि सिर्फ़ उस मॉडल कंपनी को जो यह सीखने का

1073
01:25:09,016 --> 01:25:10,699
चक्र पहले शुरू करेगी?

1074
01:25:11,166 --> 01:25:18,517
मुझे लगता है कि अनुभवजन्य रूप से क्या हो-
तो, मैं बताता हूँ क्या होगा। पहला, मुझे

1075
01:25:18,517 --> 01:25:25,869
लगता है अनुभवजन्य रूप से, जब... चलो, चलो
देखते हैं... देखते हैं कि अतीत की AIs के

1076
01:25:25,869 --> 01:25:28,534
साथ अब तक चीजें कैसी रही हैं।

1077
01:25:28,538 --> 01:25:36,294
तो एक कंपनी ने कुछ नया पेश किया, और दूसरी
कंपनी ने जल्दी से कुछ मिलती-जुलती चीजें

1078
01:25:36,294 --> 01:25:43,081
कुछ समय बाद बनाईं, और वे बाजार में
प्रतिस्पर्धा करने लगे और कीमतें नीचे

1079
01:25:43,081 --> 01:25:50,158
गिराने लगे। और इसलिए मुझे लगता है कि
बाजार के नजरिए से, वहाँ भी कुछ ऐसा ही

1080
01:25:50,158 --> 01:25:50,643
होगा।

1081
01:25:43,938 --> 01:25:44,170
हूँ

1082
01:25:50,778 --> 01:25:56,531
भले ही कोई... ठीक है, हम अच्छी दुनिया की
बात कर रहे हैं, वैसे, जहाँ... अच्छी

1083
01:25:56,531 --> 01:25:59,369
दुनिया क्या है? अच्छी दुनिया क्या है?

1084
01:26:01,158 --> 01:26:08,349
जहाँ हमारे पास ये शक्तिशाली मानव-जैसे
सीखने वाले हैं जो... और वैसे, एक और बात

1085
01:26:08,349 --> 01:26:16,109
जिस पर हमने सुपर इंटेलिजेंट एआई की खासियत
पर चर्चा नहीं की है, जो सोचने लायक है, वो

1086
01:26:16,109 --> 01:26:18,758
ये कि आप इसे सीमित रखते हैं।

1087
01:26:19,978 --> 01:26:24,850
एक साथ उपयोगी और सीमित हो सकते हैं। तो कई
संकीर्ण सुपर इंटेलिजेंट एआई हो सकते हैं।

1088
01:26:24,850 --> 01:26:26,293
पर मान लीजिए ऐसे कई हों,

1089
01:26:28,818 --> 01:26:36,307
और आपके पास कोई कंपनी है जो उससे बहुत
मुनाफा कमा रही है, फिर कोई और कंपनी आती

1090
01:26:36,307 --> 01:26:43,896
है और मुकाबला करती है, और मुकाबले का
तरीका विशेषज्ञता से होगा। मुझे लगता है कि

1091
01:26:43,896 --> 01:26:45,768
ऐसा होगा कि जिस तरह

1092
01:26:47,598 --> 01:26:51,787
प्रतियोगिता... जैसे, प्रतियोगिता
विशेषज्ञता को पसंद करती है। यह बाजार और

1093
01:26:51,787 --> 01:26:55,976
विकास दोनों में दिखता है। तो इस दुनिया
में कई अलग-अलग स्थान होंगे, और कई

1094
01:26:55,976 --> 01:26:57,629
कंपनियाँ उन पर कब्जा करेंगी।

1095
01:27:03,398 --> 01:27:09,144
जहाँ आप कहेंगे कि, एक AI कंपनी जटिल
आर्थिक गतिविधि के एक क्षेत्र में काफी

1096
01:27:09,144 --> 01:27:15,131
बेहतर है, दूसरी किसी और में, और तीसरी
मुकदमेबाजी में बहुत अच्छी है और... वही

1097
01:27:15,131 --> 01:27:16,168
वहाँ जाती है।

1098
01:27:16,138 --> 01:27:19,713
क्या यह मानव-सीखने की क्षमता के विपरीत
नहीं है?

1099
01:27:19,778 --> 01:27:25,804
यह हो सकता है, पर आपने ज्ञान जमा किया है।
आपका बड़ा निवेश है। आपने इस चीज़ में बेहद

1100
01:27:25,804 --> 01:27:31,316
असाधारण बनने के लिए बहुत कंप्यूट खर्च
किया है। और किसी और ने किसी दूसरी चीज़

1101
01:27:31,316 --> 01:27:35,799
में बेहद अच्छा बनने के लिए बहुत कंप्यूट
और अनुभव खर्च किया है।

1102
01:27:35,858 --> 01:27:36,275
ठीक

1103
01:27:36,478 --> 01:27:41,778
यहाँ तक पहुँचने में आपने बहुत कुछ सीखा
है, लेकिन अब आप उस मुकाम पर हैं जहाँ कोई

1104
01:27:41,778 --> 01:27:45,719
और कहेगा, "मैं वो सब नहीं सीखना चाहता
जिससे तुम गुज़रे हो।"

1105
01:27:45,878 --> 01:27:52,400
मुझे लगता है कि इसके लिए कई कंपनियों को
एक साथ मानव-जैसी निरंतर सीखने वाली एजेंट

1106
01:27:52,400 --> 01:27:57,766
पर काम शुरू करना होगा ताकि वे अपनी
अलग-अलग शाखाओं में शोध कर सकें।

1107
01:27:58,098 --> 01:28:03,773
लेकिन अगर एक कंपनी, आप जानते हैं, उस
एजेंट को पहले पाती है या उस सीखने वाले को

1108
01:28:03,773 --> 01:28:08,932
पहले पाती है, तो ऐसा लगता है कि, आप जानते
हैं, वे कर सकते हैं... अगर आप

1109
01:28:08,932 --> 01:28:14,386
अर्थव्यवस्था में हर एक नौकरी के बारे में
सोचते हैं, तो आपके पास बस इंस्टेंस

1110
01:28:14,386 --> 01:28:17,556
लर्निंग है, हर एक कंपनी के लिए संभव लगता
है।

1111
01:28:17,598 --> 01:28:22,706
हाँ। यह एक वैध तर्क है। मेरा प्रबल
अंतर्ज्ञान है कि ऐसा नहीं होगा।

1112
01:28:22,718 --> 01:28:23,135
हम्म

1113
01:28:24,498 --> 01:28:29,250
मेरा प्रबल अंतर्ज्ञान है कि, तर्क कहता है
कि यह ऐसे होगा। लेकिन मेरा प्रबल

1114
01:28:29,250 --> 01:28:34,132
अंतर्ज्ञान है कि यह ऐसे नहीं होगा। आप
जानते हैं, सिद्धांत और व्यवहार में कोई

1115
01:28:34,132 --> 01:28:39,405
अंतर नहीं होता। व्यवहार में, होता है। और
मुझे लगता है कि यह उन्हीं में से एक होगा।

1116
01:28:28,018 --> 01:28:28,435
हाँ।

1117
01:28:39,458 --> 01:28:43,606
बहुत से लोगों के रिकर्सिव
सेल्फ-इम्प्रूवमेंट मॉडल स्पष्ट रूप से

1118
01:28:43,606 --> 01:28:48,959
कहते हैं, 'हमारे पास एक सर्वर में दस लाख
इलिया होंगे जो अलग-अलग विचार लेकर आएंगे,

1119
01:28:48,959 --> 01:28:54,179
और इससे एक सुपरइंटेलिजेंस बहुत तेज़ी से
उभरेगी।' क्या आपको इस बात का अंदाज़ा है

1120
01:28:54,179 --> 01:28:59,331
कि आप जो कर रहे हैं वह कितना समानांतर
किया जा सकता है? इलिया की प्रतियां बनाने

1121
01:28:59,331 --> 01:29:00,402
से क्या लाभ हैं?

1122
01:29:00,838 --> 01:29:06,987
मुझे नहीं पता। मुझे लगता है कि निश्चित
रूप से घटते प्रतिफल होंगे, क्योंकि आप ऐसे

1123
01:29:06,987 --> 01:29:12,824
लोग चाहते हैं जो एक जैसा सोचने के बजाय
अलग सोचते हों। मुझे लगता है कि अगर वे

1124
01:29:12,824 --> 01:29:18,895
मेरी हूबहू नकल होते, तो मुझे यकीन नहीं कि
आपको कितना अतिरिक्त मूल्य मिलता। मुझे

1125
01:29:18,895 --> 01:29:22,943
लगता है कि... लेकिन अलग सोचने वाले लोग ही
आपको चाहिए।

1126
01:29:23,218 --> 01:29:29,915
ऐसा क्यों है कि... अगर आप अलग-अलग मॉडल
देखें, जो अलग कंपनियों ने, भिन्न डेटा सेट

1127
01:29:29,915 --> 01:29:35,849
पर प्रशिक्षित किए हैं, तो भी LLM कितने
समान हैं, यह वाकई हैरान करता है।

1128
01:29:35,918 --> 01:29:38,565
शायद डेटा सेट उतने अलग नहीं हैं।

1129
01:29:39,498 --> 01:29:44,777
लेकिन ऐसा लगता है कि भले ही एक इंसान
भविष्य के AI से कम उत्पादक हो, फिर भी

1130
01:29:44,777 --> 01:29:50,202
इंसानी टीमों में AI टीमों से ज़्यादा
विविधता होती है। लेकिन हम AI में सार्थक

1131
01:29:50,202 --> 01:29:55,916
विविधता कैसे लाएँ ताकि... मुझे लगता है कि
सिर्फ़ तापमान बढ़ाने से तो बस बकवास ही

1132
01:29:55,916 --> 01:29:56,495
निकलेगी।

1133
01:29:56,558 --> 01:30:01,958
आप अलग-अलग पूर्वाग्रहों या विचारों वाले
वैज्ञानिकों जैसा कुछ चाहते हैं। एआई

1134
01:30:01,958 --> 01:30:04,731
एजेंटों में ऐसी विविधता कैसे लाते हैं?

1135
01:30:04,760 --> 01:30:09,171
तो मेरा मानना है, विविधता न होने की वजह
प्री-ट्रेनिंग है।

1136
01:30:10,840 --> 01:30:16,405
सभी प्री-ट्रेन मॉडल एक जैसे होते हैं
क्योंकि उन्हें एक ही डेटा पर प्री-ट्रेन

1137
01:30:16,405 --> 01:30:21,970
किया जाता है। अब, आरएल और पोस्ट-ट्रेनिंग
में अंतर दिखना शुरू होता है क्योंकि

1138
01:30:21,970 --> 01:30:24,864
अलग-अलग लोग अलग आरएल ट्रेनिंग देते हैं।

1139
01:30:24,920 --> 01:30:31,431
हाँ। और मैंने पहले आपको सेल्फ-प्ले का
ज़िक्र करते सुना है, कि यह डेटा जुटाने या

1140
01:30:31,431 --> 01:30:38,027
समान बुद्धिमत्ता वाले एजेंटों को मिलाकर
सीखने की प्रक्रिया शुरू करने का एक तरीका

1141
01:30:38,027 --> 01:30:43,954
है। हमें यह क्यों सोचना चाहिए कि एलएलएम
के साथ काम करने के लिए इस तरह के

1142
01:30:43,954 --> 01:30:46,793
सार्वजनिक प्रस्ताव क्यों नहीं हैं?

1143
01:30:46,900 --> 01:30:48,896
मैं दो बातें कहना चाहूँगा।

1144
01:30:49,400 --> 01:30:55,257
मैं कहूँगा कि सेल्फ-प्ले मुझे दिलचस्प
इसलिए लगा क्योंकि यह डेटा के बिना, केवल

1145
01:30:55,257 --> 01:31:01,269
कंप्यूट का उपयोग करके मॉडल बनाने का एक
तरीका प्रदान करता था, है ना? और अगर आपको

1146
01:31:01,269 --> 01:31:07,359
लगता है कि डेटा ही सबसे बड़ी बाधा है, तो
केवल कंप्यूट का उपयोग करना बहुत दिलचस्प

1147
01:31:07,359 --> 01:31:11,598
है। तो यही बात इसे दिलचस्प बनाती है। अब,
बात यह है कि...

1148
01:31:13,700 --> 01:31:20,409
अतीत में जिस तरह से सेल्फ-प्ले होता था,
जहाँ एजेंट प्रतिस्पर्धा करते थे, वह कुछ

1149
01:31:20,409 --> 01:31:23,591
खास कौशल के विकास के लिए ही अच्छा है।

1150
01:31:23,640 --> 01:31:30,005
यह बहुत सीमित है। यह सिर्फ़ बातचीत,
विवाद, कुछ सामाजिक कौशल, रणनीति बनाने

1151
01:31:30,005 --> 01:31:36,723
जैसी चीज़ों के लिए अच्छा है। तो अगर आपको
उन कौशलों की परवाह है, तो सेल्फ-प्ले

1152
01:31:36,723 --> 01:31:38,315
उपयोगी हो सकता है।

1153
01:31:38,800 --> 01:31:46,791
अब, असल में मुझे लगता है कि सेल्फ-प्ले ने
अपनी जगह बना ली, लेकिन बस एक अलग रूप में।

1154
01:31:46,791 --> 01:31:51,663
एक अलग रूप में। तो बहस, प्रूवर-वेरिफायर
जैसी चीजें।

1155
01:31:52,160 --> 01:31:56,481
आपके पास एक तरह का एलएलएम जज के तौर पर
है, जिसे आपके काम में गलतियाँ ढूँढने के

1156
01:31:56,481 --> 01:32:00,914
लिए प्रोत्साहित किया जाता है। आप कह सकते
हैं कि यह ठीक सेल्फ-प्ले नहीं है, लेकिन

1157
01:32:00,914 --> 01:32:05,460
यह, आप जानते हैं, एक संबंधित विरोधी सेटअप
है जिसे लोग अपना रहे हैं, मेरा मानना है।

1158
01:32:05,460 --> 01:32:09,725
और वास्तव में सेल्फ-प्ले एक उदाहरण है,
उम, अधिक सामान्य, जैसे, उम, एजेंटों के

1159
01:32:09,725 --> 01:32:13,990
बीच प्रतिस्पर्धा का एक विशेष मामला है, है
ना? प्रतिस्पर्धा के प्रति स्वाभाविक

1160
01:32:13,990 --> 01:32:16,122
प्रतिक्रिया अलग होने की कोशिश करना है।

1161
01:32:04,260 --> 01:32:04,538
हूँ

1162
01:32:16,520 --> 01:32:23,058
और अगर आप कई एजेंटों को एक समस्या पर काम
करने को कहें, और आप खुद एक एजेंट के तौर

1163
01:32:23,058 --> 01:32:29,266
पर दूसरों का काम देख रहे हों, तो आप
कहेंगे, 'अगर वे पहले से ही यह तरीका अपना

1164
01:32:29,266 --> 01:32:35,143
रहे हैं, तो मुझे कुछ अलग करना चाहिए।' और
मुझे लगता है कि ऐसा कुछ विभिन्न

1165
01:32:35,143 --> 01:32:38,950
दृष्टिकोणों के लिए एक प्रोत्साहन भी दे
सकता है।

1166
01:32:38,980 --> 01:32:45,110
हाँ। उम्म, आखिरी सवाल। शोध की परख क्या
है? आप ज़ाहिर तौर पर

1167
01:32:47,160 --> 01:32:55,104
दुनिया में वह व्यक्ति जिसकी AI रिसर्च में
सबसे अच्छी परख मानी जाती है। आप, उह, डीप

1168
01:32:55,104 --> 01:33:02,755
लर्निंग के इतिहास में हुई कई सबसे बड़ी
चीज़ों के सह-लेखक हैं, एलेक्सनेट से लेकर

1169
01:33:02,755 --> 01:33:10,405
जीपीटी-3 तक और आगे भी। आप इन विचारों को
कैसे सोचते हैं, इसकी क्या विशेषता बताते

1170
01:33:10,405 --> 01:33:10,797
हैं?

1171
01:33:11,280 --> 01:33:18,450
मैं इस पर अपनी बात रख सकता हूँ। मुझे लगता
है कि लोग इसे अलग-अलग ढंग से करते हैं।

1172
01:33:18,450 --> 01:33:25,258
लेकिन एक बात जो, उम्म, मुझे व्यक्तिगत रूप
से प्रेरित करती है, वह सौंदर्य है।

1173
01:33:14,280 --> 01:33:14,512
हम्म।

1174
01:33:26,400 --> 01:33:32,483
कि AI कैसा होना चाहिए- ... लोगों के बारे
में सोचकर। लेकिन सही ढंग से सोचकर।

1175
01:33:28,440 --> 01:33:28,904
हम्म

1176
01:33:32,520 --> 01:33:33,030
हम्म

1177
01:33:33,080 --> 01:33:38,792
लोगों को गलत समझना आसान है। पर उन्हें सही
सोचना क्या है?

1178
01:33:38,860 --> 01:33:39,463
हाँ

1179
01:33:39,460 --> 01:33:44,911
तो, मैं आपको कुछ उदाहरण देता हूँ। कृत्रिम
न्यूरॉन का विचार सीधे दिमाग से प्रेरित

1180
01:33:44,911 --> 01:33:50,018
है, और यह एक शानदार विचार है। क्यों?
क्योंकि आप कहते हैं, 'दिमाग में कई अंग

1181
01:33:50,018 --> 01:33:54,227
होते हैं। इसमें सिलवटें भी हैं, पर शायद
उनसे फर्क नहीं पड़ता।'

1182
01:33:54,260 --> 01:33:54,724
हम्म

1183
01:33:54,900 --> 01:33:59,110
हम क्यों सोचते हैं कि न्यूरॉन्स मायने
रखते हैं? क्योंकि वे बहुत सारे हैं। यह

1184
01:33:59,110 --> 01:34:03,433
कुछ सही लगता है, इसलिए आप न्यूरॉन चाहते
हैं। आप एक स्थानीय सीखने का नियम चाहते

1185
01:34:03,433 --> 01:34:07,925
हैं जो कनेक्शन बदल दे। आप एक ऐसा स्थानीय
सीखने का नियम चाहते हैं, जो न्यूरॉन्स के

1186
01:34:07,925 --> 01:34:11,855
बीच के कनेक्शन बदल दे। है ना? यह
विश्वसनीय लगता है कि मस्तिष्क ऐसा करता

1187
01:34:11,855 --> 01:34:13,708
है। वितरित प्रतिनिधित्व का विचार।

1188
01:34:01,020 --> 01:34:01,298
हाँ

1189
01:34:15,600 --> 01:34:20,600
यह विचार कि मस्तिष्क, अनुभव पर
प्रतिक्रिया करता है। न्यूरल नेट को अनुभव

1190
01:34:20,600 --> 01:34:26,100
से सीखना चाहिए, प्रतिक्रिया से नहीं।
मस्तिष्क अनुभव से सीखता है, न्यूरल नेट को

1191
01:34:26,100 --> 01:34:31,528
भी। और आप खुद से पूछते हैं, क्या कुछ
मौलिक है या नहीं? चीजें कैसी होनी चाहिए।

1192
01:34:31,660 --> 01:34:32,077
हाँ।

1193
01:34:32,380 --> 01:34:37,697
और मुझे लगता है कि इसने मुझे काफी हद तक
निर्देशित किया है, कई कोणों से सोचने और

1194
01:34:37,697 --> 01:34:42,743
सुंदरता खोजने में। सुंदरता, सादगी।
बदसूरती के लिए कोई जगह नहीं है। यह सिर्फ

1195
01:34:42,743 --> 01:34:47,992
सुंदरता, सादगी, लालित्य, दिमाग से सही
प्रेरणा है, और ये सभी चीजें एक साथ मौजूद

1196
01:34:47,992 --> 01:34:53,446
होनी चाहिए। और वे जितनी अधिक मौजूद होंगी,
आप एक टॉप-डाउन विश्वास में उतने ही अधिक

1197
01:34:53,446 --> 01:34:54,810
आश्वस्त हो सकते हैं।

1198
01:34:56,140 --> 01:35:01,560
और फिर, टॉप-डाउन विश्वास ही आपको तब सहारा
देता है जब प्रयोग विपरीत हों। क्योंकि

1199
01:35:01,560 --> 01:35:06,911
हमेशा डेटा पर भरोसा करने पर, आप सही कर
रहे होते हैं पर बग होता है, और आपको पता

1200
01:35:06,911 --> 01:35:12,540
नहीं होता। आप कैसे बता सकते हैं कि बग है?
आपको कैसे पता चलेगा कि डीबगिंग जारी रखें

1201
01:35:12,540 --> 01:35:15,459
या यह गलत दिशा है? तो, क्या यह टॉप-डाउन
है?

1202
01:35:15,480 --> 01:35:17,476
तो... ऐसा ही होना चाहिए।

1203
01:35:18,420 --> 01:35:18,837
ठीक

1204
01:35:18,900 --> 01:35:25,412
इसे काम करना ही होगा। तो, हमें चलते रहना
होगा। यह ऊपर से नीचे का है। और यह दिमाग

1205
01:35:25,412 --> 01:35:28,791
की बहुमुखी सुंदरता और प्रेरणा पर टिका है।

1206
01:35:29,620 --> 01:35:31,570
ठीक है। यहीं छोड़ते हैं।

1207
01:35:31,620 --> 01:35:32,270
शुक्रिया

1208
01:35:32,240 --> 01:35:34,283
इल्या, बहुत शुक्रिया। वाह!

1209
01:35:34,860 --> 01:35:35,649
धन्यवाद

1210
01:35:35,720 --> 01:35:36,370
अच्छा था।

1211
01:35:36,380 --> 01:35:37,726
हाँ, अच्छा लगा।

1212
01:35:37,760 --> 01:35:39,896
हाँ। मैं भी। अरे सब।

1213
01:35:40,040 --> 01:35:46,862
आशा है आपको यह एपिसोड पसंद आया होगा। अगर
पसंद आया, तो इसे दूसरों के साथ साझा करें।

1214
01:35:46,862 --> 01:35:52,253
जिस भी प्लेटफॉर्म पर सुन रहे हैं, वहां
रेटिंग या कमेंट भी छोड़ें।

1215
01:35:52,740 --> 01:35:57,816
अगर आप पॉडकास्ट को स्पॉन्सर करना चाहते
हैं, तो आप dwarkesh.com/advertise पर

1216
01:35:57,816 --> 01:36:01,795
संपर्क कर सकते हैं। अन्यथा, मैं आपको अगले
वाले में मिलूंगा।

